{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e439f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/khanh/Projects/KhoaLuan/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bf632c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khanh/Projects/KhoaLuan/fairseq'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b140d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  chuẩn hóa giá trị đầu vào cho dataset \n",
    "# import os\n",
    "# import torchaudio\n",
    "# import torch\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # --- CẤU HÌNH ---\n",
    "# # Thư mục chứa file wav gốc (Tiếng Việt)\n",
    "# folder = ['source' , 'target']\n",
    "# file = ['train' , 'test',  'valid']\n",
    "\n",
    "# TARGET_SAMPLE_RATE = 16000\n",
    "# # ----------------\n",
    "\n",
    "# def convert_to_16k(input_path, output_path):\n",
    "#     try:\n",
    "#         # Load audio\n",
    "#         waveform, sample_rate = torchaudio.load(input_path)\n",
    "\n",
    "#         # 1. Chuyển về Mono (nếu đang là Stereo)\n",
    "#         if waveform.shape[0] > 1:\n",
    "#             waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "#         # 2. Resample về 16000Hz (nếu chưa đúng)\n",
    "#         if sample_rate != TARGET_SAMPLE_RATE:\n",
    "#             resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "#             waveform = resampler(waveform)\n",
    "\n",
    "#         # Tạo thư mục cha nếu chưa có\n",
    "#         os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "#         # Lưu file\n",
    "#         torchaudio.save(output_path, waveform, TARGET_SAMPLE_RATE)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Lỗi file {input_path}: {e}\")\n",
    "\n",
    "# def main():\n",
    "#     # Tìm tất cả file wav trong thư mục input (bao gồm cả thư mục con)\n",
    "#     for i in folder:\n",
    "#         for   j in file:\n",
    "#             INPUT_DIR = f\"/home/khanh/Projects/KhoaLuan/my_data/{i}_audio/{j}/\" \n",
    "\n",
    "#             # Thư mục lưu j đã chuẩn hóa (Sẽ tự tạo nếu chưa có)\n",
    "#             OUTPUT_DIR = f\"/home/khanh/Projects/KhoaLuan/my_data/{i}_wav_16k/{j}/\"\n",
    "\n",
    "#             files = glob.glob(os.path.join(INPUT_DIR, \"**/*.wav\"), recursive=True)\n",
    "        \n",
    "#             print(f\"Tìm thấy {len(files)} ({i} in {j}) file audio. Bắt đầu xử lý...\")\n",
    "\n",
    "#             for file_path in tqdm(files):\n",
    "#                 # Tạo đường dẫn output tương ứng\n",
    "#                 relative_path = os.path.relpath(file_path, INPUT_DIR)\n",
    "#                 output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "                \n",
    "#                 convert_to_16k(file_path, output_path)\n",
    "\n",
    "#             print(\"✅ Đã hoàn tất chuẩn hóa dữ liệu về 16kHz!\")\n",
    "#             print(f\"Dữ liệu mới nằm tại: {OUTPUT_DIR}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832be30",
   "metadata": {},
   "source": [
    "tair mhubert để trích xuất đặc chưng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ed9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ cấu trúc thư mục của bạn: /data/en-vi/train/wav_en/ và /data/en-vi/train/wav_vi/\n",
    "# train \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/train --dest ../manifest_temp/train_en --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/train --dest ../manifest_temp/train_vn --ext wav --valid-percent 0\n",
    "# # valid  \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/valid --dest ../manifest_temp/dev_vn --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/valid  --dest ../manifest_temp/dev_en --ext wav --valid-percent 0\n",
    "\n",
    "# #  test \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/test --dest ../manifest_temp/test_en --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/test --dest ../manifest_temp/test_vn --ext wav --valid-percent 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088909c",
   "metadata": {},
   "source": [
    "xoa file thừa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9dd167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p  ../manifest_temp/source\n",
    "# source\n",
    "!cp ../manifest_temp/dev_en/train.tsv ../manifest_temp/source/valid.tsv\n",
    "\n",
    "!cp ../manifest_temp/train_en/train.tsv ../manifest_temp/source/train.tsv\n",
    "!cp ../manifest_temp/test_en/train.tsv ../manifest_temp/source/test.tsv\n",
    "# target\n",
    "!mkdir -p ../manifest_temp/target\n",
    "!cp ../manifest_temp/dev_vn/train.tsv ../manifest_temp/target/valid.tsv\n",
    "!cp ../manifest_temp/train_vn/train.tsv ../manifest_temp/target/train.tsv\n",
    "!cp ../manifest_temp/test_vn//train.tsv ../manifest_temp/target/test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908b3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xoa thu muc cu di \n",
    "!rm -r ../manifest_temp/dev_vn/\n",
    "!rm -r ../manifest_temp/dev_en/\n",
    "!rm -r ../manifest_temp/train_en/\n",
    "!rm -r ../manifest_temp/train_vn/\n",
    "!rm -r ../manifest_temp/test_en/\n",
    "!rm -r ../manifest_temp/test_vn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75324451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 09:47:04 | INFO | dump_hubert_feature | Namespace(tsv_dir='/home/khanh/Projects/KhoaLuan/manifest_temp/target/', split='train', ckpt_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, nshard=1, rank=0, feat_dir='/home/khanh/Projects/KhoaLuan/hubert_feats', max_chunk=1600000)\n",
      "2026-01-24 09:47:07 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 09:47:07 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/home/khanh/Projects/KhoaLuan/checkpoints/', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 09:47:07 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "2026-01-24 09:47:08 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/home/khanh/Projects/KhoaLuan/checkpoints/', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 09:47:08 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2026-01-24 09:47:08 | INFO | feature_utils | rank 0 of 1, process 1541 (0-1541) out of 1541\n",
      "100%|███████████████████████████████████████| 1541/1541 [01:57<00:00, 13.11it/s]\n",
      "2026-01-24 09:49:06 | INFO | feature_utils | finished successfully\n"
     ]
    }
   ],
   "source": [
    "# Tạo thư mục chứa feature tạm thời\n",
    "!mkdir -p /home/khanh/Projects/KhoaLuan/hubert_feats\n",
    "\n",
    "# Chạy lệnh dump (Lưu ý thay HUBERT_PATH đúng chỗ bạn vừa tải)\n",
    "!python examples/hubert/simple_kmeans/dump_hubert_feature.py \\\n",
    "    /home/khanh/Projects/KhoaLuan/manifest_temp/target/ \\\n",
    "    train \\\n",
    "    /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt \\\n",
    "    11 \\\n",
    "    1 \\\n",
    "    0 \\\n",
    "    /home/khanh/Projects/KhoaLuan/hubert_feats \\\n",
    "    --max_chunk 1600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7860ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 09:54:35 | INFO | root | Namespace(feat_dir='/home/khanh/Projects/KhoaLuan/hubert_feats', split='train', nshard=1, km_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', n_clusters=1000, seed=0, percent=0.3, init='k-means++', max_iter=100, batch_size=10000, tol=0.0, max_no_improvement=100, n_init=20, reassignment_ratio=0.0)\n",
      "2026-01-24 09:54:36 | INFO | learn_kmeans | sampled 463 utterances, 140779 frames from shard 0/1\n",
      "2026-01-24 09:54:36 | INFO | root | loaded feature with dimension (140779, 768)\n",
      "Init 1/20 with method k-means++\n",
      "Inertia for init 1/20: 872291.125\n",
      "Init 2/20 with method k-means++\n",
      "Inertia for init 2/20: 868583.75\n",
      "Init 3/20 with method k-means++\n",
      "Inertia for init 3/20: 869889.3125\n",
      "Init 4/20 with method k-means++\n",
      "Inertia for init 4/20: 871182.3125\n",
      "Init 5/20 with method k-means++\n",
      "Inertia for init 5/20: 865968.0\n",
      "Init 6/20 with method k-means++\n",
      "Inertia for init 6/20: 873047.5625\n",
      "Init 7/20 with method k-means++\n",
      "Inertia for init 7/20: 866519.875\n",
      "Init 8/20 with method k-means++\n",
      "Inertia for init 8/20: 867980.75\n",
      "Init 9/20 with method k-means++\n",
      "Inertia for init 9/20: 867806.625\n",
      "Init 10/20 with method k-means++\n",
      "Inertia for init 10/20: 866404.75\n",
      "Init 11/20 with method k-means++\n",
      "Inertia for init 11/20: 870044.8125\n",
      "Init 12/20 with method k-means++\n",
      "Inertia for init 12/20: 869333.75\n",
      "Init 13/20 with method k-means++\n",
      "Inertia for init 13/20: 869834.6875\n",
      "Init 14/20 with method k-means++\n",
      "Inertia for init 14/20: 866723.875\n",
      "Init 15/20 with method k-means++\n",
      "Inertia for init 15/20: 871920.4375\n",
      "Init 16/20 with method k-means++\n",
      "Inertia for init 16/20: 870226.0\n",
      "Init 17/20 with method k-means++\n",
      "Inertia for init 17/20: 870458.625\n",
      "Init 18/20 with method k-means++\n",
      "Inertia for init 18/20: 872397.125\n",
      "Init 19/20 with method k-means++\n",
      "Inertia for init 19/20: 868024.125\n",
      "Init 20/20 with method k-means++\n",
      "Inertia for init 20/20: 868517.875\n",
      "Minibatch step 1/1407: mean batch inertia: 28.8148875\n",
      "Minibatch step 2/1407: mean batch inertia: 20.865128125, ewa inertia: 20.865128125\n",
      "Minibatch step 3/1407: mean batch inertia: 19.912671875, ewa inertia: 20.72981682367879\n",
      "Minibatch step 4/1407: mean batch inertia: 19.5360578125, ewa inertia: 20.56022469252681\n",
      "Minibatch step 5/1407: mean batch inertia: 19.4805625, ewa inertia: 20.406841798290863\n",
      "Minibatch step 6/1407: mean batch inertia: 19.241053125, ewa inertia: 20.24122329093316\n",
      "Minibatch step 7/1407: mean batch inertia: 19.080359375, ewa inertia: 20.076304422353367\n",
      "Minibatch step 8/1407: mean batch inertia: 19.1790375, ewa inertia: 19.948833627872137\n",
      "Minibatch step 9/1407: mean batch inertia: 18.8081328125, ewa inertia: 19.786779242963465\n",
      "Minibatch step 10/1407: mean batch inertia: 18.9024765625, ewa inertia: 19.66115022172984\n",
      "Minibatch step 11/1407: mean batch inertia: 19.075821875, ewa inertia: 19.5779951788644\n",
      "Minibatch step 12/1407: mean batch inertia: 18.8865609375, ewa inertia: 19.479766134772284\n",
      "Minibatch step 13/1407: mean batch inertia: 18.848128125, ewa inertia: 19.39003208025143\n",
      "Minibatch step 14/1407: mean batch inertia: 18.816253125, ewa inertia: 19.308517809012415\n",
      "Minibatch step 15/1407: mean batch inertia: 18.7196875, ewa inertia: 19.224865257653924\n",
      "Minibatch step 16/1407: mean batch inertia: 18.644340625, ewa inertia: 19.142392657475785\n",
      "Minibatch step 17/1407: mean batch inertia: 18.769865625, ewa inertia: 19.089469368304623\n",
      "Minibatch step 18/1407: mean batch inertia: 18.7890953125, ewa inertia: 19.04679653753255\n",
      "Minibatch step 19/1407: mean batch inertia: 18.6422765625, ewa inertia: 18.989328150683203\n",
      "Minibatch step 20/1407: mean batch inertia: 18.8255125, ewa inertia: 18.966055576356847\n",
      "Minibatch step 21/1407: mean batch inertia: 18.775475, ewa inertia: 18.938980625887055\n",
      "Minibatch step 22/1407: mean batch inertia: 18.5039203125, ewa inertia: 18.877173506496934\n",
      "Minibatch step 23/1407: mean batch inertia: 18.6425765625, ewa inertia: 18.843845342837756\n",
      "Minibatch step 24/1407: mean batch inertia: 18.64824375, ewa inertia: 18.81605707847666\n",
      "Minibatch step 25/1407: mean batch inertia: 18.6703140625, ewa inertia: 18.79535200446378\n",
      "Minibatch step 26/1407: mean batch inertia: 18.5678359375, ewa inertia: 18.763029790091885\n",
      "Minibatch step 27/1407: mean batch inertia: 18.6391859375, ewa inertia: 18.745435834616405\n",
      "Minibatch step 28/1407: mean batch inertia: 18.4187625, ewa inertia: 18.6990267801177\n",
      "Minibatch step 29/1407: mean batch inertia: 18.549275, ewa inertia: 18.677752198484267\n",
      "Minibatch step 30/1407: mean batch inertia: 18.533690625, ewa inertia: 18.657285999665646\n",
      "Minibatch step 31/1407: mean batch inertia: 18.685246875, ewa inertia: 18.66125827915625\n",
      "Minibatch step 32/1407: mean batch inertia: 18.6299640625, ewa inertia: 18.656812446416335\n",
      "Minibatch step 33/1407: mean batch inertia: 18.500053125, ewa inertia: 18.634542333983273\n",
      "Minibatch step 34/1407: mean batch inertia: 18.699146875, ewa inertia: 18.643720419082964\n",
      "Minibatch step 35/1407: mean batch inertia: 18.6643078125, ewa inertia: 18.64664518018781\n",
      "Minibatch step 36/1407: mean batch inertia: 18.488065625, ewa inertia: 18.624116475089387\n",
      "Minibatch step 37/1407: mean batch inertia: 18.6636796875, ewa inertia: 18.629737047956358\n",
      "Minibatch step 38/1407: mean batch inertia: 18.4091390625, ewa inertia: 18.59839765522211\n",
      "Minibatch step 39/1407: mean batch inertia: 18.671365625, ewa inertia: 18.608763896133873\n",
      "Minibatch step 40/1407: mean batch inertia: 18.529465625, ewa inertia: 18.59749833694452\n",
      "Minibatch step 41/1407: mean batch inertia: 18.4808171875, ewa inertia: 18.580921955435137\n",
      "Minibatch step 42/1407: mean batch inertia: 18.6072046875, ewa inertia: 18.584655828437675\n",
      "Minibatch step 43/1407: mean batch inertia: 18.5351203125, ewa inertia: 18.57761853394447\n",
      "Minibatch step 44/1407: mean batch inertia: 18.5787421875, ewa inertia: 18.577778166499595\n",
      "Minibatch step 45/1407: mean batch inertia: 18.657209375, ewa inertia: 18.58906261152025\n",
      "Minibatch step 46/1407: mean batch inertia: 18.609146875, ewa inertia: 18.591915895151413\n",
      "Minibatch step 47/1407: mean batch inertia: 18.4642859375, ewa inertia: 18.573784064259037\n",
      "Minibatch step 48/1407: mean batch inertia: 18.4289609375, ewa inertia: 18.553209674891363\n",
      "Minibatch step 49/1407: mean batch inertia: 18.55065, ewa inertia: 18.55284603305426\n",
      "Minibatch step 50/1407: mean batch inertia: 18.5163734375, ewa inertia: 18.547664530631433\n",
      "Minibatch step 51/1407: mean batch inertia: 18.4304078125, ewa inertia: 18.53100638059145\n",
      "Minibatch step 52/1407: mean batch inertia: 18.497003125, ewa inertia: 18.5261756865168\n",
      "Minibatch step 53/1407: mean batch inertia: 18.422696875, ewa inertia: 18.51147490351967\n",
      "Minibatch step 54/1407: mean batch inertia: 18.544734375, ewa inertia: 18.516199931432773\n",
      "Minibatch step 55/1407: mean batch inertia: 18.4149125, ewa inertia: 18.501810468237323\n",
      "Minibatch step 56/1407: mean batch inertia: 18.55570625, ewa inertia: 18.509467206660773\n",
      "Minibatch step 57/1407: mean batch inertia: 18.490696875, ewa inertia: 18.50680058758693\n",
      "Minibatch step 58/1407: mean batch inertia: 18.562753125, ewa inertia: 18.514749520306502\n",
      "Minibatch step 59/1407: mean batch inertia: 18.6002, ewa inertia: 18.526889096907368\n",
      "Minibatch step 60/1407: mean batch inertia: 18.356715625, ewa inertia: 18.50271329467589\n",
      "Minibatch step 61/1407: mean batch inertia: 18.491028125, ewa inertia: 18.501053233633712\n",
      "Minibatch step 62/1407: mean batch inertia: 18.4155109375, ewa inertia: 18.488900613072026\n",
      "Minibatch step 63/1407: mean batch inertia: 18.47500625, ewa inertia: 18.486926701568684\n",
      "Minibatch step 64/1407: mean batch inertia: 18.3436625, ewa inertia: 18.466573781897043\n",
      "Minibatch step 65/1407: mean batch inertia: 18.4399375, ewa inertia: 18.46278968161333\n",
      "Minibatch step 66/1407: mean batch inertia: 18.4891484375, ewa inertia: 18.466534354988333\n",
      "Minibatch step 67/1407: mean batch inertia: 18.532121875, ewa inertia: 18.475852087622467\n",
      "Minibatch step 68/1407: mean batch inertia: 18.3428515625, ewa inertia: 18.456957283655644\n",
      "Minibatch step 69/1407: mean batch inertia: 18.4904234375, ewa inertia: 18.461711674029893\n",
      "Minibatch step 70/1407: mean batch inertia: 18.360490625, ewa inertia: 18.447331641492617\n",
      "Minibatch step 71/1407: mean batch inertia: 18.533725, ewa inertia: 18.459605168770267\n",
      "Minibatch step 72/1407: mean batch inertia: 18.4277125, ewa inertia: 18.455074316551162\n",
      "Minibatch step 73/1407: mean batch inertia: 18.5461875, ewa inertia: 18.468018368753015\n",
      "Minibatch step 74/1407: mean batch inertia: 18.495265625, ewa inertia: 18.471889267495303\n",
      "Minibatch step 75/1407: mean batch inertia: 18.390165625, ewa inertia: 18.460279146385016\n",
      "Minibatch step 76/1407: mean batch inertia: 18.4178265625, ewa inertia: 18.454248093126736\n",
      "Minibatch step 77/1407: mean batch inertia: 18.4008796875, ewa inertia: 18.446666276728564\n",
      "Minibatch step 78/1407: mean batch inertia: 18.375203125, ewa inertia: 18.43651381874752\n",
      "Minibatch step 79/1407: mean batch inertia: 18.474284375, ewa inertia: 18.441879716780264\n",
      "Minibatch step 80/1407: mean batch inertia: 18.526171875, ewa inertia: 18.45385473570621\n",
      "Minibatch step 81/1407: mean batch inertia: 18.5760921875, ewa inertia: 18.471220476833327\n",
      "Minibatch step 82/1407: mean batch inertia: 18.5153125, ewa inertia: 18.477484438073088\n",
      "Minibatch step 83/1407: mean batch inertia: 18.52299375, ewa inertia: 18.48394974733959\n",
      "Minibatch step 84/1407: mean batch inertia: 18.5171546875, ewa inertia: 18.48866702822614\n",
      "Minibatch step 85/1407: mean batch inertia: 18.4538953125, ewa inertia: 18.483727162375004\n",
      "Minibatch step 86/1407: mean batch inertia: 18.5260015625, ewa inertia: 18.489732901844388\n",
      "Minibatch step 87/1407: mean batch inertia: 18.384328125, ewa inertia: 18.474758505361308\n",
      "Minibatch step 88/1407: mean batch inertia: 18.335609375, ewa inertia: 18.454990195891025\n",
      "Minibatch step 89/1407: mean batch inertia: 18.45571875, ewa inertia: 18.455093698392652\n",
      "Minibatch step 90/1407: mean batch inertia: 18.40131875, ewa inertia: 18.44745412623856\n",
      "Minibatch step 91/1407: mean batch inertia: 18.4224296875, ewa inertia: 18.44389901347559\n",
      "Minibatch step 92/1407: mean batch inertia: 18.574015625, ewa inertia: 18.46238411242777\n",
      "Minibatch step 93/1407: mean batch inertia: 18.4232578125, ewa inertia: 18.456825609809815\n",
      "Minibatch step 94/1407: mean batch inertia: 18.3615578125, ewa inertia: 18.443291329754434\n",
      "Minibatch step 95/1407: mean batch inertia: 18.507484375, ewa inertia: 18.452410955446375\n",
      "Minibatch step 96/1407: mean batch inertia: 18.3124015625, ewa inertia: 18.43252043222626\n",
      "Minibatch step 97/1407: mean batch inertia: 18.3870390625, ewa inertia: 18.426059092586215\n",
      "Minibatch step 98/1407: mean batch inertia: 18.47054375, ewa inertia: 18.43237883365935\n",
      "Minibatch step 99/1407: mean batch inertia: 18.3605734375, ewa inertia: 18.42217775450615\n",
      "Minibatch step 100/1407: mean batch inertia: 18.4708109375, ewa inertia: 18.42908685849732\n",
      "Minibatch step 101/1407: mean batch inertia: 18.551515625, ewa inertia: 18.446479778869914\n",
      "Minibatch step 102/1407: mean batch inertia: 18.4998609375, ewa inertia: 18.454063407031597\n",
      "Minibatch step 103/1407: mean batch inertia: 18.487934375, ewa inertia: 18.458875307581163\n",
      "Minibatch step 104/1407: mean batch inertia: 18.433140625, ewa inertia: 18.455219293576167\n",
      "Minibatch step 105/1407: mean batch inertia: 18.4877875, ewa inertia: 18.459846116480534\n",
      "Minibatch step 106/1407: mean batch inertia: 18.386278125, ewa inertia: 18.44939463310498\n",
      "Minibatch step 107/1407: mean batch inertia: 18.44635, ewa inertia: 18.448962095371638\n",
      "Minibatch step 108/1407: mean batch inertia: 18.4177921875, ewa inertia: 18.444533922638062\n",
      "Minibatch step 109/1407: mean batch inertia: 18.4182390625, ewa inertia: 18.440798326653113\n",
      "Minibatch step 110/1407: mean batch inertia: 18.45326875, ewa inertia: 18.442569945256164\n",
      "Minibatch step 111/1407: mean batch inertia: 18.37266875, ewa inertia: 18.432639387612156\n",
      "Minibatch step 112/1407: mean batch inertia: 18.4487421875, ewa inertia: 18.4349270420926\n",
      "Minibatch step 113/1407: mean batch inertia: 18.358603125, ewa inertia: 18.424084036396817\n",
      "Minibatch step 114/1407: mean batch inertia: 18.367703125, ewa inertia: 18.416074246455516\n",
      "Minibatch step 115/1407: mean batch inertia: 18.293890625, ewa inertia: 18.39871615276955\n",
      "Minibatch step 116/1407: mean batch inertia: 18.2927703125, ewa inertia: 18.383664889767765\n",
      "Minibatch step 117/1407: mean batch inertia: 18.400521875, ewa inertia: 18.386059688067554\n",
      "Minibatch step 118/1407: mean batch inertia: 18.41995, ewa inertia: 18.39087433672964\n",
      "Minibatch step 119/1407: mean batch inertia: 18.4089390625, ewa inertia: 18.393440713455078\n",
      "Minibatch step 120/1407: mean batch inertia: 18.29528125, ewa inertia: 18.37949562701452\n",
      "Minibatch step 121/1407: mean batch inertia: 18.4490328125, ewa inertia: 18.389374471379554\n",
      "Minibatch step 122/1407: mean batch inertia: 18.4194359375, ewa inertia: 18.393645172632635\n",
      "Minibatch step 123/1407: mean batch inertia: 18.378915625, ewa inertia: 18.391552610104913\n",
      "Minibatch step 124/1407: mean batch inertia: 18.3094359375, ewa inertia: 18.379886652922796\n",
      "Minibatch step 125/1407: mean batch inertia: 18.5587, ewa inertia: 18.405289884500746\n",
      "Minibatch step 126/1407: mean batch inertia: 18.472928125, ewa inertia: 18.414898954041767\n",
      "Minibatch step 127/1407: mean batch inertia: 18.52443125, ewa inertia: 18.430459729145934\n",
      "Minibatch step 128/1407: mean batch inertia: 18.4712609375, ewa inertia: 18.4362561786919\n",
      "Minibatch step 129/1407: mean batch inertia: 18.519615625, ewa inertia: 18.448098691308477\n",
      "Minibatch step 130/1407: mean batch inertia: 18.43300625, ewa inertia: 18.445954574060504\n",
      "Minibatch step 131/1407: mean batch inertia: 18.229221875, ewa inertia: 18.415164305689924\n",
      "Minibatch step 132/1407: mean batch inertia: 18.598190625, ewa inertia: 18.441166055840522\n",
      "Minibatch step 133/1407: mean batch inertia: 18.3565578125, ewa inertia: 18.42914613208139\n",
      "Minibatch step 134/1407: mean batch inertia: 18.5487703125, ewa inertia: 18.446140617152935\n",
      "Minibatch step 135/1407: mean batch inertia: 18.420634375, ewa inertia: 18.44251705668228\n",
      "Minibatch step 136/1407: mean batch inertia: 18.4298015625, ewa inertia: 18.440710621935544\n",
      "Minibatch step 137/1407: mean batch inertia: 18.4424796875, ewa inertia: 18.440961945357117\n",
      "Minibatch step 138/1407: mean batch inertia: 18.2417078125, ewa inertia: 18.412654780581278\n",
      "Minibatch step 139/1407: mean batch inertia: 18.458209375, ewa inertia: 18.419126522933702\n",
      "Minibatch step 140/1407: mean batch inertia: 18.36225, ewa inertia: 18.411046323625033\n",
      "Minibatch step 141/1407: mean batch inertia: 18.544378125, ewa inertia: 18.429988190562803\n",
      "Minibatch step 142/1407: mean batch inertia: 18.40454375, ewa inertia: 18.426373409974254\n",
      "Minibatch step 143/1407: mean batch inertia: 18.3990203125, ewa inertia: 18.42248747483087\n",
      "Minibatch step 144/1407: mean batch inertia: 18.3898375, ewa inertia: 18.41784903544589\n",
      "Minibatch step 145/1407: mean batch inertia: 18.34760625, ewa inertia: 18.4078699495749\n",
      "Minibatch step 146/1407: mean batch inertia: 18.4683609375, ewa inertia: 18.416463640145306\n",
      "Minibatch step 147/1407: mean batch inertia: 18.3988515625, ewa inertia: 18.41396156916288\n",
      "Minibatch step 148/1407: mean batch inertia: 18.248309375, ewa inertia: 18.390428085122124\n",
      "Minibatch step 149/1407: mean batch inertia: 18.483096875, ewa inertia: 18.403593135538074\n",
      "Minibatch step 150/1407: mean batch inertia: 18.23963125, ewa inertia: 18.380299786264302\n",
      "Minibatch step 151/1407: mean batch inertia: 18.336115625, ewa inertia: 18.374022735367255\n",
      "Minibatch step 152/1407: mean batch inertia: 18.4959015625, ewa inertia: 18.391337528254418\n",
      "Minibatch step 153/1407: mean batch inertia: 18.344140625, ewa inertia: 18.384632470255493\n",
      "Minibatch step 154/1407: mean batch inertia: 18.4887390625, ewa inertia: 18.39942243931992\n",
      "Minibatch step 155/1407: mean batch inertia: 18.4140734375, ewa inertia: 18.40150384266984\n",
      "Minibatch step 156/1407: mean batch inertia: 18.3245828125, ewa inertia: 18.39057600772598\n",
      "Minibatch step 157/1407: mean batch inertia: 18.4581546875, ewa inertia: 18.40017661573479\n",
      "Minibatch step 158/1407: mean batch inertia: 18.48539375, ewa inertia: 18.412283041969367\n",
      "Minibatch step 159/1407: mean batch inertia: 18.2969296875, ewa inertia: 18.395895294495382\n",
      "Minibatch step 160/1407: mean batch inertia: 18.35923125, ewa inertia: 18.390686593757295\n",
      "Minibatch step 161/1407: mean batch inertia: 18.3954328125, ewa inertia: 18.391360868333614\n",
      "Minibatch step 162/1407: mean batch inertia: 18.282290625, ewa inertia: 18.375865735028654\n",
      "Minibatch step 163/1407: mean batch inertia: 18.3507046875, ewa inertia: 18.372291214851263\n",
      "Minibatch step 164/1407: mean batch inertia: 18.426028125, ewa inertia: 18.379925383078103\n",
      "Minibatch step 165/1407: mean batch inertia: 18.3770375, ewa inertia: 18.37951511413676\n",
      "Minibatch step 166/1407: mean batch inertia: 18.32401875, ewa inertia: 18.371630987963044\n",
      "Minibatch step 167/1407: mean batch inertia: 18.1515984375, ewa inertia: 18.340371924109792\n",
      "Minibatch step 168/1407: mean batch inertia: 18.2443484375, ewa inertia: 18.32673028657466\n",
      "Minibatch step 169/1407: mean batch inertia: 18.4046140625, ewa inertia: 18.337794894604965\n",
      "Minibatch step 170/1407: mean batch inertia: 18.322459375, ewa inertia: 18.335616244284612\n",
      "Minibatch step 171/1407: mean batch inertia: 18.3583671875, ewa inertia: 18.338848371463953\n",
      "Minibatch step 172/1407: mean batch inertia: 18.450421875, ewa inertia: 18.354699132017444\n",
      "Minibatch step 173/1407: mean batch inertia: 18.4525609375, ewa inertia: 18.368601931489323\n",
      "Minibatch step 174/1407: mean batch inertia: 18.3128203125, ewa inertia: 18.360677280404037\n",
      "Minibatch step 175/1407: mean batch inertia: 18.473575, ewa inertia: 18.376716166552065\n",
      "Minibatch step 176/1407: mean batch inertia: 18.481803125, ewa inertia: 18.39164541196305\n",
      "Minibatch step 177/1407: mean batch inertia: 18.3515, ewa inertia: 18.38594212854736\n",
      "Minibatch step 178/1407: mean batch inertia: 18.5246953125, ewa inertia: 18.405654187639936\n",
      "Minibatch step 179/1407: mean batch inertia: 18.5026671875, ewa inertia: 18.41943640100264\n",
      "Minibatch step 180/1407: mean batch inertia: 18.36843125, ewa inertia: 18.412190321871705\n",
      "Minibatch step 181/1407: mean batch inertia: 18.3346140625, ewa inertia: 18.401169401375654\n",
      "Minibatch step 182/1407: mean batch inertia: 18.2954984375, ewa inertia: 18.38615718886313\n",
      "Minibatch step 183/1407: mean batch inertia: 18.148184375, ewa inertia: 18.35234943011002\n",
      "Minibatch step 184/1407: mean batch inertia: 18.280353125, ewa inertia: 18.342121229355648\n",
      "Minibatch step 185/1407: mean batch inertia: 18.4419234375, ewa inertia: 18.356299693362516\n",
      "Minibatch step 186/1407: mean batch inertia: 18.417703125, ewa inertia: 18.365023010827706\n",
      "Minibatch step 187/1407: mean batch inertia: 18.295328125, ewa inertia: 18.35512176266352\n",
      "Minibatch step 188/1407: mean batch inertia: 18.4271046875, ewa inertia: 18.36534806254084\n",
      "Minibatch step 189/1407: mean batch inertia: 18.35471875, ewa inertia: 18.363838002512306\n",
      "Minibatch step 190/1407: mean batch inertia: 18.258690625, ewa inertia: 18.348900173628614\n",
      "Minibatch step 191/1407: mean batch inertia: 18.550275, ewa inertia: 18.377508616073758\n",
      "Minibatch step 192/1407: mean batch inertia: 18.31469375, ewa inertia: 18.36858478228007\n",
      "Minibatch step 193/1407: mean batch inertia: 18.4812234375, ewa inertia: 18.384586864283186\n",
      "Minibatch step 194/1407: mean batch inertia: 18.253878125, ewa inertia: 18.366017644325353\n",
      "Minibatch step 195/1407: mean batch inertia: 18.4248046875, ewa inertia: 18.374369262903937\n",
      "Minibatch step 196/1407: mean batch inertia: 18.3935546875, ewa inertia: 18.37709485241893\n",
      "Minibatch step 197/1407: mean batch inertia: 18.3144328125, ewa inertia: 18.36819272996987\n",
      "Minibatch step 198/1407: mean batch inertia: 18.3748765625, ewa inertia: 18.36914227287797\n",
      "Minibatch step 199/1407: mean batch inertia: 18.3503984375, ewa inertia: 18.36647941801535\n",
      "Minibatch step 200/1407: mean batch inertia: 18.394575, ewa inertia: 18.370470834691673\n",
      "Minibatch step 201/1407: mean batch inertia: 18.4567625, ewa inertia: 18.382729914860494\n",
      "Minibatch step 202/1407: mean batch inertia: 18.5481328125, ewa inertia: 18.406227982432522\n",
      "Minibatch step 203/1407: mean batch inertia: 18.41330625, ewa inertia: 18.407233561004404\n",
      "Minibatch step 204/1407: mean batch inertia: 18.3306921875, ewa inertia: 18.396359662225542\n",
      "Minibatch step 205/1407: mean batch inertia: 18.4777609375, ewa inertia: 18.407923986032113\n",
      "Minibatch step 206/1407: mean batch inertia: 18.417421875, ewa inertia: 18.409273309653067\n",
      "Minibatch step 207/1407: mean batch inertia: 18.4700765625, ewa inertia: 18.41791136233767\n",
      "Minibatch step 208/1407: mean batch inertia: 18.3764078125, ewa inertia: 18.412015134203322\n",
      "Minibatch step 209/1407: mean batch inertia: 18.4141796875, ewa inertia: 18.41232264284044\n",
      "Minibatch step 210/1407: mean batch inertia: 18.4660140625, ewa inertia: 18.41995034843208\n",
      "Minibatch step 211/1407: mean batch inertia: 18.372371875, ewa inertia: 18.41319108242383\n",
      "Minibatch step 212/1407: mean batch inertia: 18.4021671875, ewa inertia: 18.411624965798765\n",
      "Minibatch step 213/1407: mean batch inertia: 18.498971875, ewa inertia: 18.424033959860594\n",
      "Minibatch step 214/1407: mean batch inertia: 18.480140625, ewa inertia: 18.432004788833375\n",
      "Minibatch step 215/1407: mean batch inertia: 18.4512234375, ewa inertia: 18.43473509834703\n",
      "Minibatch step 216/1407: mean batch inertia: 18.2176203125, ewa inertia: 18.403890548574754\n",
      "Minibatch step 217/1407: mean batch inertia: 18.374046875, ewa inertia: 18.399650788157825\n",
      "Minibatch step 218/1407: mean batch inertia: 18.29171875, ewa inertia: 18.384317354693152\n",
      "Minibatch step 219/1407: mean batch inertia: 18.243834375, ewa inertia: 18.364359551071452\n",
      "Minibatch step 220/1407: mean batch inertia: 18.440753125, ewa inertia: 18.375212452609816\n",
      "Minibatch step 221/1407: mean batch inertia: 18.3553328125, ewa inertia: 18.3723882389275\n",
      "Minibatch step 222/1407: mean batch inertia: 18.48011875, ewa inertia: 18.38769304231896\n",
      "Minibatch step 223/1407: mean batch inertia: 18.520965625, ewa inertia: 18.40662649631541\n",
      "Minibatch step 224/1407: mean batch inertia: 18.3765921875, ewa inertia: 18.402359653182092\n",
      "Minibatch step 225/1407: mean batch inertia: 18.46258125, ewa inertia: 18.410915072533975\n",
      "Minibatch step 226/1407: mean batch inertia: 18.3708734375, ewa inertia: 18.40522653225354\n",
      "Minibatch step 227/1407: mean batch inertia: 18.220884375, ewa inertia: 18.379037846750833\n",
      "Minibatch step 228/1407: mean batch inertia: 18.3597171875, ewa inertia: 18.37629304503882\n",
      "Minibatch step 229/1407: mean batch inertia: 18.4757578125, ewa inertia: 18.390423570320987\n",
      "Minibatch step 230/1407: mean batch inertia: 18.44161875, ewa inertia: 18.39769664599637\n",
      "Minibatch step 231/1407: mean batch inertia: 18.3763265625, ewa inertia: 18.39466069152892\n",
      "Minibatch step 232/1407: mean batch inertia: 18.399321875, ewa inertia: 18.395322885515434\n",
      "Minibatch step 233/1407: mean batch inertia: 18.562434375, ewa inertia: 18.419063685271727\n",
      "Minibatch step 234/1407: mean batch inertia: 18.399771875, ewa inertia: 18.416322982008232\n",
      "Minibatch step 235/1407: mean batch inertia: 18.3220890625, ewa inertia: 18.402935580458546\n",
      "Minibatch step 236/1407: mean batch inertia: 18.3369421875, ewa inertia: 18.393560187226758\n",
      "Minibatch step 237/1407: mean batch inertia: 18.3567203125, ewa inertia: 18.388326507055318\n",
      "Minibatch step 238/1407: mean batch inertia: 18.396359375, ewa inertia: 18.389467701535313\n",
      "Minibatch step 239/1407: mean batch inertia: 18.2696421875, ewa inertia: 18.3724446138758\n",
      "Minibatch step 240/1407: mean batch inertia: 18.3744859375, ewa inertia: 18.37273461581133\n",
      "Minibatch step 241/1407: mean batch inertia: 18.421621875, ewa inertia: 18.37967981529828\n",
      "Minibatch step 242/1407: mean batch inertia: 18.425959375, ewa inertia: 18.386254550303498\n",
      "Minibatch step 243/1407: mean batch inertia: 18.286328125, ewa inertia: 18.37205843930712\n",
      "Minibatch step 244/1407: mean batch inertia: 18.39159375, ewa inertia: 18.374833735612402\n",
      "Minibatch step 245/1407: mean batch inertia: 18.367053125, ewa inertia: 18.37372837823033\n",
      "Minibatch step 246/1407: mean batch inertia: 18.4065015625, ewa inertia: 18.378384321442386\n",
      "Minibatch step 247/1407: mean batch inertia: 18.3023671875, ewa inertia: 18.367584899089437\n",
      "Minibatch step 248/1407: mean batch inertia: 18.3248796875, ewa inertia: 18.36151795611608\n",
      "Minibatch step 249/1407: mean batch inertia: 18.3219609375, ewa inertia: 18.355898263174456\n",
      "Minibatch step 250/1407: mean batch inertia: 18.5070859375, ewa inertia: 18.37737683602934\n",
      "Minibatch step 251/1407: mean batch inertia: 18.2696125, ewa inertia: 18.36206722727393\n",
      "Minibatch step 252/1407: mean batch inertia: 18.4320015625, ewa inertia: 18.372002492968782\n",
      "Minibatch step 253/1407: mean batch inertia: 18.3697234375, ewa inertia: 18.37167871750795\n",
      "Minibatch step 254/1407: mean batch inertia: 18.5222390625, ewa inertia: 18.393068168423145\n",
      "Minibatch step 255/1407: mean batch inertia: 18.4223734375, ewa inertia: 18.397231440063557\n",
      "Minibatch step 256/1407: mean batch inertia: 18.40870625, ewa inertia: 18.398861616215914\n",
      "Minibatch step 257/1407: mean batch inertia: 18.35095, ewa inertia: 18.392055022066756\n",
      "Minibatch step 258/1407: mean batch inertia: 18.28710625, ewa inertia: 18.377145408191666\n",
      "Minibatch step 259/1407: mean batch inertia: 18.50611875, ewa inertia: 18.395468087806428\n",
      "Minibatch step 260/1407: mean batch inertia: 18.150059375, ewa inertia: 18.360603943353176\n",
      "Minibatch step 261/1407: mean batch inertia: 18.3959546875, ewa inertia: 18.365626069244186\n",
      "Minibatch step 262/1407: mean batch inertia: 18.3406828125, ewa inertia: 18.36208248965274\n",
      "Minibatch step 263/1407: mean batch inertia: 18.3260609375, ewa inertia: 18.356965064996857\n",
      "Minibatch step 264/1407: mean batch inertia: 18.2697265625, ewa inertia: 18.344571471802247\n",
      "Minibatch step 265/1407: mean batch inertia: 18.3887515625, ewa inertia: 18.350847944411672\n",
      "Minibatch step 266/1407: mean batch inertia: 18.4695796875, ewa inertia: 18.367715644807795\n",
      "Minibatch step 267/1407: mean batch inertia: 18.328028125, ewa inertia: 18.36207741213159\n",
      "Minibatch step 268/1407: mean batch inertia: 18.5337359375, ewa inertia: 18.386464189425013\n",
      "Converged (lack of improvement in inertia) at step 268/1407\n",
      "2026-01-24 10:15:44 | INFO | learn_kmeans | total intertia: 18.37105\n",
      "2026-01-24 10:15:44 | INFO | learn_kmeans | finished successfully\n"
     ]
    }
   ],
   "source": [
    "!python examples/hubert/simple_kmeans/learn_kmeans.py \\\n",
    "    /home/khanh/Projects/KhoaLuan/hubert_feats \\\n",
    "    train \\\n",
    "    1 \\\n",
    "    /home/khanh/Projects/KhoaLuan/kmeans_model.joblib \\\n",
    "    1000 \\\n",
    "    --percent 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d929dad",
   "metadata": {},
   "source": [
    "Trích xuất đặc trưng (Feature Extraction): Bạn cần dump các vector đặc trưng từ mHuBERT ra để train K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu đoạn này vào file make_dict.py rồi chạy: python make_dict.py\n",
    "# Hoặc chạy trực tiếp trong terminal python\n",
    "\n",
    "path = \"/home/khanh/Projects/KhoaLuan/checkpoints/dict.txt\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    # Tạo 1003 dòng giả\n",
    "    for i in range(1000):\n",
    "        f.write(f\"token_{i} 1\\n\")\n",
    "\n",
    "print(f\"Đã tạo xong dict.txt với 1003 dòng tại {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f22a55",
   "metadata": {},
   "source": [
    "trainign k_means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3e0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31ea12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 10:17:23 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, kmeans_model_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', features_path=None, manifest_path='/home/khanh/Projects/KhoaLuan/manifest_temp/target/train.tsv', out_quantized_file_path='/home/khanh/Projects/KhoaLuan/train.unit', extension='.wav', channel_id=None, hide_fname=False)\n",
      "2026-01-24 10:17:23 | INFO | __main__ | Extracting hubert acoustic features...\n",
      "2026-01-24 10:17:24 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 10:17:24 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 10:17:25 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "100%|███████████████████████████████████████| 1541/1541 [01:57<00:00, 13.07it/s]\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Features extracted for 1541 utterances.\n",
      "\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Dimensionality of representation = 768\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Loading K-means model from /home/khanh/Projects/KhoaLuan/kmeans_model.joblib ...\n",
      "Writing quantized predictions to /home/khanh/Projects/KhoaLuan/train.unit\n"
     ]
    }
   ],
   "source": [
    "#  cho tâp train \n",
    "!PYTHONPATH=. python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /home/khanh/Projects/KhoaLuan/kmeans_model.joblib --acoustic_model_path /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt --layer 11 --manifest_path /home/khanh/Projects/KhoaLuan/manifest_temp/target/train.tsv --out_quantized_file_path /home/khanh/Projects/KhoaLuan/train.unit --extension .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a86868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 10:19:41 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, kmeans_model_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', features_path=None, manifest_path='/home/khanh/Projects/KhoaLuan/manifest_temp/target/valid.tsv', out_quantized_file_path='/home/khanh/Projects/KhoaLuan/valid.unit', extension='.wav', channel_id=None, hide_fname=False)\n",
      "2026-01-24 10:19:41 | INFO | __main__ | Extracting hubert acoustic features...\n",
      "2026-01-24 10:19:44 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 10:19:44 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 10:19:44 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "100%|█████████████████████████████████████████| 192/192 [00:15<00:00, 12.39it/s]\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Features extracted for 192 utterances.\n",
      "\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Dimensionality of representation = 768\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Loading K-means model from /home/khanh/Projects/KhoaLuan/kmeans_model.joblib ...\n",
      "Writing quantized predictions to /home/khanh/Projects/KhoaLuan/valid.unit\n"
     ]
    }
   ],
   "source": [
    "#  cho tập valid \n",
    "!PYTHONPATH=. python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /home/khanh/Projects/KhoaLuan/kmeans_model.joblib --acoustic_model_path /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt --layer 11 --manifest_path /home/khanh/Projects/KhoaLuan/manifest_temp/target/valid.tsv --out_quantized_file_path /home/khanh/Projects/KhoaLuan/valid.unit --extension .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9b3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file .unit (file này có chứa ID|...) sang thành .txt\n",
    "!mkdir -p ../manifest_temp/train\n",
    "!mkdir -p ../manifest_temp/valid\n",
    "!cp /home/khanh/Projects/KhoaLuan/train.unit /home/khanh/Projects/KhoaLuan/manifest_temp/train/train.txt\n",
    "\n",
    "# Nếu bạn có cả valid thì copy luôn\n",
    "!cp /home/khanh/Projects/KhoaLuan/valid.unit /home/khanh/Projects/KhoaLuan/manifest_temp/valid/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ca9213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khanh/Projects/KhoaLuan/fairseq'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19b4e7",
   "metadata": {},
   "source": [
    "các điều kiện bắt buộc để chạy chương trình này đó là \n",
    "1 : file train và vali phỉa là dạng .txt (chuyển từ unit -> txt)\n",
    "2: chuyển về đúng đinh dạng .wav nữa (chạy lệnh sed -i 's/|/.wav|/' /home/khanh/Projects/KhoaLuan/manifest_temp/train_vn/train.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252dc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  chuyển train.tsv vào trong train \n",
    "!cp ../manifest_temp/source/train.tsv ../manifest_temp/train/train.tsv\n",
    "!cp ../manifest_temp/source/valid.tsv ../manifest_temp/valid/valid.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72134286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sắp xếp lại "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fcd529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n",
      "Processing train\n",
      "100%|█████████████████████████████████████| 1541/1541 [00:00<00:00, 5596.54it/s]\n",
      "Processed 1541 samples\n",
      "Writing manifest to /home/khanh/Projects/KhoaLuan/data_bin/train.tsv...\n"
     ]
    }
   ],
   "source": [
    "# Set đường dẫn gốc cho gọn\n",
    "\n",
    "!PYTHONPATH=. python examples/speech_to_speech/preprocessing/prep_s2ut_data.py --source-dir /home/khanh/Projects/KhoaLuan/data/source --target-dir /home/khanh/Projects/KhoaLuan/manifest_temp/train --data-split train  --output-root /home/khanh/Projects/KhoaLuan/data_bin/ --reduce-unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f34674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n",
      "Processing valid\n",
      "100%|███████████████████████████████████████| 192/192 [00:00<00:00, 5358.39it/s]\n",
      "Processed 192 samples\n",
      "Writing manifest to /home/khanh/Projects/KhoaLuan/data_bin/valid.tsv...\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=. python examples/speech_to_speech/preprocessing/prep_s2ut_data.py --source-dir /home/khanh/Projects/KhoaLuan/data/source --target-dir /home/khanh/Projects/KhoaLuan/manifest_temp/valid --data-split valid  --output-root /home/khanh/Projects/KhoaLuan/data_bin/ --reduce-unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f0bc837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tạo /home/khanh/Projects/KhoaLuan/data_bin/dict.txt...\n",
      "✅ Đã tạo xong! File chuẩn phải có 1000 dòng.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn file dict (sửa lại nếu cần)\n",
    "output_file = '/home/khanh/Projects/KhoaLuan/data_bin/dict.txt'\n",
    "\n",
    "print(f\"Đang tạo {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # mHuBERT dùng 1000 clusters -> chạy từ 0 đến 999\n",
    "    for i in range(1000):\n",
    "        # Cấu trúc chuẩn: <Tên Unit> <Khoảng trắng> <Tần suất giả>\n",
    "        f.write(f\"{i} 1\\n\")\n",
    "\n",
    "print(\"✅ Đã tạo xong! File chuẩn phải có 1000 dòng.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a17eec",
   "metadata": {},
   "source": [
    "lệnh train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c5aa080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 3000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 20, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_text', num_workers=4, skip_invalid_size_inputs_valid_test=False, max_tokens=2000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=2000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2t_transformer_s', max_epoch=0, max_update=3000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[8], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=20, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, decoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_normalize_before=True, decoder_learned_pos=False, attention_dropout=0.3, activation_dropout=0.3, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, quant_noise_pq=0, _name='s2t_transformer_s'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_text', num_workers=4, skip_invalid_size_inputs_valid_test=False, max_tokens=2000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=2000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2t_transformer_s', max_epoch=0, max_update=3000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[8], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=20, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, decoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_normalize_before=True, decoder_learned_pos=False, attention_dropout=0.3, activation_dropout=0.3, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, quant_noise_pq=0, _name='speech_to_text'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2026-01-24 10:55:19 | INFO | fairseq.tasks.speech_to_text | dictionary size (dict.txt): 1,004\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | S2TTransformerModel(\n",
      "  (encoder): S2TTransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (subsample): Conv1dSubsampler(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoderScriptable(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(1004, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=1004, bias=False)\n",
      "  )\n",
      ")\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | task: SpeechToTextTask\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | model: S2TTransformerModel\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | num. shared model params: 27,233,280 (num. trained: 27,233,280)\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2026-01-24 10:55:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 10:55:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 10:55:19 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 10:55:19 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
      "2026-01-24 10:55:19 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"valid\", n_samples=192, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 10:55:19 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2026-01-24 10:55:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2026-01-24 10:55:19 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 3.631 GB ; name = NVIDIA GeForce GTX 1650 with Max-Q Design\n",
      "2026-01-24 10:55:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2026-01-24 10:55:19 | INFO | fairseq_cli.train | max tokens per device = 2000 and max sentences per device = None\n",
      "2026-01-24 10:55:19 | INFO | fairseq.trainer | Preparing to load checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint_last.pt\n",
      "2026-01-24 10:55:19 | INFO | fairseq.trainer | No existing checkpoint found /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint_last.pt\n",
      "2026-01-24 10:55:19 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2026-01-24 10:55:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 10:55:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 10:55:19 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 10:55:20 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
      "2026-01-24 10:55:20 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"train\", n_samples=1_541, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2026-01-24 10:55:20 | WARNING | fairseq.tasks.fairseq_task | 11 samples have invalid sizes and will be skipped, max_positions=(2000, 1024), first few sample ids=[714, 1387, 757, 822, 571, 892, 147, 526, 889, 904]\n",
      "2026-01-24 10:55:20 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 10:55:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2026-01-24 10:55:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 10:55:20 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2026-01-24 10:55:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2026-01-24 10:55:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "2026-01-24 10:58:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 10:58:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 10:58:28 | INFO | valid | {\"epoch\": 1, \"valid_loss\": \"10.293\", \"valid_nll_loss\": \"10.278\", \"valid_ppl\": \"1241.86\", \"valid_wps\": \"5636.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"64\"}\n",
      "2026-01-24 10:58:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 64 updates\n",
      "2026-01-24 10:58:28 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint1.pt\n",
      "2026-01-24 10:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint1.pt\n",
      "2026-01-24 10:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint1.pt (epoch 1 @ 64 updates, score 10.293) (writing took 2.9118935690003127 seconds)\n",
      "2026-01-24 10:58:31 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2026-01-24 10:58:31 | INFO | train | {\"epoch\": 1, \"train_loss\": \"10.759\", \"train_nll_loss\": \"10.755\", \"train_ppl\": \"1728.65\", \"train_wps\": \"1835.2\", \"train_ups\": \"0.34\", \"train_wpb\": \"5376.8\", \"train_bsz\": \"23.4\", \"train_num_updates\": \"64\", \"train_lr\": \"1.6e-05\", \"train_gnorm\": \"1.87\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"179\", \"train_gb_free\": \"2.9\", \"train_wall\": \"191\"}\n",
      "2026-01-24 10:58:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 10:58:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 10:58:31 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2026-01-24 10:58:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:00:09 | INFO | train_inner | {\"epoch\": 2, \"update\": 1.554, \"loss\": \"10.576\", \"nll_loss\": \"10.567\", \"ppl\": \"1517.29\", \"wps\": \"1878.7\", \"ups\": \"0.35\", \"wpb\": \"5374.7\", \"bsz\": \"23.3\", \"num_updates\": \"100\", \"lr\": \"2.5e-05\", \"gnorm\": \"1.383\", \"loss_scale\": \"64\", \"train_wall\": \"277\", \"gb_free\": \"2.9\", \"wall\": \"290\"}\n",
      "2026-01-24 11:01:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:01:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:01:38 | INFO | valid | {\"epoch\": 2, \"valid_loss\": \"9.94\", \"valid_nll_loss\": \"9.908\", \"valid_ppl\": \"960.86\", \"valid_wps\": \"5665.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"129\", \"valid_best_loss\": \"9.94\"}\n",
      "2026-01-24 11:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 129 updates\n",
      "2026-01-24 11:01:38 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint2.pt\n",
      "2026-01-24 11:01:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint2.pt\n",
      "2026-01-24 11:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint2.pt (epoch 2 @ 129 updates, score 9.94) (writing took 2.6380779929995697 seconds)\n",
      "2026-01-24 11:01:40 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2026-01-24 11:01:40 | INFO | train | {\"epoch\": 2, \"train_loss\": \"10.168\", \"train_nll_loss\": \"10.146\", \"train_ppl\": \"1133.03\", \"train_wps\": \"1844.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"129\", \"train_lr\": \"3.225e-05\", \"train_gnorm\": \"0.474\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"381\"}\n",
      "2026-01-24 11:01:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:01:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:01:40 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2026-01-24 11:01:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:04:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:04:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:04:47 | INFO | valid | {\"epoch\": 3, \"valid_loss\": \"9.434\", \"valid_nll_loss\": \"9.335\", \"valid_ppl\": \"645.67\", \"valid_wps\": \"5640.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"194\", \"valid_best_loss\": \"9.434\"}\n",
      "2026-01-24 11:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 194 updates\n",
      "2026-01-24 11:04:47 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint3.pt\n",
      "2026-01-24 11:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint3.pt\n",
      "2026-01-24 11:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint3.pt (epoch 3 @ 194 updates, score 9.434) (writing took 3.970648163000078 seconds)\n",
      "2026-01-24 11:04:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2026-01-24 11:04:51 | INFO | train | {\"epoch\": 3, \"train_loss\": \"9.834\", \"train_nll_loss\": \"9.786\", \"train_ppl\": \"882.78\", \"train_wps\": \"1830.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"194\", \"train_lr\": \"4.85e-05\", \"train_gnorm\": \"0.406\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"572\"}\n",
      "2026-01-24 11:04:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:04:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:04:51 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2026-01-24 11:04:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:05:08 | INFO | train_inner | {\"epoch\": 4, \"update\": 3.092, \"loss\": \"9.884\", \"nll_loss\": \"9.839\", \"ppl\": \"916.01\", \"wps\": \"1803.8\", \"ups\": \"0.34\", \"wpb\": \"5382.4\", \"bsz\": \"23.6\", \"num_updates\": \"200\", \"lr\": \"5e-05\", \"gnorm\": \"0.412\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"588\"}\n",
      "2026-01-24 11:07:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:07:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:07:58 | INFO | valid | {\"epoch\": 4, \"valid_loss\": \"7.927\", \"valid_nll_loss\": \"7.603\", \"valid_ppl\": \"194.38\", \"valid_wps\": \"5642.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"259\", \"valid_best_loss\": \"7.927\"}\n",
      "2026-01-24 11:07:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 259 updates\n",
      "2026-01-24 11:07:58 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint4.pt\n",
      "2026-01-24 11:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint4.pt\n",
      "2026-01-24 11:08:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint4.pt (epoch 4 @ 259 updates, score 7.927) (writing took 4.454651678000118 seconds)\n",
      "2026-01-24 11:08:03 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2026-01-24 11:08:03 | INFO | train | {\"epoch\": 4, \"train_loss\": \"9.091\", \"train_nll_loss\": \"8.93\", \"train_ppl\": \"487.77\", \"train_wps\": \"1824.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"259\", \"train_lr\": \"6.475e-05\", \"train_gnorm\": \"0.557\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"764\"}\n",
      "2026-01-24 11:08:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:08:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:08:03 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2026-01-24 11:08:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:09:56 | INFO | train_inner | {\"epoch\": 5, \"update\": 4.631, \"loss\": \"8.632\", \"nll_loss\": \"8.401\", \"ppl\": \"338.04\", \"wps\": \"1867.6\", \"ups\": \"0.35\", \"wpb\": \"5383.4\", \"bsz\": \"23.4\", \"num_updates\": \"300\", \"lr\": \"7.5e-05\", \"gnorm\": \"0.645\", \"loss_scale\": \"64\", \"train_wall\": \"275\", \"gb_free\": \"2.9\", \"wall\": \"877\"}\n",
      "2026-01-24 11:11:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:11:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:11:10 | INFO | valid | {\"epoch\": 5, \"valid_loss\": \"6.401\", \"valid_nll_loss\": \"5.838\", \"valid_ppl\": \"57.22\", \"valid_wps\": \"5640.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"324\", \"valid_best_loss\": \"6.401\"}\n",
      "2026-01-24 11:11:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 324 updates\n",
      "2026-01-24 11:11:10 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint5.pt\n",
      "2026-01-24 11:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint5.pt\n",
      "2026-01-24 11:11:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint5.pt (epoch 5 @ 324 updates, score 6.401) (writing took 3.9536813439999605 seconds)\n",
      "2026-01-24 11:11:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2026-01-24 11:11:14 | INFO | train | {\"epoch\": 5, \"train_loss\": \"7.804\", \"train_nll_loss\": \"7.451\", \"train_ppl\": \"174.98\", \"train_wps\": \"1829.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"324\", \"train_lr\": \"8.1e-05\", \"train_gnorm\": \"0.77\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"955\"}\n",
      "2026-01-24 11:11:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:11:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:11:14 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2026-01-24 11:11:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:14:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:14:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:14:21 | INFO | valid | {\"epoch\": 6, \"valid_loss\": \"5.587\", \"valid_nll_loss\": \"4.859\", \"valid_ppl\": \"29.02\", \"valid_wps\": \"5639.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"389\", \"valid_best_loss\": \"5.587\"}\n",
      "2026-01-24 11:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 389 updates\n",
      "2026-01-24 11:14:21 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint6.pt\n",
      "2026-01-24 11:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint6.pt\n",
      "2026-01-24 11:14:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint6.pt (epoch 6 @ 389 updates, score 5.587) (writing took 3.183677701999841 seconds)\n",
      "2026-01-24 11:14:24 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2026-01-24 11:14:24 | INFO | train | {\"epoch\": 6, \"train_loss\": \"6.695\", \"train_nll_loss\": \"6.193\", \"train_ppl\": \"73.14\", \"train_wps\": \"1837.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"389\", \"train_lr\": \"9.725e-05\", \"train_gnorm\": \"0.789\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"1145\"}\n",
      "2026-01-24 11:14:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:14:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:14:24 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2026-01-24 11:14:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:14:55 | INFO | train_inner | {\"epoch\": 7, \"update\": 6.169, \"loss\": \"6.806\", \"nll_loss\": \"6.318\", \"ppl\": \"79.77\", \"wps\": \"1800.5\", \"ups\": \"0.33\", \"wpb\": \"5376.2\", \"bsz\": \"23.5\", \"num_updates\": \"400\", \"lr\": \"0.0001\", \"gnorm\": \"0.785\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"1175\"}\n",
      "2026-01-24 11:17:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:17:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:17:31 | INFO | valid | {\"epoch\": 7, \"valid_loss\": \"5.087\", \"valid_nll_loss\": \"4.226\", \"valid_ppl\": \"18.71\", \"valid_wps\": \"5641.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"454\", \"valid_best_loss\": \"5.087\"}\n",
      "2026-01-24 11:17:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 454 updates\n",
      "2026-01-24 11:17:31 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint7.pt\n",
      "2026-01-24 11:17:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint7.pt\n",
      "2026-01-24 11:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint7.pt (epoch 7 @ 454 updates, score 5.087) (writing took 5.23202138600027 seconds)\n",
      "2026-01-24 11:17:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2026-01-24 11:17:37 | INFO | train | {\"epoch\": 7, \"train_loss\": \"5.915\", \"train_nll_loss\": \"5.298\", \"train_ppl\": \"39.34\", \"train_wps\": \"1817.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"454\", \"train_lr\": \"0.0001135\", \"train_gnorm\": \"0.723\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"1337\"}\n",
      "2026-01-24 11:17:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:17:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:17:37 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2026-01-24 11:17:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:19:42 | INFO | train_inner | {\"epoch\": 8, \"update\": 7.708, \"loss\": \"5.645\", \"nll_loss\": \"4.982\", \"ppl\": \"31.61\", \"wps\": \"1866.5\", \"ups\": \"0.35\", \"wpb\": \"5368.9\", \"bsz\": \"23.1\", \"num_updates\": \"500\", \"lr\": \"0.000125\", \"gnorm\": \"0.701\", \"loss_scale\": \"64\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"1463\"}\n",
      "2026-01-24 11:20:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:20:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:20:44 | INFO | valid | {\"epoch\": 8, \"valid_loss\": \"4.744\", \"valid_nll_loss\": \"3.767\", \"valid_ppl\": \"13.61\", \"valid_wps\": \"5643.1\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"519\", \"valid_best_loss\": \"4.744\"}\n",
      "2026-01-24 11:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 519 updates\n",
      "2026-01-24 11:20:44 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint8.pt\n",
      "2026-01-24 11:20:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint8.pt\n",
      "2026-01-24 11:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint8.pt (epoch 8 @ 519 updates, score 4.744) (writing took 2.847846523000044 seconds)\n",
      "2026-01-24 11:20:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2026-01-24 11:20:47 | INFO | train | {\"epoch\": 8, \"train_loss\": \"5.312\", \"train_nll_loss\": \"4.59\", \"train_ppl\": \"24.09\", \"train_wps\": \"1840.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"519\", \"train_lr\": \"0.00012975\", \"train_gnorm\": \"0.664\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"1527\"}\n",
      "2026-01-24 11:20:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:20:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:20:47 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2026-01-24 11:20:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:23:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:23:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:23:54 | INFO | valid | {\"epoch\": 9, \"valid_loss\": \"4.503\", \"valid_nll_loss\": \"3.434\", \"valid_ppl\": \"10.81\", \"valid_wps\": \"5641.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"584\", \"valid_best_loss\": \"4.503\"}\n",
      "2026-01-24 11:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 584 updates\n",
      "2026-01-24 11:23:54 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint9.pt\n",
      "2026-01-24 11:23:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint9.pt\n",
      "2026-01-24 11:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint9.pt (epoch 9 @ 584 updates, score 4.503) (writing took 4.2619807220007715 seconds)\n",
      "2026-01-24 11:23:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2026-01-24 11:23:58 | INFO | train | {\"epoch\": 9, \"train_loss\": \"4.852\", \"train_nll_loss\": \"4.037\", \"train_ppl\": \"16.41\", \"train_wps\": \"1827\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"584\", \"train_lr\": \"0.000146\", \"train_gnorm\": \"0.618\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"1718\"}\n",
      "2026-01-24 11:23:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:23:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:23:58 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2026-01-24 11:23:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:24:42 | INFO | train_inner | {\"epoch\": 10, \"update\": 9.246, \"loss\": \"4.861\", \"nll_loss\": \"4.048\", \"ppl\": \"16.54\", \"wps\": \"1796.8\", \"ups\": \"0.33\", \"wpb\": \"5391.6\", \"bsz\": \"24\", \"num_updates\": \"600\", \"lr\": \"0.00015\", \"gnorm\": \"0.612\", \"loss_scale\": \"64\", \"train_wall\": \"275\", \"gb_free\": \"2.9\", \"wall\": \"1763\"}\n",
      "2026-01-24 11:26:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:26:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:27:05 | INFO | valid | {\"epoch\": 10, \"valid_loss\": \"4.336\", \"valid_nll_loss\": \"3.193\", \"valid_ppl\": \"9.15\", \"valid_wps\": \"5645.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"649\", \"valid_best_loss\": \"4.336\"}\n",
      "2026-01-24 11:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 649 updates\n",
      "2026-01-24 11:27:05 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint10.pt\n",
      "2026-01-24 11:27:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint10.pt\n",
      "2026-01-24 11:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint10.pt (epoch 10 @ 649 updates, score 4.336) (writing took 4.233275706000313 seconds)\n",
      "2026-01-24 11:27:09 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2026-01-24 11:27:09 | INFO | train | {\"epoch\": 10, \"train_loss\": \"4.525\", \"train_nll_loss\": \"3.631\", \"train_ppl\": \"12.39\", \"train_wps\": \"1827.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"649\", \"train_lr\": \"0.00016225\", \"train_gnorm\": \"0.556\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"1910\"}\n",
      "2026-01-24 11:27:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:27:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:27:09 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2026-01-24 11:27:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:29:31 | INFO | train_inner | {\"epoch\": 11, \"update\": 10.785, \"loss\": \"4.413\", \"nll_loss\": \"3.486\", \"ppl\": \"11.2\", \"wps\": \"1873\", \"ups\": \"0.35\", \"wpb\": \"5397.2\", \"bsz\": \"23.4\", \"num_updates\": \"700\", \"lr\": \"0.000175\", \"gnorm\": \"0.542\", \"loss_scale\": \"64\", \"train_wall\": \"275\", \"gb_free\": \"2.9\", \"wall\": \"2051\"}\n",
      "2026-01-24 11:30:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:30:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:30:16 | INFO | valid | {\"epoch\": 11, \"valid_loss\": \"4.229\", \"valid_nll_loss\": \"3.033\", \"valid_ppl\": \"8.19\", \"valid_wps\": \"5648.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"714\", \"valid_best_loss\": \"4.229\"}\n",
      "2026-01-24 11:30:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 714 updates\n",
      "2026-01-24 11:30:16 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint11.pt\n",
      "2026-01-24 11:30:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint11.pt\n",
      "2026-01-24 11:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint11.pt (epoch 11 @ 714 updates, score 4.229) (writing took 10.427532534999955 seconds)\n",
      "2026-01-24 11:30:27 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2026-01-24 11:30:27 | INFO | train | {\"epoch\": 11, \"train_loss\": \"4.311\", \"train_nll_loss\": \"3.354\", \"train_ppl\": \"10.22\", \"train_wps\": \"1769.7\", \"train_ups\": \"0.33\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"714\", \"train_lr\": \"0.0001785\", \"train_gnorm\": \"0.534\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"2107\"}\n",
      "2026-01-24 11:30:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:30:27 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2026-01-24 11:30:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:33:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:33:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:33:34 | INFO | valid | {\"epoch\": 12, \"valid_loss\": \"4.165\", \"valid_nll_loss\": \"2.929\", \"valid_ppl\": \"7.62\", \"valid_wps\": \"5675\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"779\", \"valid_best_loss\": \"4.165\"}\n",
      "2026-01-24 11:33:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 779 updates\n",
      "2026-01-24 11:33:34 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint12.pt\n",
      "2026-01-24 11:33:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint12.pt\n",
      "2026-01-24 11:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint12.pt (epoch 12 @ 779 updates, score 4.165) (writing took 5.564773019000313 seconds)\n",
      "2026-01-24 11:33:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2026-01-24 11:33:39 | INFO | train | {\"epoch\": 12, \"train_loss\": \"4.17\", \"train_nll_loss\": \"3.163\", \"train_ppl\": \"8.96\", \"train_wps\": \"1815.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"779\", \"train_lr\": \"0.00019475\", \"train_gnorm\": \"0.499\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"2300\"}\n",
      "2026-01-24 11:33:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:33:39 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2026-01-24 11:33:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:34:37 | INFO | train_inner | {\"epoch\": 13, \"update\": 12.323, \"loss\": \"4.164\", \"nll_loss\": \"3.155\", \"ppl\": \"8.91\", \"wps\": \"1747\", \"ups\": \"0.33\", \"wpb\": \"5347.5\", \"bsz\": \"23.3\", \"num_updates\": \"800\", \"lr\": \"0.0002\", \"gnorm\": \"0.502\", \"loss_scale\": \"64\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"2357\"}\n",
      "2026-01-24 11:36:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:36:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:36:46 | INFO | valid | {\"epoch\": 13, \"valid_loss\": \"4.124\", \"valid_nll_loss\": \"2.867\", \"valid_ppl\": \"7.29\", \"valid_wps\": \"5673.2\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"844\", \"valid_best_loss\": \"4.124\"}\n",
      "2026-01-24 11:36:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 844 updates\n",
      "2026-01-24 11:36:46 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint13.pt\n",
      "2026-01-24 11:36:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint13.pt\n",
      "2026-01-24 11:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint13.pt (epoch 13 @ 844 updates, score 4.124) (writing took 7.612164961000417 seconds)\n",
      "2026-01-24 11:36:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2026-01-24 11:36:54 | INFO | train | {\"epoch\": 13, \"train_loss\": \"4.081\", \"train_nll_loss\": \"3.04\", \"train_ppl\": \"8.23\", \"train_wps\": \"1798.2\", \"train_ups\": \"0.33\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"844\", \"train_lr\": \"0.000211\", \"train_gnorm\": \"0.48\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"2494\"}\n",
      "2026-01-24 11:36:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:36:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:36:54 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2026-01-24 11:36:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:39:27 | INFO | train_inner | {\"epoch\": 14, \"update\": 13.862, \"loss\": \"4.045\", \"nll_loss\": \"2.99\", \"ppl\": \"7.94\", \"wps\": \"1853\", \"ups\": \"0.34\", \"wpb\": \"5389.1\", \"bsz\": \"23.5\", \"num_updates\": \"900\", \"lr\": \"0.000225\", \"gnorm\": \"0.475\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"2648\"}\n",
      "2026-01-24 11:39:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:39:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:40:00 | INFO | valid | {\"epoch\": 14, \"valid_loss\": \"4.083\", \"valid_nll_loss\": \"2.814\", \"valid_ppl\": \"7.03\", \"valid_wps\": \"5675.1\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"909\", \"valid_best_loss\": \"4.083\"}\n",
      "2026-01-24 11:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 909 updates\n",
      "2026-01-24 11:40:00 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint14.pt\n",
      "2026-01-24 11:40:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint14.pt\n",
      "2026-01-24 11:40:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint14.pt (epoch 14 @ 909 updates, score 4.083) (writing took 4.376504533000116 seconds)\n",
      "2026-01-24 11:40:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2026-01-24 11:40:05 | INFO | train | {\"epoch\": 14, \"train_loss\": \"4.02\", \"train_nll_loss\": \"2.956\", \"train_ppl\": \"7.76\", \"train_wps\": \"1829\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"909\", \"train_lr\": \"0.00022725\", \"train_gnorm\": \"0.471\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"2685\"}\n",
      "2026-01-24 11:40:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:40:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:40:05 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2026-01-24 11:40:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:43:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:43:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:43:12 | INFO | valid | {\"epoch\": 15, \"valid_loss\": \"4.055\", \"valid_nll_loss\": \"2.779\", \"valid_ppl\": \"6.86\", \"valid_wps\": \"5674.2\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"974\", \"valid_best_loss\": \"4.055\"}\n",
      "2026-01-24 11:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 974 updates\n",
      "2026-01-24 11:43:12 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint15.pt\n",
      "2026-01-24 11:43:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint15.pt\n",
      "2026-01-24 11:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint15.pt (epoch 15 @ 974 updates, score 4.055) (writing took 3.9991152020011214 seconds)\n",
      "2026-01-24 11:43:16 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2026-01-24 11:43:16 | INFO | train | {\"epoch\": 15, \"train_loss\": \"3.977\", \"train_nll_loss\": \"2.9\", \"train_ppl\": \"7.46\", \"train_wps\": \"1832\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"974\", \"train_lr\": \"0.0002435\", \"train_gnorm\": \"0.454\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"2876\"}\n",
      "2026-01-24 11:43:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:43:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:43:16 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2026-01-24 11:43:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:44:27 | INFO | train_inner | {\"epoch\": 16, \"update\": 15.4, \"loss\": \"3.97\", \"nll_loss\": \"2.891\", \"ppl\": \"7.42\", \"wps\": \"1790.8\", \"ups\": \"0.33\", \"wpb\": \"5359.5\", \"bsz\": \"23.4\", \"num_updates\": \"1000\", \"lr\": \"0.00025\", \"gnorm\": \"0.452\", \"loss_scale\": \"64\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"2947\"}\n",
      "2026-01-24 11:46:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:46:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:46:22 | INFO | valid | {\"epoch\": 16, \"valid_loss\": \"4.035\", \"valid_nll_loss\": \"2.755\", \"valid_ppl\": \"6.75\", \"valid_wps\": \"5676.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1039\", \"valid_best_loss\": \"4.035\"}\n",
      "2026-01-24 11:46:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1039 updates\n",
      "2026-01-24 11:46:22 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint16.pt\n",
      "2026-01-24 11:46:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint16.pt\n",
      "2026-01-24 11:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint16.pt (epoch 16 @ 1039 updates, score 4.035) (writing took 3.671450972000457 seconds)\n",
      "2026-01-24 11:46:26 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2026-01-24 11:46:26 | INFO | train | {\"epoch\": 16, \"train_loss\": \"3.94\", \"train_nll_loss\": \"2.853\", \"train_ppl\": \"7.22\", \"train_wps\": \"1835.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1039\", \"train_lr\": \"0.00025975\", \"train_gnorm\": \"0.453\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"3067\"}\n",
      "2026-01-24 11:46:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:46:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:46:26 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2026-01-24 11:46:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:49:14 | INFO | train_inner | {\"epoch\": 17, \"update\": 16.938, \"loss\": \"3.923\", \"nll_loss\": \"2.832\", \"ppl\": \"7.12\", \"wps\": \"1880.9\", \"ups\": \"0.35\", \"wpb\": \"5396.1\", \"bsz\": \"23.6\", \"num_updates\": \"1100\", \"lr\": \"0.000275\", \"gnorm\": \"0.465\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"3234\"}\n",
      "2026-01-24 11:49:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:49:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:49:33 | INFO | valid | {\"epoch\": 17, \"valid_loss\": \"4.015\", \"valid_nll_loss\": \"2.733\", \"valid_ppl\": \"6.65\", \"valid_wps\": \"5675.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1104\", \"valid_best_loss\": \"4.015\"}\n",
      "2026-01-24 11:49:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1104 updates\n",
      "2026-01-24 11:49:33 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint17.pt\n",
      "2026-01-24 11:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint17.pt\n",
      "2026-01-24 11:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint17.pt (epoch 17 @ 1104 updates, score 4.015) (writing took 3.3479346240001178 seconds)\n",
      "2026-01-24 11:49:36 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2026-01-24 11:49:36 | INFO | train | {\"epoch\": 17, \"train_loss\": \"3.912\", \"train_nll_loss\": \"2.82\", \"train_ppl\": \"7.06\", \"train_wps\": \"1838.2\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1104\", \"train_lr\": \"0.000276\", \"train_gnorm\": \"0.466\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"3257\"}\n",
      "2026-01-24 11:49:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:49:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:49:36 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2026-01-24 11:49:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:52:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:52:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:52:43 | INFO | valid | {\"epoch\": 18, \"valid_loss\": \"4.011\", \"valid_nll_loss\": \"2.732\", \"valid_ppl\": \"6.64\", \"valid_wps\": \"5675.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1169\", \"valid_best_loss\": \"4.011\"}\n",
      "2026-01-24 11:52:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1169 updates\n",
      "2026-01-24 11:52:43 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint18.pt\n",
      "2026-01-24 11:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint18.pt\n",
      "2026-01-24 11:52:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint18.pt (epoch 18 @ 1169 updates, score 4.011) (writing took 2.3741163609993237 seconds)\n",
      "2026-01-24 11:52:45 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2026-01-24 11:52:45 | INFO | train | {\"epoch\": 18, \"train_loss\": \"3.885\", \"train_nll_loss\": \"2.789\", \"train_ppl\": \"6.91\", \"train_wps\": \"1848.2\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1169\", \"train_lr\": \"0.00029225\", \"train_gnorm\": \"0.413\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"3446\"}\n",
      "2026-01-24 11:52:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:52:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:52:45 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2026-01-24 11:52:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:54:10 | INFO | train_inner | {\"epoch\": 19, \"update\": 18.477, \"loss\": \"3.88\", \"nll_loss\": \"2.783\", \"ppl\": \"6.88\", \"wps\": \"1805.1\", \"ups\": \"0.34\", \"wpb\": \"5349.5\", \"bsz\": \"23.4\", \"num_updates\": \"1200\", \"lr\": \"0.0003\", \"gnorm\": \"0.411\", \"loss_scale\": \"64\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"3531\"}\n",
      "2026-01-24 11:55:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:55:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:55:52 | INFO | valid | {\"epoch\": 19, \"valid_loss\": \"3.986\", \"valid_nll_loss\": \"2.707\", \"valid_ppl\": \"6.53\", \"valid_wps\": \"5672.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1234\", \"valid_best_loss\": \"3.986\"}\n",
      "2026-01-24 11:55:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1234 updates\n",
      "2026-01-24 11:55:52 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint19.pt\n",
      "2026-01-24 11:55:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint19.pt\n",
      "2026-01-24 11:55:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint19.pt (epoch 19 @ 1234 updates, score 3.986) (writing took 4.2881405299995095 seconds)\n",
      "2026-01-24 11:55:57 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2026-01-24 11:55:57 | INFO | train | {\"epoch\": 19, \"train_loss\": \"3.862\", \"train_nll_loss\": \"2.763\", \"train_ppl\": \"6.79\", \"train_wps\": \"1829.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1234\", \"train_lr\": \"0.0003085\", \"train_gnorm\": \"0.402\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"3637\"}\n",
      "2026-01-24 11:55:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:55:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:55:57 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2026-01-24 11:55:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:58:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 11:58:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:59:03 | INFO | valid | {\"epoch\": 20, \"valid_loss\": \"3.978\", \"valid_nll_loss\": \"2.699\", \"valid_ppl\": \"6.5\", \"valid_wps\": \"5668\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1299\", \"valid_best_loss\": \"3.978\"}\n",
      "2026-01-24 11:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1299 updates\n",
      "2026-01-24 11:59:03 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint20.pt\n",
      "2026-01-24 11:59:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint20.pt\n",
      "2026-01-24 11:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint20.pt (epoch 20 @ 1299 updates, score 3.978) (writing took 3.1796269189999293 seconds)\n",
      "2026-01-24 11:59:06 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2026-01-24 11:59:06 | INFO | train | {\"epoch\": 20, \"train_loss\": \"3.843\", \"train_nll_loss\": \"2.743\", \"train_ppl\": \"6.69\", \"train_wps\": \"1840.3\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1299\", \"train_lr\": \"0.00032475\", \"train_gnorm\": \"0.416\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"3827\"}\n",
      "2026-01-24 11:59:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 11:59:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 11:59:07 | INFO | fairseq.trainer | begin training epoch 21\n",
      "2026-01-24 11:59:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 11:59:10 | INFO | train_inner | {\"epoch\": 21, \"update\": 20.015, \"loss\": \"3.85\", \"nll_loss\": \"2.75\", \"ppl\": \"6.73\", \"wps\": \"1804.3\", \"ups\": \"0.33\", \"wpb\": \"5406.8\", \"bsz\": \"23.4\", \"num_updates\": \"1300\", \"lr\": \"0.000325\", \"gnorm\": \"0.41\", \"loss_scale\": \"64\", \"train_wall\": \"275\", \"gb_free\": \"2.8\", \"wall\": \"3830\"}\n",
      "2026-01-24 12:02:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:02:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:02:13 | INFO | valid | {\"epoch\": 21, \"valid_loss\": \"3.971\", \"valid_nll_loss\": \"2.696\", \"valid_ppl\": \"6.48\", \"valid_wps\": \"5676\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1364\", \"valid_best_loss\": \"3.971\"}\n",
      "2026-01-24 12:02:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1364 updates\n",
      "2026-01-24 12:02:13 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint21.pt\n",
      "2026-01-24 12:02:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint21.pt\n",
      "2026-01-24 12:02:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint21.pt (epoch 21 @ 1364 updates, score 3.971) (writing took 3.6238383580002846 seconds)\n",
      "2026-01-24 12:02:17 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2026-01-24 12:02:17 | INFO | train | {\"epoch\": 21, \"train_loss\": \"3.825\", \"train_nll_loss\": \"2.724\", \"train_ppl\": \"6.6\", \"train_wps\": \"1836\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1364\", \"train_lr\": \"0.000341\", \"train_gnorm\": \"0.417\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"4017\"}\n",
      "2026-01-24 12:02:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:02:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:02:17 | INFO | fairseq.trainer | begin training epoch 22\n",
      "2026-01-24 12:02:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:03:56 | INFO | train_inner | {\"epoch\": 22, \"update\": 21.554, \"loss\": \"3.815\", \"nll_loss\": \"2.712\", \"ppl\": \"6.55\", \"wps\": \"1877.7\", \"ups\": \"0.35\", \"wpb\": \"5376.5\", \"bsz\": \"23.5\", \"num_updates\": \"1400\", \"lr\": \"0.00035\", \"gnorm\": \"0.407\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"4117\"}\n",
      "2026-01-24 12:05:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:05:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:05:24 | INFO | valid | {\"epoch\": 22, \"valid_loss\": \"3.953\", \"valid_nll_loss\": \"2.679\", \"valid_ppl\": \"6.41\", \"valid_wps\": \"5676\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1429\", \"valid_best_loss\": \"3.953\"}\n",
      "2026-01-24 12:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1429 updates\n",
      "2026-01-24 12:05:24 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint22.pt\n",
      "2026-01-24 12:05:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint22.pt\n",
      "2026-01-24 12:05:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint22.pt (epoch 22 @ 1429 updates, score 3.953) (writing took 2.422963536999305 seconds)\n",
      "2026-01-24 12:05:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2026-01-24 12:05:26 | INFO | train | {\"epoch\": 22, \"train_loss\": \"3.803\", \"train_nll_loss\": \"2.7\", \"train_ppl\": \"6.5\", \"train_wps\": \"1847.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1429\", \"train_lr\": \"0.00035725\", \"train_gnorm\": \"0.381\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"4207\"}\n",
      "2026-01-24 12:05:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:05:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:05:26 | INFO | fairseq.trainer | begin training epoch 23\n",
      "2026-01-24 12:05:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:08:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:08:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:08:33 | INFO | valid | {\"epoch\": 23, \"valid_loss\": \"3.94\", \"valid_nll_loss\": \"2.669\", \"valid_ppl\": \"6.36\", \"valid_wps\": \"5676.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1494\", \"valid_best_loss\": \"3.94\"}\n",
      "2026-01-24 12:08:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1494 updates\n",
      "2026-01-24 12:08:33 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint23.pt\n",
      "2026-01-24 12:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint23.pt\n",
      "2026-01-24 12:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint23.pt (epoch 23 @ 1494 updates, score 3.94) (writing took 2.392174963000798 seconds)\n",
      "2026-01-24 12:08:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2026-01-24 12:08:35 | INFO | train | {\"epoch\": 23, \"train_loss\": \"3.785\", \"train_nll_loss\": \"2.68\", \"train_ppl\": \"6.41\", \"train_wps\": \"1847.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1494\", \"train_lr\": \"0.0003735\", \"train_gnorm\": \"0.364\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"4396\"}\n",
      "2026-01-24 12:08:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:08:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:08:35 | INFO | fairseq.trainer | begin training epoch 24\n",
      "2026-01-24 12:08:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:08:52 | INFO | train_inner | {\"epoch\": 24, \"update\": 23.092, \"loss\": \"3.792\", \"nll_loss\": \"2.688\", \"ppl\": \"6.44\", \"wps\": \"1817.2\", \"ups\": \"0.34\", \"wpb\": \"5379.4\", \"bsz\": \"23.4\", \"num_updates\": \"1500\", \"lr\": \"0.000375\", \"gnorm\": \"0.367\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"4413\"}\n",
      "2026-01-24 12:11:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:11:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:11:42 | INFO | valid | {\"epoch\": 24, \"valid_loss\": \"3.935\", \"valid_nll_loss\": \"2.665\", \"valid_ppl\": \"6.34\", \"valid_wps\": \"5671.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1559\", \"valid_best_loss\": \"3.935\"}\n",
      "2026-01-24 12:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1559 updates\n",
      "2026-01-24 12:11:42 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint24.pt\n",
      "2026-01-24 12:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint24.pt\n",
      "2026-01-24 12:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint24.pt (epoch 24 @ 1559 updates, score 3.935) (writing took 2.983906257999479 seconds)\n",
      "2026-01-24 12:11:45 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2026-01-24 12:11:45 | INFO | train | {\"epoch\": 24, \"train_loss\": \"3.77\", \"train_nll_loss\": \"2.665\", \"train_ppl\": \"6.34\", \"train_wps\": \"1841.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1559\", \"train_lr\": \"0.00038975\", \"train_gnorm\": \"0.359\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"4586\"}\n",
      "2026-01-24 12:11:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:11:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:11:45 | INFO | fairseq.trainer | begin training epoch 25\n",
      "2026-01-24 12:11:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:13:38 | INFO | train_inner | {\"epoch\": 25, \"update\": 24.631, \"loss\": \"3.762\", \"nll_loss\": \"2.656\", \"ppl\": \"6.3\", \"wps\": \"1883.4\", \"ups\": \"0.35\", \"wpb\": \"5390.6\", \"bsz\": \"23.4\", \"num_updates\": \"1600\", \"lr\": \"0.0004\", \"gnorm\": \"0.359\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"4699\"}\n",
      "2026-01-24 12:14:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:14:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:14:52 | INFO | valid | {\"epoch\": 25, \"valid_loss\": \"3.917\", \"valid_nll_loss\": \"2.648\", \"valid_ppl\": \"6.27\", \"valid_wps\": \"5677.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1624\", \"valid_best_loss\": \"3.917\"}\n",
      "2026-01-24 12:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1624 updates\n",
      "2026-01-24 12:14:52 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint25.pt\n",
      "2026-01-24 12:14:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint25.pt\n",
      "2026-01-24 12:14:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint25.pt (epoch 25 @ 1624 updates, score 3.917) (writing took 3.3453732810012298 seconds)\n",
      "2026-01-24 12:14:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2026-01-24 12:14:55 | INFO | train | {\"epoch\": 25, \"train_loss\": \"3.751\", \"train_nll_loss\": \"2.645\", \"train_ppl\": \"6.25\", \"train_wps\": \"1838.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1624\", \"train_lr\": \"0.000406\", \"train_gnorm\": \"0.357\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"4776\"}\n",
      "2026-01-24 12:14:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:14:55 | INFO | fairseq.trainer | begin training epoch 26\n",
      "2026-01-24 12:14:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:17:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:17:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:18:02 | INFO | valid | {\"epoch\": 26, \"valid_loss\": \"3.907\", \"valid_nll_loss\": \"2.641\", \"valid_ppl\": \"6.24\", \"valid_wps\": \"5667.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1689\", \"valid_best_loss\": \"3.907\"}\n",
      "2026-01-24 12:18:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1689 updates\n",
      "2026-01-24 12:18:02 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint26.pt\n",
      "2026-01-24 12:18:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint26.pt\n",
      "2026-01-24 12:18:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint26.pt (epoch 26 @ 1689 updates, score 3.907) (writing took 2.819085477000044 seconds)\n",
      "2026-01-24 12:18:05 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2026-01-24 12:18:05 | INFO | train | {\"epoch\": 26, \"train_loss\": \"3.735\", \"train_nll_loss\": \"2.627\", \"train_ppl\": \"6.18\", \"train_wps\": \"1843.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1689\", \"train_lr\": \"0.00042225\", \"train_gnorm\": \"0.345\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"4966\"}\n",
      "2026-01-24 12:18:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:18:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:18:05 | INFO | fairseq.trainer | begin training epoch 27\n",
      "2026-01-24 12:18:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:18:36 | INFO | train_inner | {\"epoch\": 27, \"update\": 26.169, \"loss\": \"3.736\", \"nll_loss\": \"2.628\", \"ppl\": \"6.18\", \"wps\": \"1805.4\", \"ups\": \"0.34\", \"wpb\": \"5368.2\", \"bsz\": \"23.5\", \"num_updates\": \"1700\", \"lr\": \"0.000425\", \"gnorm\": \"0.346\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"4996\"}\n",
      "2026-01-24 12:21:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:21:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:21:12 | INFO | valid | {\"epoch\": 27, \"valid_loss\": \"3.894\", \"valid_nll_loss\": \"2.629\", \"valid_ppl\": \"6.19\", \"valid_wps\": \"5672.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1754\", \"valid_best_loss\": \"3.894\"}\n",
      "2026-01-24 12:21:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1754 updates\n",
      "2026-01-24 12:21:12 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint27.pt\n",
      "2026-01-24 12:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint27.pt\n",
      "2026-01-24 12:21:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint27.pt (epoch 27 @ 1754 updates, score 3.894) (writing took 2.372172180999769 seconds)\n",
      "2026-01-24 12:21:14 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2026-01-24 12:21:14 | INFO | train | {\"epoch\": 27, \"train_loss\": \"3.719\", \"train_nll_loss\": \"2.609\", \"train_ppl\": \"6.1\", \"train_wps\": \"1848\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1754\", \"train_lr\": \"0.0004385\", \"train_gnorm\": \"0.339\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"5155\"}\n",
      "2026-01-24 12:21:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:21:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:21:14 | INFO | fairseq.trainer | begin training epoch 28\n",
      "2026-01-24 12:21:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:23:21 | INFO | train_inner | {\"epoch\": 28, \"update\": 27.708, \"loss\": \"3.712\", \"nll_loss\": \"2.603\", \"ppl\": \"6.07\", \"wps\": \"1883\", \"ups\": \"0.35\", \"wpb\": \"5366.1\", \"bsz\": \"23.5\", \"num_updates\": \"1800\", \"lr\": \"0.00045\", \"gnorm\": \"0.339\", \"loss_scale\": \"64\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"5281\"}\n",
      "2026-01-24 12:24:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:24:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:24:21 | INFO | valid | {\"epoch\": 28, \"valid_loss\": \"3.894\", \"valid_nll_loss\": \"2.63\", \"valid_ppl\": \"6.19\", \"valid_wps\": \"5674.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1819\", \"valid_best_loss\": \"3.894\"}\n",
      "2026-01-24 12:24:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1819 updates\n",
      "2026-01-24 12:24:21 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint28.pt\n",
      "2026-01-24 12:24:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint28.pt\n",
      "2026-01-24 12:24:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint28.pt (epoch 28 @ 1819 updates, score 3.894) (writing took 4.284506984000473 seconds)\n",
      "2026-01-24 12:24:25 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2026-01-24 12:24:25 | INFO | train | {\"epoch\": 28, \"train_loss\": \"3.704\", \"train_nll_loss\": \"2.593\", \"train_ppl\": \"6.04\", \"train_wps\": \"1829.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1819\", \"train_lr\": \"0.00045475\", \"train_gnorm\": \"0.336\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"5346\"}\n",
      "2026-01-24 12:24:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:24:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:24:25 | INFO | fairseq.trainer | begin training epoch 29\n",
      "2026-01-24 12:24:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:27:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:27:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:27:32 | INFO | valid | {\"epoch\": 29, \"valid_loss\": \"3.883\", \"valid_nll_loss\": \"2.619\", \"valid_ppl\": \"6.14\", \"valid_wps\": \"5670.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1884\", \"valid_best_loss\": \"3.883\"}\n",
      "2026-01-24 12:27:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1884 updates\n",
      "2026-01-24 12:27:32 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint29.pt\n",
      "2026-01-24 12:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint29.pt\n",
      "2026-01-24 12:27:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint29.pt (epoch 29 @ 1884 updates, score 3.883) (writing took 3.1788942920011323 seconds)\n",
      "2026-01-24 12:27:35 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2026-01-24 12:27:35 | INFO | train | {\"epoch\": 29, \"train_loss\": \"3.689\", \"train_nll_loss\": \"2.577\", \"train_ppl\": \"5.97\", \"train_wps\": \"1839.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1884\", \"train_lr\": \"0.000471\", \"train_gnorm\": \"0.345\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"5536\"}\n",
      "2026-01-24 12:27:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:27:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:27:35 | INFO | fairseq.trainer | begin training epoch 30\n",
      "2026-01-24 12:27:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:28:18 | INFO | train_inner | {\"epoch\": 30, \"update\": 29.246, \"loss\": \"3.691\", \"nll_loss\": \"2.579\", \"ppl\": \"5.98\", \"wps\": \"1800.8\", \"ups\": \"0.34\", \"wpb\": \"5360.8\", \"bsz\": \"23.4\", \"num_updates\": \"1900\", \"lr\": \"0.000475\", \"gnorm\": \"0.351\", \"loss_scale\": \"64\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"5579\"}\n",
      "2026-01-24 12:30:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:30:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:30:42 | INFO | valid | {\"epoch\": 30, \"valid_loss\": \"3.865\", \"valid_nll_loss\": \"2.6\", \"valid_ppl\": \"6.06\", \"valid_wps\": \"5671.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"1949\", \"valid_best_loss\": \"3.865\"}\n",
      "2026-01-24 12:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1949 updates\n",
      "2026-01-24 12:30:42 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint30.pt\n",
      "2026-01-24 12:30:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint30.pt\n",
      "2026-01-24 12:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint30.pt (epoch 30 @ 1949 updates, score 3.865) (writing took 2.345748759000344 seconds)\n",
      "2026-01-24 12:30:45 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2026-01-24 12:30:45 | INFO | train | {\"epoch\": 30, \"train_loss\": \"3.67\", \"train_nll_loss\": \"2.555\", \"train_ppl\": \"5.88\", \"train_wps\": \"1847\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"1949\", \"train_lr\": \"0.00048725\", \"train_gnorm\": \"0.35\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"5725\"}\n",
      "2026-01-24 12:30:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:30:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:30:45 | INFO | fairseq.trainer | begin training epoch 31\n",
      "2026-01-24 12:30:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:33:05 | INFO | train_inner | {\"epoch\": 31, \"update\": 30.785, \"loss\": \"3.659\", \"nll_loss\": \"2.543\", \"ppl\": \"5.83\", \"wps\": \"1887.3\", \"ups\": \"0.35\", \"wpb\": \"5417.2\", \"bsz\": \"23.8\", \"num_updates\": \"2000\", \"lr\": \"0.0005\", \"gnorm\": \"0.327\", \"loss_scale\": \"64\", \"train_wall\": \"276\", \"gb_free\": \"2.8\", \"wall\": \"5866\"}\n",
      "2026-01-24 12:33:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:33:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:33:51 | INFO | valid | {\"epoch\": 31, \"valid_loss\": \"3.857\", \"valid_nll_loss\": \"2.595\", \"valid_ppl\": \"6.04\", \"valid_wps\": \"5675.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2014\", \"valid_best_loss\": \"3.857\"}\n",
      "2026-01-24 12:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2014 updates\n",
      "2026-01-24 12:33:51 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint31.pt\n",
      "2026-01-24 12:33:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint31.pt\n",
      "2026-01-24 12:33:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint31.pt (epoch 31 @ 2014 updates, score 3.857) (writing took 2.5034497800006648 seconds)\n",
      "2026-01-24 12:33:54 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2026-01-24 12:33:54 | INFO | train | {\"epoch\": 31, \"train_loss\": \"3.658\", \"train_nll_loss\": \"2.543\", \"train_ppl\": \"5.83\", \"train_wps\": \"1846.3\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2014\", \"train_lr\": \"0.0005035\", \"train_gnorm\": \"0.322\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"5915\"}\n",
      "2026-01-24 12:33:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:33:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:33:54 | INFO | fairseq.trainer | begin training epoch 32\n",
      "2026-01-24 12:33:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:36:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:36:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:37:01 | INFO | valid | {\"epoch\": 32, \"valid_loss\": \"3.858\", \"valid_nll_loss\": \"2.59\", \"valid_ppl\": \"6.02\", \"valid_wps\": \"5675\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2079\", \"valid_best_loss\": \"3.857\"}\n",
      "2026-01-24 12:37:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2079 updates\n",
      "2026-01-24 12:37:01 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint32.pt\n",
      "2026-01-24 12:37:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint32.pt\n",
      "2026-01-24 12:37:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint32.pt (epoch 32 @ 2079 updates, score 3.858) (writing took 1.655113306000203 seconds)\n",
      "2026-01-24 12:37:02 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2026-01-24 12:37:02 | INFO | train | {\"epoch\": 32, \"train_loss\": \"3.641\", \"train_nll_loss\": \"2.523\", \"train_ppl\": \"5.75\", \"train_wps\": \"1855.1\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2079\", \"train_lr\": \"0.00051975\", \"train_gnorm\": \"0.32\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"6103\"}\n",
      "2026-01-24 12:37:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:37:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:37:02 | INFO | fairseq.trainer | begin training epoch 33\n",
      "2026-01-24 12:37:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:38:00 | INFO | train_inner | {\"epoch\": 33, \"update\": 32.323, \"loss\": \"3.643\", \"nll_loss\": \"2.526\", \"ppl\": \"5.76\", \"wps\": \"1817.2\", \"ups\": \"0.34\", \"wpb\": \"5361.2\", \"bsz\": \"23.2\", \"num_updates\": \"2100\", \"lr\": \"0.000525\", \"gnorm\": \"0.32\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"6161\"}\n",
      "2026-01-24 12:40:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:40:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:40:09 | INFO | valid | {\"epoch\": 33, \"valid_loss\": \"3.842\", \"valid_nll_loss\": \"2.575\", \"valid_ppl\": \"5.96\", \"valid_wps\": \"5667.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2144\", \"valid_best_loss\": \"3.842\"}\n",
      "2026-01-24 12:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2144 updates\n",
      "2026-01-24 12:40:09 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint33.pt\n",
      "2026-01-24 12:40:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint33.pt\n",
      "2026-01-24 12:40:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint33.pt (epoch 33 @ 2144 updates, score 3.842) (writing took 2.523718716000076 seconds)\n",
      "2026-01-24 12:40:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2026-01-24 12:40:12 | INFO | train | {\"epoch\": 33, \"train_loss\": \"3.626\", \"train_nll_loss\": \"2.507\", \"train_ppl\": \"5.68\", \"train_wps\": \"1846.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2144\", \"train_lr\": \"0.000536\", \"train_gnorm\": \"0.323\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"6292\"}\n",
      "2026-01-24 12:40:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:40:12 | INFO | fairseq.trainer | begin training epoch 34\n",
      "2026-01-24 12:40:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:42:45 | INFO | train_inner | {\"epoch\": 34, \"update\": 33.862, \"loss\": \"3.622\", \"nll_loss\": \"2.502\", \"ppl\": \"5.66\", \"wps\": \"1885.9\", \"ups\": \"0.35\", \"wpb\": \"5366.2\", \"bsz\": \"23.3\", \"num_updates\": \"2200\", \"lr\": \"0.00055\", \"gnorm\": \"0.32\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"6445\"}\n",
      "2026-01-24 12:43:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:43:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:43:19 | INFO | valid | {\"epoch\": 34, \"valid_loss\": \"3.828\", \"valid_nll_loss\": \"2.564\", \"valid_ppl\": \"5.91\", \"valid_wps\": \"5674.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2209\", \"valid_best_loss\": \"3.828\"}\n",
      "2026-01-24 12:43:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2209 updates\n",
      "2026-01-24 12:43:19 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint34.pt\n",
      "2026-01-24 12:43:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint34.pt\n",
      "2026-01-24 12:43:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint34.pt (epoch 34 @ 2209 updates, score 3.828) (writing took 2.649733919000937 seconds)\n",
      "2026-01-24 12:43:21 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2026-01-24 12:43:21 | INFO | train | {\"epoch\": 34, \"train_loss\": \"3.612\", \"train_nll_loss\": \"2.491\", \"train_ppl\": \"5.62\", \"train_wps\": \"1844.5\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2209\", \"train_lr\": \"0.00055225\", \"train_gnorm\": \"0.315\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"6482\"}\n",
      "2026-01-24 12:43:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:43:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:43:21 | INFO | fairseq.trainer | begin training epoch 35\n",
      "2026-01-24 12:43:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:46:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:46:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:46:28 | INFO | valid | {\"epoch\": 35, \"valid_loss\": \"3.825\", \"valid_nll_loss\": \"2.556\", \"valid_ppl\": \"5.88\", \"valid_wps\": \"5674.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2274\", \"valid_best_loss\": \"3.825\"}\n",
      "2026-01-24 12:46:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2274 updates\n",
      "2026-01-24 12:46:28 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint35.pt\n",
      "2026-01-24 12:46:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint35.pt\n",
      "2026-01-24 12:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint35.pt (epoch 35 @ 2274 updates, score 3.825) (writing took 2.4624615290013026 seconds)\n",
      "2026-01-24 12:46:30 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2026-01-24 12:46:30 | INFO | train | {\"epoch\": 35, \"train_loss\": \"3.596\", \"train_nll_loss\": \"2.472\", \"train_ppl\": \"5.55\", \"train_wps\": \"1847.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2274\", \"train_lr\": \"0.0005685\", \"train_gnorm\": \"0.313\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"6671\"}\n",
      "2026-01-24 12:46:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:46:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:46:30 | INFO | fairseq.trainer | begin training epoch 36\n",
      "2026-01-24 12:46:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:47:42 | INFO | train_inner | {\"epoch\": 36, \"update\": 35.4, \"loss\": \"3.59\", \"nll_loss\": \"2.466\", \"ppl\": \"5.52\", \"wps\": \"1815.3\", \"ups\": \"0.34\", \"wpb\": \"5392.5\", \"bsz\": \"23.5\", \"num_updates\": \"2300\", \"lr\": \"0.000575\", \"gnorm\": \"0.312\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"6743\"}\n",
      "2026-01-24 12:49:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:49:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:49:37 | INFO | valid | {\"epoch\": 36, \"valid_loss\": \"3.837\", \"valid_nll_loss\": \"2.571\", \"valid_ppl\": \"5.94\", \"valid_wps\": \"5674.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2339\", \"valid_best_loss\": \"3.825\"}\n",
      "2026-01-24 12:49:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2339 updates\n",
      "2026-01-24 12:49:37 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint36.pt\n",
      "2026-01-24 12:49:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint36.pt\n",
      "2026-01-24 12:49:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint36.pt (epoch 36 @ 2339 updates, score 3.837) (writing took 1.7026346370003012 seconds)\n",
      "2026-01-24 12:49:39 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2026-01-24 12:49:39 | INFO | train | {\"epoch\": 36, \"train_loss\": \"3.583\", \"train_nll_loss\": \"2.457\", \"train_ppl\": \"5.49\", \"train_wps\": \"1854.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2339\", \"train_lr\": \"0.00058475\", \"train_gnorm\": \"0.313\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"6860\"}\n",
      "2026-01-24 12:49:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:49:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:49:39 | INFO | fairseq.trainer | begin training epoch 37\n",
      "2026-01-24 12:49:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:52:27 | INFO | train_inner | {\"epoch\": 37, \"update\": 36.938, \"loss\": \"3.579\", \"nll_loss\": \"2.452\", \"ppl\": \"5.47\", \"wps\": \"1890.6\", \"ups\": \"0.35\", \"wpb\": \"5383.7\", \"bsz\": \"23.5\", \"num_updates\": \"2400\", \"lr\": \"0.0006\", \"gnorm\": \"0.32\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"7027\"}\n",
      "2026-01-24 12:52:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:52:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:52:46 | INFO | valid | {\"epoch\": 37, \"valid_loss\": \"3.819\", \"valid_nll_loss\": \"2.549\", \"valid_ppl\": \"5.85\", \"valid_wps\": \"5671.1\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2404\", \"valid_best_loss\": \"3.819\"}\n",
      "2026-01-24 12:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2404 updates\n",
      "2026-01-24 12:52:46 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint37.pt\n",
      "2026-01-24 12:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint37.pt\n",
      "2026-01-24 12:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint37.pt (epoch 37 @ 2404 updates, score 3.819) (writing took 2.850831914000082 seconds)\n",
      "2026-01-24 12:52:49 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2026-01-24 12:52:49 | INFO | train | {\"epoch\": 37, \"train_loss\": \"3.573\", \"train_nll_loss\": \"2.445\", \"train_ppl\": \"5.45\", \"train_wps\": \"1843.7\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2404\", \"train_lr\": \"0.000601\", \"train_gnorm\": \"0.325\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"7049\"}\n",
      "2026-01-24 12:52:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:52:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:52:49 | INFO | fairseq.trainer | begin training epoch 38\n",
      "2026-01-24 12:52:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:55:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:55:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:55:55 | INFO | valid | {\"epoch\": 38, \"valid_loss\": \"3.816\", \"valid_nll_loss\": \"2.543\", \"valid_ppl\": \"5.83\", \"valid_wps\": \"5673.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2469\", \"valid_best_loss\": \"3.816\"}\n",
      "2026-01-24 12:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2469 updates\n",
      "2026-01-24 12:55:55 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint38.pt\n",
      "2026-01-24 12:55:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint38.pt\n",
      "2026-01-24 12:56:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint38.pt (epoch 38 @ 2469 updates, score 3.816) (writing took 5.695252219000395 seconds)\n",
      "2026-01-24 12:56:01 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2026-01-24 12:56:01 | INFO | train | {\"epoch\": 38, \"train_loss\": \"3.56\", \"train_nll_loss\": \"2.431\", \"train_ppl\": \"5.39\", \"train_wps\": \"1816.2\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2469\", \"train_lr\": \"0.00061725\", \"train_gnorm\": \"0.321\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"7242\"}\n",
      "2026-01-24 12:56:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:56:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:56:01 | INFO | fairseq.trainer | begin training epoch 39\n",
      "2026-01-24 12:56:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 12:57:27 | INFO | train_inner | {\"epoch\": 39, \"update\": 38.477, \"loss\": \"3.555\", \"nll_loss\": \"2.424\", \"ppl\": \"5.37\", \"wps\": \"1792\", \"ups\": \"0.33\", \"wpb\": \"5387.2\", \"bsz\": \"23.4\", \"num_updates\": \"2500\", \"lr\": \"0.000625\", \"gnorm\": \"0.319\", \"loss_scale\": \"128\", \"train_wall\": \"275\", \"gb_free\": \"2.9\", \"wall\": \"7328\"}\n",
      "2026-01-24 12:58:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 12:58:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:59:08 | INFO | valid | {\"epoch\": 39, \"valid_loss\": \"3.8\", \"valid_nll_loss\": \"2.531\", \"valid_ppl\": \"5.78\", \"valid_wps\": \"5672.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2534\", \"valid_best_loss\": \"3.8\"}\n",
      "2026-01-24 12:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2534 updates\n",
      "2026-01-24 12:59:08 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint39.pt\n",
      "2026-01-24 12:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint39.pt\n",
      "2026-01-24 12:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint39.pt (epoch 39 @ 2534 updates, score 3.8) (writing took 2.4822479679987737 seconds)\n",
      "2026-01-24 12:59:10 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2026-01-24 12:59:10 | INFO | train | {\"epoch\": 39, \"train_loss\": \"3.544\", \"train_nll_loss\": \"2.412\", \"train_ppl\": \"5.32\", \"train_wps\": \"1846.5\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2534\", \"train_lr\": \"0.0006335\", \"train_gnorm\": \"0.31\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"7431\"}\n",
      "2026-01-24 12:59:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 12:59:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 12:59:10 | INFO | fairseq.trainer | begin training epoch 40\n",
      "2026-01-24 12:59:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:02:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:02:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:02:17 | INFO | valid | {\"epoch\": 40, \"valid_loss\": \"3.806\", \"valid_nll_loss\": \"2.535\", \"valid_ppl\": \"5.8\", \"valid_wps\": \"5675.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2599\", \"valid_best_loss\": \"3.8\"}\n",
      "2026-01-24 13:02:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2599 updates\n",
      "2026-01-24 13:02:17 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint40.pt\n",
      "2026-01-24 13:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint40.pt\n",
      "2026-01-24 13:02:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint40.pt (epoch 40 @ 2599 updates, score 3.806) (writing took 1.7310936829999264 seconds)\n",
      "2026-01-24 13:02:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2026-01-24 13:02:19 | INFO | train | {\"epoch\": 40, \"train_loss\": \"3.53\", \"train_nll_loss\": \"2.396\", \"train_ppl\": \"5.26\", \"train_wps\": \"1854.6\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2599\", \"train_lr\": \"0.00064975\", \"train_gnorm\": \"0.309\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"7619\"}\n",
      "2026-01-24 13:02:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:02:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:02:19 | INFO | fairseq.trainer | begin training epoch 41\n",
      "2026-01-24 13:02:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:02:22 | INFO | train_inner | {\"epoch\": 41, \"update\": 40.015, \"loss\": \"3.534\", \"nll_loss\": \"2.4\", \"ppl\": \"5.28\", \"wps\": \"1820.6\", \"ups\": \"0.34\", \"wpb\": \"5357.4\", \"bsz\": \"23.6\", \"num_updates\": \"2600\", \"lr\": \"0.00065\", \"gnorm\": \"0.31\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"7622\"}\n",
      "2026-01-24 13:05:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:05:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:05:26 | INFO | valid | {\"epoch\": 41, \"valid_loss\": \"3.8\", \"valid_nll_loss\": \"2.523\", \"valid_ppl\": \"5.75\", \"valid_wps\": \"5675.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2664\", \"valid_best_loss\": \"3.8\"}\n",
      "2026-01-24 13:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2664 updates\n",
      "2026-01-24 13:05:26 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint41.pt\n",
      "2026-01-24 13:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint41.pt\n",
      "2026-01-24 13:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint41.pt (epoch 41 @ 2664 updates, score 3.8) (writing took 4.451901843000087 seconds)\n",
      "2026-01-24 13:05:30 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2026-01-24 13:05:30 | INFO | train | {\"epoch\": 41, \"train_loss\": \"3.519\", \"train_nll_loss\": \"2.383\", \"train_ppl\": \"5.22\", \"train_wps\": \"1827.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2664\", \"train_lr\": \"0.000666\", \"train_gnorm\": \"0.309\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"7811\"}\n",
      "2026-01-24 13:05:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:05:30 | INFO | fairseq.trainer | begin training epoch 42\n",
      "2026-01-24 13:05:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:07:09 | INFO | train_inner | {\"epoch\": 42, \"update\": 41.554, \"loss\": \"3.512\", \"nll_loss\": \"2.375\", \"ppl\": \"5.19\", \"wps\": \"1874.4\", \"ups\": \"0.35\", \"wpb\": \"5380.5\", \"bsz\": \"23.4\", \"num_updates\": \"2700\", \"lr\": \"0.000675\", \"gnorm\": \"0.308\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"7909\"}\n",
      "2026-01-24 13:08:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:08:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:08:37 | INFO | valid | {\"epoch\": 42, \"valid_loss\": \"3.791\", \"valid_nll_loss\": \"2.52\", \"valid_ppl\": \"5.74\", \"valid_wps\": \"5670.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2729\", \"valid_best_loss\": \"3.791\"}\n",
      "2026-01-24 13:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2729 updates\n",
      "2026-01-24 13:08:37 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint42.pt\n",
      "2026-01-24 13:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint42.pt\n",
      "2026-01-24 13:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint42.pt (epoch 42 @ 2729 updates, score 3.791) (writing took 3.751889201001177 seconds)\n",
      "2026-01-24 13:08:41 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2026-01-24 13:08:41 | INFO | train | {\"epoch\": 42, \"train_loss\": \"3.508\", \"train_nll_loss\": \"2.37\", \"train_ppl\": \"5.17\", \"train_wps\": \"1834\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2729\", \"train_lr\": \"0.00068225\", \"train_gnorm\": \"0.307\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"8001\"}\n",
      "2026-01-24 13:08:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:08:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:08:41 | INFO | fairseq.trainer | begin training epoch 43\n",
      "2026-01-24 13:08:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:11:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:11:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:11:48 | INFO | valid | {\"epoch\": 43, \"valid_loss\": \"3.796\", \"valid_nll_loss\": \"2.517\", \"valid_ppl\": \"5.73\", \"valid_wps\": \"5670.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2794\", \"valid_best_loss\": \"3.791\"}\n",
      "2026-01-24 13:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2794 updates\n",
      "2026-01-24 13:11:48 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint43.pt\n",
      "2026-01-24 13:11:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint43.pt\n",
      "2026-01-24 13:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint43.pt (epoch 43 @ 2794 updates, score 3.796) (writing took 1.5460538800016366 seconds)\n",
      "2026-01-24 13:11:49 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2026-01-24 13:11:49 | INFO | train | {\"epoch\": 43, \"train_loss\": \"3.496\", \"train_nll_loss\": \"2.356\", \"train_ppl\": \"5.12\", \"train_wps\": \"1855.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2794\", \"train_lr\": \"0.0006985\", \"train_gnorm\": \"0.307\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"8190\"}\n",
      "2026-01-24 13:11:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:11:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:11:49 | INFO | fairseq.trainer | begin training epoch 44\n",
      "2026-01-24 13:11:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:12:06 | INFO | train_inner | {\"epoch\": 44, \"update\": 43.092, \"loss\": \"3.499\", \"nll_loss\": \"2.36\", \"ppl\": \"5.13\", \"wps\": \"1810.5\", \"ups\": \"0.34\", \"wpb\": \"5374.6\", \"bsz\": \"23.7\", \"num_updates\": \"2800\", \"lr\": \"0.0007\", \"gnorm\": \"0.307\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"8206\"}\n",
      "2026-01-24 13:14:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:14:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:14:56 | INFO | valid | {\"epoch\": 44, \"valid_loss\": \"3.789\", \"valid_nll_loss\": \"2.51\", \"valid_ppl\": \"5.7\", \"valid_wps\": \"5674.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2859\", \"valid_best_loss\": \"3.789\"}\n",
      "2026-01-24 13:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2859 updates\n",
      "2026-01-24 13:14:56 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint44.pt\n",
      "2026-01-24 13:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint44.pt\n",
      "2026-01-24 13:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint44.pt (epoch 44 @ 2859 updates, score 3.789) (writing took 3.7476338079995912 seconds)\n",
      "2026-01-24 13:15:00 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
      "2026-01-24 13:15:00 | INFO | train | {\"epoch\": 44, \"train_loss\": \"3.485\", \"train_nll_loss\": \"2.343\", \"train_ppl\": \"5.07\", \"train_wps\": \"1834.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2859\", \"train_lr\": \"0.00071475\", \"train_gnorm\": \"0.308\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"8380\"}\n",
      "2026-01-24 13:15:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:15:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:15:00 | INFO | fairseq.trainer | begin training epoch 45\n",
      "2026-01-24 13:15:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:16:53 | INFO | train_inner | {\"epoch\": 45, \"update\": 44.631, \"loss\": \"3.479\", \"nll_loss\": \"2.337\", \"ppl\": \"5.05\", \"wps\": \"1877.5\", \"ups\": \"0.35\", \"wpb\": \"5389.2\", \"bsz\": \"23.5\", \"num_updates\": \"2900\", \"lr\": \"0.000725\", \"gnorm\": \"0.307\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"8493\"}\n",
      "2026-01-24 13:17:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:17:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:18:06 | INFO | valid | {\"epoch\": 45, \"valid_loss\": \"3.788\", \"valid_nll_loss\": \"2.507\", \"valid_ppl\": \"5.68\", \"valid_wps\": \"5675.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2924\", \"valid_best_loss\": \"3.788\"}\n",
      "2026-01-24 13:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2924 updates\n",
      "2026-01-24 13:18:06 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint45.pt\n",
      "2026-01-24 13:18:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint45.pt\n",
      "2026-01-24 13:18:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint45.pt (epoch 45 @ 2924 updates, score 3.788) (writing took 2.9001296139995247 seconds)\n",
      "2026-01-24 13:18:09 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2026-01-24 13:18:09 | INFO | train | {\"epoch\": 45, \"train_loss\": \"3.474\", \"train_nll_loss\": \"2.331\", \"train_ppl\": \"5.03\", \"train_wps\": \"1842.3\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2924\", \"train_lr\": \"0.000731\", \"train_gnorm\": \"0.306\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"8570\"}\n",
      "2026-01-24 13:18:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:18:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:18:09 | INFO | fairseq.trainer | begin training epoch 46\n",
      "2026-01-24 13:18:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:21:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:21:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:21:16 | INFO | valid | {\"epoch\": 46, \"valid_loss\": \"3.788\", \"valid_nll_loss\": \"2.508\", \"valid_ppl\": \"5.69\", \"valid_wps\": \"5674.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"2989\", \"valid_best_loss\": \"3.788\"}\n",
      "2026-01-24 13:21:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2989 updates\n",
      "2026-01-24 13:21:16 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint46.pt\n",
      "2026-01-24 13:21:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint46.pt\n",
      "2026-01-24 13:21:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint46.pt (epoch 46 @ 2989 updates, score 3.788) (writing took 2.539291990999118 seconds)\n",
      "2026-01-24 13:21:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2026-01-24 13:21:19 | INFO | train | {\"epoch\": 46, \"train_loss\": \"3.464\", \"train_nll_loss\": \"2.318\", \"train_ppl\": \"4.99\", \"train_wps\": \"1845.8\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"2989\", \"train_lr\": \"0.00074725\", \"train_gnorm\": \"0.306\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"8759\"}\n",
      "2026-01-24 13:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:21:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 13:21:19 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2026-01-24 13:21:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 13:21:50 | INFO | train_inner | {\"epoch\": 47, \"update\": 46.169, \"loss\": \"3.467\", \"nll_loss\": \"2.321\", \"ppl\": \"5\", \"wps\": \"1811.6\", \"ups\": \"0.34\", \"wpb\": \"5385.5\", \"bsz\": \"23.3\", \"num_updates\": \"3000\", \"lr\": \"0.00075\", \"gnorm\": \"0.305\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"8790\"}\n",
      "2026-01-24 13:21:50 | INFO | fairseq_cli.train | Stopping training due to num_updates: 3000 >= max_update: 3000\n",
      "2026-01-24 13:21:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 13:21:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:21:58 | INFO | valid | {\"epoch\": 47, \"valid_loss\": \"3.792\", \"valid_nll_loss\": \"2.511\", \"valid_ppl\": \"5.7\", \"valid_wps\": \"5670.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3000\", \"valid_best_loss\": \"3.788\"}\n",
      "2026-01-24 13:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3000 updates\n",
      "2026-01-24 13:21:58 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint.best_loss_3.7920.pt\n",
      "2026-01-24 13:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint.best_loss_3.7920.pt\n",
      "2026-01-24 13:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint.best_loss_3.7920.pt (epoch 47 @ 3000 updates, score 3.792) (writing took 1.1658846169993922 seconds)\n",
      "2026-01-24 13:22:00 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2026-01-24 13:22:00 | INFO | train | {\"epoch\": 47, \"train_loss\": \"3.429\", \"train_nll_loss\": \"2.278\", \"train_ppl\": \"4.85\", \"train_wps\": \"1489.7\", \"train_ups\": \"0.27\", \"train_wpb\": \"5508\", \"train_bsz\": \"23.6\", \"train_num_updates\": \"3000\", \"train_lr\": \"0.00075\", \"train_gnorm\": \"0.296\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"31\", \"train_gb_free\": \"2.9\", \"train_wall\": \"8800\"}\n",
      "2026-01-24 13:22:00 | INFO | fairseq_cli.train | done training in 8799.4 seconds\n"
     ]
    }
   ],
   "source": [
    "!fairseq-train /home/khanh/Projects/KhoaLuan/data_bin \\\n",
    "    --config-yaml config.yaml \\\n",
    "    --task speech_to_text \\\n",
    "    --arch s2t_transformer_s \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-8 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt \\\n",
    "    --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --max-tokens 2000 \\\n",
    "    --update-freq 8 \\\n",
    "    --max-update 3000 \\\n",
    "    --log-format json --log-interval 100 \\\n",
    "    --save-dir /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official \\\n",
    "    --keep-last-epochs 10 \\\n",
    "    --keep-best-checkpoints 5 \\\n",
    "    --patience 20 \\\n",
    "    --num-workers 4 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f92aab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 13:54:15 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../checkpoints/s2ut_official/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': '/home/khanh/Projects/KhoaLuan/results/test_run'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='speech_to_text', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='valid', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='../checkpoints/s2ut_official/checkpoint_best.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/home/khanh/Projects/KhoaLuan/results/test_run', beam=5, beam_mt=0, nbest=1, max_len_a=0, max_len_b=200, max_len_a_mt=0, max_len_b_mt=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, lenpen_mt=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, arch='wav2vec2', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='speech_to_text'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2026-01-24 13:54:15 | INFO | fairseq.tasks.speech_to_text | dictionary size (dict.txt): 1,004\n",
      "2026-01-24 13:54:16 | INFO | fairseq_cli.generate | loading model(s) from ../checkpoints/s2ut_official/checkpoint_best.pt\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 13:54:17 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 13:54:17 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
      "2026-01-24 13:54:17 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"valid\", n_samples=192, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "  0%|                                                   | 0/191 [00:00<?, ?it/s]2026-01-24 13:54:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 13:54:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 13:56:27 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2026-01-24 13:56:27 | INFO | fairseq_cli.generate | Translated 191 sentences (33,521 tokens) in 129.3s (1.48 sentences/s, 259.32 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-generate /home/khanh/Projects/KhoaLuan/data_bin \\\n",
    "    --config-yaml config.yaml \\\n",
    "    --path ../checkpoints/s2ut_official/checkpoint_best.pt \\\n",
    "    --task speech_to_text \\\n",
    "    --gen-subset valid \\\n",
    "    --beam 5 \\\n",
    "    --batch-size 1 \\\n",
    "    --results-path /home/khanh/Projects/KhoaLuan/results/test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tesst vocoder \n",
    "# Tạo thư mục kết quả\n",
    "!mkdir -p results_audio\n",
    "\n",
    "\n",
    "# Chạy lệnh convert\n",
    "!python examples/speech_to_speech/generate_waveform_from_code.py \\\n",
    "    --in-code-file ../test_unit.txt \\\n",
    "    --vocoder ../vocoder/g_00500000 \\\n",
    "    --vocoder-cfg ../vocoder/config.json \\\n",
    "    --results-path ../results_audio \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
