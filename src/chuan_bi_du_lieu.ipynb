{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e439f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/khanh/Projects/KhoaLuan/fairseq\n"
     ]
    }
   ],
   "source": [
    "%cd fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bf632c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khanh/Projects/KhoaLuan/fairseq'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b140d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  chuẩn hóa giá trị đầu vào cho dataset \n",
    "# import os\n",
    "# import torchaudio\n",
    "# import torch\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # --- CẤU HÌNH ---\n",
    "# # Thư mục chứa file wav gốc (Tiếng Việt)\n",
    "# folder = ['source' , 'target']\n",
    "# file = ['train' , 'test',  'valid']\n",
    "\n",
    "# TARGET_SAMPLE_RATE = 16000\n",
    "# # ----------------\n",
    "\n",
    "# def convert_to_16k(input_path, output_path):\n",
    "#     try:\n",
    "#         # Load audio\n",
    "#         waveform, sample_rate = torchaudio.load(input_path)\n",
    "\n",
    "#         # 1. Chuyển về Mono (nếu đang là Stereo)\n",
    "#         if waveform.shape[0] > 1:\n",
    "#             waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "#         # 2. Resample về 16000Hz (nếu chưa đúng)\n",
    "#         if sample_rate != TARGET_SAMPLE_RATE:\n",
    "#             resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "#             waveform = resampler(waveform)\n",
    "\n",
    "#         # Tạo thư mục cha nếu chưa có\n",
    "#         os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "#         # Lưu file\n",
    "#         torchaudio.save(output_path, waveform, TARGET_SAMPLE_RATE)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Lỗi file {input_path}: {e}\")\n",
    "\n",
    "# def main():\n",
    "#     # Tìm tất cả file wav trong thư mục input (bao gồm cả thư mục con)\n",
    "#     for i in folder:\n",
    "#         for   j in file:\n",
    "#             INPUT_DIR = f\"/home/khanh/Projects/KhoaLuan/my_data/{i}_audio/{j}/\" \n",
    "\n",
    "#             # Thư mục lưu j đã chuẩn hóa (Sẽ tự tạo nếu chưa có)\n",
    "#             OUTPUT_DIR = f\"/home/khanh/Projects/KhoaLuan/my_data/{i}_wav_16k/{j}/\"\n",
    "\n",
    "#             files = glob.glob(os.path.join(INPUT_DIR, \"**/*.wav\"), recursive=True)\n",
    "        \n",
    "#             print(f\"Tìm thấy {len(files)} ({i} in {j}) file audio. Bắt đầu xử lý...\")\n",
    "\n",
    "#             for file_path in tqdm(files):\n",
    "#                 # Tạo đường dẫn output tương ứng\n",
    "#                 relative_path = os.path.relpath(file_path, INPUT_DIR)\n",
    "#                 output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "                \n",
    "#                 convert_to_16k(file_path, output_path)\n",
    "\n",
    "#             print(\"✅ Đã hoàn tất chuẩn hóa dữ liệu về 16kHz!\")\n",
    "#             print(f\"Dữ liệu mới nằm tại: {OUTPUT_DIR}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832be30",
   "metadata": {},
   "source": [
    "tair mhubert để trích xuất đặc chưng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ed9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ cấu trúc thư mục của bạn: /data/en-vi/train/wav_en/ và /data/en-vi/train/wav_vi/\n",
    "# train \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/train --dest ../manifest_temp/train_en --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/train --dest ../manifest_temp/train_vn --ext wav --valid-percent 0\n",
    "# # valid  \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/valid --dest ../manifest_temp/dev_vn --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/valid  --dest ../manifest_temp/dev_en --ext wav --valid-percent 0\n",
    "\n",
    "# #  test \n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/source/test --dest ../manifest_temp/test_en --ext wav --valid-percent 0\n",
    "!python examples/wav2vec/wav2vec_manifest.py ../data/target/test --dest ../manifest_temp/test_vn --ext wav --valid-percent 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088909c",
   "metadata": {},
   "source": [
    "xoa file thừa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9dd167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p  ../manifest_temp/source\n",
    "# source\n",
    "!cp ../manifest_temp/dev_en/train.tsv ../manifest_temp/source/valid.tsv\n",
    "\n",
    "!cp ../manifest_temp/train_en/train.tsv ../manifest_temp/source/train.tsv\n",
    "!cp ../manifest_temp/test_en/train.tsv ../manifest_temp/source/test.tsv\n",
    "# target\n",
    "!mkdir -p ../manifest_temp/target\n",
    "!cp ../manifest_temp/dev_vn/train.tsv ../manifest_temp/target/valid.tsv\n",
    "!cp ../manifest_temp/train_vn/train.tsv ../manifest_temp/target/train.tsv\n",
    "!cp ../manifest_temp/test_vn//train.tsv ../manifest_temp/target/test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908b3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xoa thu muc cu di \n",
    "!rm -r ../manifest_temp/dev_vn/\n",
    "!rm -r ../manifest_temp/dev_en/\n",
    "!rm -r ../manifest_temp/train_en/\n",
    "!rm -r ../manifest_temp/train_vn/\n",
    "!rm -r ../manifest_temp/test_en/\n",
    "!rm -r ../manifest_temp/test_vn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75324451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 09:47:04 | INFO | dump_hubert_feature | Namespace(tsv_dir='/home/khanh/Projects/KhoaLuan/manifest_temp/target/', split='train', ckpt_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, nshard=1, rank=0, feat_dir='/home/khanh/Projects/KhoaLuan/hubert_feats', max_chunk=1600000)\n",
      "2026-01-24 09:47:07 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 09:47:07 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/home/khanh/Projects/KhoaLuan/checkpoints/', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 09:47:07 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "2026-01-24 09:47:08 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/home/khanh/Projects/KhoaLuan/checkpoints/', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 09:47:08 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2026-01-24 09:47:08 | INFO | feature_utils | rank 0 of 1, process 1541 (0-1541) out of 1541\n",
      "100%|███████████████████████████████████████| 1541/1541 [01:57<00:00, 13.11it/s]\n",
      "2026-01-24 09:49:06 | INFO | feature_utils | finished successfully\n"
     ]
    }
   ],
   "source": [
    "# Tạo thư mục chứa feature tạm thời\n",
    "!mkdir -p /home/khanh/Projects/KhoaLuan/hubert_feats\n",
    "\n",
    "# Chạy lệnh dump (Lưu ý thay HUBERT_PATH đúng chỗ bạn vừa tải)\n",
    "!python examples/hubert/simple_kmeans/dump_hubert_feature.py \\\n",
    "    /home/khanh/Projects/KhoaLuan/manifest_temp/target/ \\\n",
    "    train \\\n",
    "    /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt \\\n",
    "    11 \\\n",
    "    1 \\\n",
    "    0 \\\n",
    "    /home/khanh/Projects/KhoaLuan/hubert_feats \\\n",
    "    --max_chunk 1600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7860ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 09:54:35 | INFO | root | Namespace(feat_dir='/home/khanh/Projects/KhoaLuan/hubert_feats', split='train', nshard=1, km_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', n_clusters=1000, seed=0, percent=0.3, init='k-means++', max_iter=100, batch_size=10000, tol=0.0, max_no_improvement=100, n_init=20, reassignment_ratio=0.0)\n",
      "2026-01-24 09:54:36 | INFO | learn_kmeans | sampled 463 utterances, 140779 frames from shard 0/1\n",
      "2026-01-24 09:54:36 | INFO | root | loaded feature with dimension (140779, 768)\n",
      "Init 1/20 with method k-means++\n",
      "Inertia for init 1/20: 872291.125\n",
      "Init 2/20 with method k-means++\n",
      "Inertia for init 2/20: 868583.75\n",
      "Init 3/20 with method k-means++\n",
      "Inertia for init 3/20: 869889.3125\n",
      "Init 4/20 with method k-means++\n",
      "Inertia for init 4/20: 871182.3125\n",
      "Init 5/20 with method k-means++\n",
      "Inertia for init 5/20: 865968.0\n",
      "Init 6/20 with method k-means++\n",
      "Inertia for init 6/20: 873047.5625\n",
      "Init 7/20 with method k-means++\n",
      "Inertia for init 7/20: 866519.875\n",
      "Init 8/20 with method k-means++\n",
      "Inertia for init 8/20: 867980.75\n",
      "Init 9/20 with method k-means++\n",
      "Inertia for init 9/20: 867806.625\n",
      "Init 10/20 with method k-means++\n",
      "Inertia for init 10/20: 866404.75\n",
      "Init 11/20 with method k-means++\n",
      "Inertia for init 11/20: 870044.8125\n",
      "Init 12/20 with method k-means++\n",
      "Inertia for init 12/20: 869333.75\n",
      "Init 13/20 with method k-means++\n",
      "Inertia for init 13/20: 869834.6875\n",
      "Init 14/20 with method k-means++\n",
      "Inertia for init 14/20: 866723.875\n",
      "Init 15/20 with method k-means++\n",
      "Inertia for init 15/20: 871920.4375\n",
      "Init 16/20 with method k-means++\n",
      "Inertia for init 16/20: 870226.0\n",
      "Init 17/20 with method k-means++\n",
      "Inertia for init 17/20: 870458.625\n",
      "Init 18/20 with method k-means++\n",
      "Inertia for init 18/20: 872397.125\n",
      "Init 19/20 with method k-means++\n",
      "Inertia for init 19/20: 868024.125\n",
      "Init 20/20 with method k-means++\n",
      "Inertia for init 20/20: 868517.875\n",
      "Minibatch step 1/1407: mean batch inertia: 28.8148875\n",
      "Minibatch step 2/1407: mean batch inertia: 20.865128125, ewa inertia: 20.865128125\n",
      "Minibatch step 3/1407: mean batch inertia: 19.912671875, ewa inertia: 20.72981682367879\n",
      "Minibatch step 4/1407: mean batch inertia: 19.5360578125, ewa inertia: 20.56022469252681\n",
      "Minibatch step 5/1407: mean batch inertia: 19.4805625, ewa inertia: 20.406841798290863\n",
      "Minibatch step 6/1407: mean batch inertia: 19.241053125, ewa inertia: 20.24122329093316\n",
      "Minibatch step 7/1407: mean batch inertia: 19.080359375, ewa inertia: 20.076304422353367\n",
      "Minibatch step 8/1407: mean batch inertia: 19.1790375, ewa inertia: 19.948833627872137\n",
      "Minibatch step 9/1407: mean batch inertia: 18.8081328125, ewa inertia: 19.786779242963465\n",
      "Minibatch step 10/1407: mean batch inertia: 18.9024765625, ewa inertia: 19.66115022172984\n",
      "Minibatch step 11/1407: mean batch inertia: 19.075821875, ewa inertia: 19.5779951788644\n",
      "Minibatch step 12/1407: mean batch inertia: 18.8865609375, ewa inertia: 19.479766134772284\n",
      "Minibatch step 13/1407: mean batch inertia: 18.848128125, ewa inertia: 19.39003208025143\n",
      "Minibatch step 14/1407: mean batch inertia: 18.816253125, ewa inertia: 19.308517809012415\n",
      "Minibatch step 15/1407: mean batch inertia: 18.7196875, ewa inertia: 19.224865257653924\n",
      "Minibatch step 16/1407: mean batch inertia: 18.644340625, ewa inertia: 19.142392657475785\n",
      "Minibatch step 17/1407: mean batch inertia: 18.769865625, ewa inertia: 19.089469368304623\n",
      "Minibatch step 18/1407: mean batch inertia: 18.7890953125, ewa inertia: 19.04679653753255\n",
      "Minibatch step 19/1407: mean batch inertia: 18.6422765625, ewa inertia: 18.989328150683203\n",
      "Minibatch step 20/1407: mean batch inertia: 18.8255125, ewa inertia: 18.966055576356847\n",
      "Minibatch step 21/1407: mean batch inertia: 18.775475, ewa inertia: 18.938980625887055\n",
      "Minibatch step 22/1407: mean batch inertia: 18.5039203125, ewa inertia: 18.877173506496934\n",
      "Minibatch step 23/1407: mean batch inertia: 18.6425765625, ewa inertia: 18.843845342837756\n",
      "Minibatch step 24/1407: mean batch inertia: 18.64824375, ewa inertia: 18.81605707847666\n",
      "Minibatch step 25/1407: mean batch inertia: 18.6703140625, ewa inertia: 18.79535200446378\n",
      "Minibatch step 26/1407: mean batch inertia: 18.5678359375, ewa inertia: 18.763029790091885\n",
      "Minibatch step 27/1407: mean batch inertia: 18.6391859375, ewa inertia: 18.745435834616405\n",
      "Minibatch step 28/1407: mean batch inertia: 18.4187625, ewa inertia: 18.6990267801177\n",
      "Minibatch step 29/1407: mean batch inertia: 18.549275, ewa inertia: 18.677752198484267\n",
      "Minibatch step 30/1407: mean batch inertia: 18.533690625, ewa inertia: 18.657285999665646\n",
      "Minibatch step 31/1407: mean batch inertia: 18.685246875, ewa inertia: 18.66125827915625\n",
      "Minibatch step 32/1407: mean batch inertia: 18.6299640625, ewa inertia: 18.656812446416335\n",
      "Minibatch step 33/1407: mean batch inertia: 18.500053125, ewa inertia: 18.634542333983273\n",
      "Minibatch step 34/1407: mean batch inertia: 18.699146875, ewa inertia: 18.643720419082964\n",
      "Minibatch step 35/1407: mean batch inertia: 18.6643078125, ewa inertia: 18.64664518018781\n",
      "Minibatch step 36/1407: mean batch inertia: 18.488065625, ewa inertia: 18.624116475089387\n",
      "Minibatch step 37/1407: mean batch inertia: 18.6636796875, ewa inertia: 18.629737047956358\n",
      "Minibatch step 38/1407: mean batch inertia: 18.4091390625, ewa inertia: 18.59839765522211\n",
      "Minibatch step 39/1407: mean batch inertia: 18.671365625, ewa inertia: 18.608763896133873\n",
      "Minibatch step 40/1407: mean batch inertia: 18.529465625, ewa inertia: 18.59749833694452\n",
      "Minibatch step 41/1407: mean batch inertia: 18.4808171875, ewa inertia: 18.580921955435137\n",
      "Minibatch step 42/1407: mean batch inertia: 18.6072046875, ewa inertia: 18.584655828437675\n",
      "Minibatch step 43/1407: mean batch inertia: 18.5351203125, ewa inertia: 18.57761853394447\n",
      "Minibatch step 44/1407: mean batch inertia: 18.5787421875, ewa inertia: 18.577778166499595\n",
      "Minibatch step 45/1407: mean batch inertia: 18.657209375, ewa inertia: 18.58906261152025\n",
      "Minibatch step 46/1407: mean batch inertia: 18.609146875, ewa inertia: 18.591915895151413\n",
      "Minibatch step 47/1407: mean batch inertia: 18.4642859375, ewa inertia: 18.573784064259037\n",
      "Minibatch step 48/1407: mean batch inertia: 18.4289609375, ewa inertia: 18.553209674891363\n",
      "Minibatch step 49/1407: mean batch inertia: 18.55065, ewa inertia: 18.55284603305426\n",
      "Minibatch step 50/1407: mean batch inertia: 18.5163734375, ewa inertia: 18.547664530631433\n",
      "Minibatch step 51/1407: mean batch inertia: 18.4304078125, ewa inertia: 18.53100638059145\n",
      "Minibatch step 52/1407: mean batch inertia: 18.497003125, ewa inertia: 18.5261756865168\n",
      "Minibatch step 53/1407: mean batch inertia: 18.422696875, ewa inertia: 18.51147490351967\n",
      "Minibatch step 54/1407: mean batch inertia: 18.544734375, ewa inertia: 18.516199931432773\n",
      "Minibatch step 55/1407: mean batch inertia: 18.4149125, ewa inertia: 18.501810468237323\n",
      "Minibatch step 56/1407: mean batch inertia: 18.55570625, ewa inertia: 18.509467206660773\n",
      "Minibatch step 57/1407: mean batch inertia: 18.490696875, ewa inertia: 18.50680058758693\n",
      "Minibatch step 58/1407: mean batch inertia: 18.562753125, ewa inertia: 18.514749520306502\n",
      "Minibatch step 59/1407: mean batch inertia: 18.6002, ewa inertia: 18.526889096907368\n",
      "Minibatch step 60/1407: mean batch inertia: 18.356715625, ewa inertia: 18.50271329467589\n",
      "Minibatch step 61/1407: mean batch inertia: 18.491028125, ewa inertia: 18.501053233633712\n",
      "Minibatch step 62/1407: mean batch inertia: 18.4155109375, ewa inertia: 18.488900613072026\n",
      "Minibatch step 63/1407: mean batch inertia: 18.47500625, ewa inertia: 18.486926701568684\n",
      "Minibatch step 64/1407: mean batch inertia: 18.3436625, ewa inertia: 18.466573781897043\n",
      "Minibatch step 65/1407: mean batch inertia: 18.4399375, ewa inertia: 18.46278968161333\n",
      "Minibatch step 66/1407: mean batch inertia: 18.4891484375, ewa inertia: 18.466534354988333\n",
      "Minibatch step 67/1407: mean batch inertia: 18.532121875, ewa inertia: 18.475852087622467\n",
      "Minibatch step 68/1407: mean batch inertia: 18.3428515625, ewa inertia: 18.456957283655644\n",
      "Minibatch step 69/1407: mean batch inertia: 18.4904234375, ewa inertia: 18.461711674029893\n",
      "Minibatch step 70/1407: mean batch inertia: 18.360490625, ewa inertia: 18.447331641492617\n",
      "Minibatch step 71/1407: mean batch inertia: 18.533725, ewa inertia: 18.459605168770267\n",
      "Minibatch step 72/1407: mean batch inertia: 18.4277125, ewa inertia: 18.455074316551162\n",
      "Minibatch step 73/1407: mean batch inertia: 18.5461875, ewa inertia: 18.468018368753015\n",
      "Minibatch step 74/1407: mean batch inertia: 18.495265625, ewa inertia: 18.471889267495303\n",
      "Minibatch step 75/1407: mean batch inertia: 18.390165625, ewa inertia: 18.460279146385016\n",
      "Minibatch step 76/1407: mean batch inertia: 18.4178265625, ewa inertia: 18.454248093126736\n",
      "Minibatch step 77/1407: mean batch inertia: 18.4008796875, ewa inertia: 18.446666276728564\n",
      "Minibatch step 78/1407: mean batch inertia: 18.375203125, ewa inertia: 18.43651381874752\n",
      "Minibatch step 79/1407: mean batch inertia: 18.474284375, ewa inertia: 18.441879716780264\n",
      "Minibatch step 80/1407: mean batch inertia: 18.526171875, ewa inertia: 18.45385473570621\n",
      "Minibatch step 81/1407: mean batch inertia: 18.5760921875, ewa inertia: 18.471220476833327\n",
      "Minibatch step 82/1407: mean batch inertia: 18.5153125, ewa inertia: 18.477484438073088\n",
      "Minibatch step 83/1407: mean batch inertia: 18.52299375, ewa inertia: 18.48394974733959\n",
      "Minibatch step 84/1407: mean batch inertia: 18.5171546875, ewa inertia: 18.48866702822614\n",
      "Minibatch step 85/1407: mean batch inertia: 18.4538953125, ewa inertia: 18.483727162375004\n",
      "Minibatch step 86/1407: mean batch inertia: 18.5260015625, ewa inertia: 18.489732901844388\n",
      "Minibatch step 87/1407: mean batch inertia: 18.384328125, ewa inertia: 18.474758505361308\n",
      "Minibatch step 88/1407: mean batch inertia: 18.335609375, ewa inertia: 18.454990195891025\n",
      "Minibatch step 89/1407: mean batch inertia: 18.45571875, ewa inertia: 18.455093698392652\n",
      "Minibatch step 90/1407: mean batch inertia: 18.40131875, ewa inertia: 18.44745412623856\n",
      "Minibatch step 91/1407: mean batch inertia: 18.4224296875, ewa inertia: 18.44389901347559\n",
      "Minibatch step 92/1407: mean batch inertia: 18.574015625, ewa inertia: 18.46238411242777\n",
      "Minibatch step 93/1407: mean batch inertia: 18.4232578125, ewa inertia: 18.456825609809815\n",
      "Minibatch step 94/1407: mean batch inertia: 18.3615578125, ewa inertia: 18.443291329754434\n",
      "Minibatch step 95/1407: mean batch inertia: 18.507484375, ewa inertia: 18.452410955446375\n",
      "Minibatch step 96/1407: mean batch inertia: 18.3124015625, ewa inertia: 18.43252043222626\n",
      "Minibatch step 97/1407: mean batch inertia: 18.3870390625, ewa inertia: 18.426059092586215\n",
      "Minibatch step 98/1407: mean batch inertia: 18.47054375, ewa inertia: 18.43237883365935\n",
      "Minibatch step 99/1407: mean batch inertia: 18.3605734375, ewa inertia: 18.42217775450615\n",
      "Minibatch step 100/1407: mean batch inertia: 18.4708109375, ewa inertia: 18.42908685849732\n",
      "Minibatch step 101/1407: mean batch inertia: 18.551515625, ewa inertia: 18.446479778869914\n",
      "Minibatch step 102/1407: mean batch inertia: 18.4998609375, ewa inertia: 18.454063407031597\n",
      "Minibatch step 103/1407: mean batch inertia: 18.487934375, ewa inertia: 18.458875307581163\n",
      "Minibatch step 104/1407: mean batch inertia: 18.433140625, ewa inertia: 18.455219293576167\n",
      "Minibatch step 105/1407: mean batch inertia: 18.4877875, ewa inertia: 18.459846116480534\n",
      "Minibatch step 106/1407: mean batch inertia: 18.386278125, ewa inertia: 18.44939463310498\n",
      "Minibatch step 107/1407: mean batch inertia: 18.44635, ewa inertia: 18.448962095371638\n",
      "Minibatch step 108/1407: mean batch inertia: 18.4177921875, ewa inertia: 18.444533922638062\n",
      "Minibatch step 109/1407: mean batch inertia: 18.4182390625, ewa inertia: 18.440798326653113\n",
      "Minibatch step 110/1407: mean batch inertia: 18.45326875, ewa inertia: 18.442569945256164\n",
      "Minibatch step 111/1407: mean batch inertia: 18.37266875, ewa inertia: 18.432639387612156\n",
      "Minibatch step 112/1407: mean batch inertia: 18.4487421875, ewa inertia: 18.4349270420926\n",
      "Minibatch step 113/1407: mean batch inertia: 18.358603125, ewa inertia: 18.424084036396817\n",
      "Minibatch step 114/1407: mean batch inertia: 18.367703125, ewa inertia: 18.416074246455516\n",
      "Minibatch step 115/1407: mean batch inertia: 18.293890625, ewa inertia: 18.39871615276955\n",
      "Minibatch step 116/1407: mean batch inertia: 18.2927703125, ewa inertia: 18.383664889767765\n",
      "Minibatch step 117/1407: mean batch inertia: 18.400521875, ewa inertia: 18.386059688067554\n",
      "Minibatch step 118/1407: mean batch inertia: 18.41995, ewa inertia: 18.39087433672964\n",
      "Minibatch step 119/1407: mean batch inertia: 18.4089390625, ewa inertia: 18.393440713455078\n",
      "Minibatch step 120/1407: mean batch inertia: 18.29528125, ewa inertia: 18.37949562701452\n",
      "Minibatch step 121/1407: mean batch inertia: 18.4490328125, ewa inertia: 18.389374471379554\n",
      "Minibatch step 122/1407: mean batch inertia: 18.4194359375, ewa inertia: 18.393645172632635\n",
      "Minibatch step 123/1407: mean batch inertia: 18.378915625, ewa inertia: 18.391552610104913\n",
      "Minibatch step 124/1407: mean batch inertia: 18.3094359375, ewa inertia: 18.379886652922796\n",
      "Minibatch step 125/1407: mean batch inertia: 18.5587, ewa inertia: 18.405289884500746\n",
      "Minibatch step 126/1407: mean batch inertia: 18.472928125, ewa inertia: 18.414898954041767\n",
      "Minibatch step 127/1407: mean batch inertia: 18.52443125, ewa inertia: 18.430459729145934\n",
      "Minibatch step 128/1407: mean batch inertia: 18.4712609375, ewa inertia: 18.4362561786919\n",
      "Minibatch step 129/1407: mean batch inertia: 18.519615625, ewa inertia: 18.448098691308477\n",
      "Minibatch step 130/1407: mean batch inertia: 18.43300625, ewa inertia: 18.445954574060504\n",
      "Minibatch step 131/1407: mean batch inertia: 18.229221875, ewa inertia: 18.415164305689924\n",
      "Minibatch step 132/1407: mean batch inertia: 18.598190625, ewa inertia: 18.441166055840522\n",
      "Minibatch step 133/1407: mean batch inertia: 18.3565578125, ewa inertia: 18.42914613208139\n",
      "Minibatch step 134/1407: mean batch inertia: 18.5487703125, ewa inertia: 18.446140617152935\n",
      "Minibatch step 135/1407: mean batch inertia: 18.420634375, ewa inertia: 18.44251705668228\n",
      "Minibatch step 136/1407: mean batch inertia: 18.4298015625, ewa inertia: 18.440710621935544\n",
      "Minibatch step 137/1407: mean batch inertia: 18.4424796875, ewa inertia: 18.440961945357117\n",
      "Minibatch step 138/1407: mean batch inertia: 18.2417078125, ewa inertia: 18.412654780581278\n",
      "Minibatch step 139/1407: mean batch inertia: 18.458209375, ewa inertia: 18.419126522933702\n",
      "Minibatch step 140/1407: mean batch inertia: 18.36225, ewa inertia: 18.411046323625033\n",
      "Minibatch step 141/1407: mean batch inertia: 18.544378125, ewa inertia: 18.429988190562803\n",
      "Minibatch step 142/1407: mean batch inertia: 18.40454375, ewa inertia: 18.426373409974254\n",
      "Minibatch step 143/1407: mean batch inertia: 18.3990203125, ewa inertia: 18.42248747483087\n",
      "Minibatch step 144/1407: mean batch inertia: 18.3898375, ewa inertia: 18.41784903544589\n",
      "Minibatch step 145/1407: mean batch inertia: 18.34760625, ewa inertia: 18.4078699495749\n",
      "Minibatch step 146/1407: mean batch inertia: 18.4683609375, ewa inertia: 18.416463640145306\n",
      "Minibatch step 147/1407: mean batch inertia: 18.3988515625, ewa inertia: 18.41396156916288\n",
      "Minibatch step 148/1407: mean batch inertia: 18.248309375, ewa inertia: 18.390428085122124\n",
      "Minibatch step 149/1407: mean batch inertia: 18.483096875, ewa inertia: 18.403593135538074\n",
      "Minibatch step 150/1407: mean batch inertia: 18.23963125, ewa inertia: 18.380299786264302\n",
      "Minibatch step 151/1407: mean batch inertia: 18.336115625, ewa inertia: 18.374022735367255\n",
      "Minibatch step 152/1407: mean batch inertia: 18.4959015625, ewa inertia: 18.391337528254418\n",
      "Minibatch step 153/1407: mean batch inertia: 18.344140625, ewa inertia: 18.384632470255493\n",
      "Minibatch step 154/1407: mean batch inertia: 18.4887390625, ewa inertia: 18.39942243931992\n",
      "Minibatch step 155/1407: mean batch inertia: 18.4140734375, ewa inertia: 18.40150384266984\n",
      "Minibatch step 156/1407: mean batch inertia: 18.3245828125, ewa inertia: 18.39057600772598\n",
      "Minibatch step 157/1407: mean batch inertia: 18.4581546875, ewa inertia: 18.40017661573479\n",
      "Minibatch step 158/1407: mean batch inertia: 18.48539375, ewa inertia: 18.412283041969367\n",
      "Minibatch step 159/1407: mean batch inertia: 18.2969296875, ewa inertia: 18.395895294495382\n",
      "Minibatch step 160/1407: mean batch inertia: 18.35923125, ewa inertia: 18.390686593757295\n",
      "Minibatch step 161/1407: mean batch inertia: 18.3954328125, ewa inertia: 18.391360868333614\n",
      "Minibatch step 162/1407: mean batch inertia: 18.282290625, ewa inertia: 18.375865735028654\n",
      "Minibatch step 163/1407: mean batch inertia: 18.3507046875, ewa inertia: 18.372291214851263\n",
      "Minibatch step 164/1407: mean batch inertia: 18.426028125, ewa inertia: 18.379925383078103\n",
      "Minibatch step 165/1407: mean batch inertia: 18.3770375, ewa inertia: 18.37951511413676\n",
      "Minibatch step 166/1407: mean batch inertia: 18.32401875, ewa inertia: 18.371630987963044\n",
      "Minibatch step 167/1407: mean batch inertia: 18.1515984375, ewa inertia: 18.340371924109792\n",
      "Minibatch step 168/1407: mean batch inertia: 18.2443484375, ewa inertia: 18.32673028657466\n",
      "Minibatch step 169/1407: mean batch inertia: 18.4046140625, ewa inertia: 18.337794894604965\n",
      "Minibatch step 170/1407: mean batch inertia: 18.322459375, ewa inertia: 18.335616244284612\n",
      "Minibatch step 171/1407: mean batch inertia: 18.3583671875, ewa inertia: 18.338848371463953\n",
      "Minibatch step 172/1407: mean batch inertia: 18.450421875, ewa inertia: 18.354699132017444\n",
      "Minibatch step 173/1407: mean batch inertia: 18.4525609375, ewa inertia: 18.368601931489323\n",
      "Minibatch step 174/1407: mean batch inertia: 18.3128203125, ewa inertia: 18.360677280404037\n",
      "Minibatch step 175/1407: mean batch inertia: 18.473575, ewa inertia: 18.376716166552065\n",
      "Minibatch step 176/1407: mean batch inertia: 18.481803125, ewa inertia: 18.39164541196305\n",
      "Minibatch step 177/1407: mean batch inertia: 18.3515, ewa inertia: 18.38594212854736\n",
      "Minibatch step 178/1407: mean batch inertia: 18.5246953125, ewa inertia: 18.405654187639936\n",
      "Minibatch step 179/1407: mean batch inertia: 18.5026671875, ewa inertia: 18.41943640100264\n",
      "Minibatch step 180/1407: mean batch inertia: 18.36843125, ewa inertia: 18.412190321871705\n",
      "Minibatch step 181/1407: mean batch inertia: 18.3346140625, ewa inertia: 18.401169401375654\n",
      "Minibatch step 182/1407: mean batch inertia: 18.2954984375, ewa inertia: 18.38615718886313\n",
      "Minibatch step 183/1407: mean batch inertia: 18.148184375, ewa inertia: 18.35234943011002\n",
      "Minibatch step 184/1407: mean batch inertia: 18.280353125, ewa inertia: 18.342121229355648\n",
      "Minibatch step 185/1407: mean batch inertia: 18.4419234375, ewa inertia: 18.356299693362516\n",
      "Minibatch step 186/1407: mean batch inertia: 18.417703125, ewa inertia: 18.365023010827706\n",
      "Minibatch step 187/1407: mean batch inertia: 18.295328125, ewa inertia: 18.35512176266352\n",
      "Minibatch step 188/1407: mean batch inertia: 18.4271046875, ewa inertia: 18.36534806254084\n",
      "Minibatch step 189/1407: mean batch inertia: 18.35471875, ewa inertia: 18.363838002512306\n",
      "Minibatch step 190/1407: mean batch inertia: 18.258690625, ewa inertia: 18.348900173628614\n",
      "Minibatch step 191/1407: mean batch inertia: 18.550275, ewa inertia: 18.377508616073758\n",
      "Minibatch step 192/1407: mean batch inertia: 18.31469375, ewa inertia: 18.36858478228007\n",
      "Minibatch step 193/1407: mean batch inertia: 18.4812234375, ewa inertia: 18.384586864283186\n",
      "Minibatch step 194/1407: mean batch inertia: 18.253878125, ewa inertia: 18.366017644325353\n",
      "Minibatch step 195/1407: mean batch inertia: 18.4248046875, ewa inertia: 18.374369262903937\n",
      "Minibatch step 196/1407: mean batch inertia: 18.3935546875, ewa inertia: 18.37709485241893\n",
      "Minibatch step 197/1407: mean batch inertia: 18.3144328125, ewa inertia: 18.36819272996987\n",
      "Minibatch step 198/1407: mean batch inertia: 18.3748765625, ewa inertia: 18.36914227287797\n",
      "Minibatch step 199/1407: mean batch inertia: 18.3503984375, ewa inertia: 18.36647941801535\n",
      "Minibatch step 200/1407: mean batch inertia: 18.394575, ewa inertia: 18.370470834691673\n",
      "Minibatch step 201/1407: mean batch inertia: 18.4567625, ewa inertia: 18.382729914860494\n",
      "Minibatch step 202/1407: mean batch inertia: 18.5481328125, ewa inertia: 18.406227982432522\n",
      "Minibatch step 203/1407: mean batch inertia: 18.41330625, ewa inertia: 18.407233561004404\n",
      "Minibatch step 204/1407: mean batch inertia: 18.3306921875, ewa inertia: 18.396359662225542\n",
      "Minibatch step 205/1407: mean batch inertia: 18.4777609375, ewa inertia: 18.407923986032113\n",
      "Minibatch step 206/1407: mean batch inertia: 18.417421875, ewa inertia: 18.409273309653067\n",
      "Minibatch step 207/1407: mean batch inertia: 18.4700765625, ewa inertia: 18.41791136233767\n",
      "Minibatch step 208/1407: mean batch inertia: 18.3764078125, ewa inertia: 18.412015134203322\n",
      "Minibatch step 209/1407: mean batch inertia: 18.4141796875, ewa inertia: 18.41232264284044\n",
      "Minibatch step 210/1407: mean batch inertia: 18.4660140625, ewa inertia: 18.41995034843208\n",
      "Minibatch step 211/1407: mean batch inertia: 18.372371875, ewa inertia: 18.41319108242383\n",
      "Minibatch step 212/1407: mean batch inertia: 18.4021671875, ewa inertia: 18.411624965798765\n",
      "Minibatch step 213/1407: mean batch inertia: 18.498971875, ewa inertia: 18.424033959860594\n",
      "Minibatch step 214/1407: mean batch inertia: 18.480140625, ewa inertia: 18.432004788833375\n",
      "Minibatch step 215/1407: mean batch inertia: 18.4512234375, ewa inertia: 18.43473509834703\n",
      "Minibatch step 216/1407: mean batch inertia: 18.2176203125, ewa inertia: 18.403890548574754\n",
      "Minibatch step 217/1407: mean batch inertia: 18.374046875, ewa inertia: 18.399650788157825\n",
      "Minibatch step 218/1407: mean batch inertia: 18.29171875, ewa inertia: 18.384317354693152\n",
      "Minibatch step 219/1407: mean batch inertia: 18.243834375, ewa inertia: 18.364359551071452\n",
      "Minibatch step 220/1407: mean batch inertia: 18.440753125, ewa inertia: 18.375212452609816\n",
      "Minibatch step 221/1407: mean batch inertia: 18.3553328125, ewa inertia: 18.3723882389275\n",
      "Minibatch step 222/1407: mean batch inertia: 18.48011875, ewa inertia: 18.38769304231896\n",
      "Minibatch step 223/1407: mean batch inertia: 18.520965625, ewa inertia: 18.40662649631541\n",
      "Minibatch step 224/1407: mean batch inertia: 18.3765921875, ewa inertia: 18.402359653182092\n",
      "Minibatch step 225/1407: mean batch inertia: 18.46258125, ewa inertia: 18.410915072533975\n",
      "Minibatch step 226/1407: mean batch inertia: 18.3708734375, ewa inertia: 18.40522653225354\n",
      "Minibatch step 227/1407: mean batch inertia: 18.220884375, ewa inertia: 18.379037846750833\n",
      "Minibatch step 228/1407: mean batch inertia: 18.3597171875, ewa inertia: 18.37629304503882\n",
      "Minibatch step 229/1407: mean batch inertia: 18.4757578125, ewa inertia: 18.390423570320987\n",
      "Minibatch step 230/1407: mean batch inertia: 18.44161875, ewa inertia: 18.39769664599637\n",
      "Minibatch step 231/1407: mean batch inertia: 18.3763265625, ewa inertia: 18.39466069152892\n",
      "Minibatch step 232/1407: mean batch inertia: 18.399321875, ewa inertia: 18.395322885515434\n",
      "Minibatch step 233/1407: mean batch inertia: 18.562434375, ewa inertia: 18.419063685271727\n",
      "Minibatch step 234/1407: mean batch inertia: 18.399771875, ewa inertia: 18.416322982008232\n",
      "Minibatch step 235/1407: mean batch inertia: 18.3220890625, ewa inertia: 18.402935580458546\n",
      "Minibatch step 236/1407: mean batch inertia: 18.3369421875, ewa inertia: 18.393560187226758\n",
      "Minibatch step 237/1407: mean batch inertia: 18.3567203125, ewa inertia: 18.388326507055318\n",
      "Minibatch step 238/1407: mean batch inertia: 18.396359375, ewa inertia: 18.389467701535313\n",
      "Minibatch step 239/1407: mean batch inertia: 18.2696421875, ewa inertia: 18.3724446138758\n",
      "Minibatch step 240/1407: mean batch inertia: 18.3744859375, ewa inertia: 18.37273461581133\n",
      "Minibatch step 241/1407: mean batch inertia: 18.421621875, ewa inertia: 18.37967981529828\n",
      "Minibatch step 242/1407: mean batch inertia: 18.425959375, ewa inertia: 18.386254550303498\n",
      "Minibatch step 243/1407: mean batch inertia: 18.286328125, ewa inertia: 18.37205843930712\n",
      "Minibatch step 244/1407: mean batch inertia: 18.39159375, ewa inertia: 18.374833735612402\n",
      "Minibatch step 245/1407: mean batch inertia: 18.367053125, ewa inertia: 18.37372837823033\n",
      "Minibatch step 246/1407: mean batch inertia: 18.4065015625, ewa inertia: 18.378384321442386\n",
      "Minibatch step 247/1407: mean batch inertia: 18.3023671875, ewa inertia: 18.367584899089437\n",
      "Minibatch step 248/1407: mean batch inertia: 18.3248796875, ewa inertia: 18.36151795611608\n",
      "Minibatch step 249/1407: mean batch inertia: 18.3219609375, ewa inertia: 18.355898263174456\n",
      "Minibatch step 250/1407: mean batch inertia: 18.5070859375, ewa inertia: 18.37737683602934\n",
      "Minibatch step 251/1407: mean batch inertia: 18.2696125, ewa inertia: 18.36206722727393\n",
      "Minibatch step 252/1407: mean batch inertia: 18.4320015625, ewa inertia: 18.372002492968782\n",
      "Minibatch step 253/1407: mean batch inertia: 18.3697234375, ewa inertia: 18.37167871750795\n",
      "Minibatch step 254/1407: mean batch inertia: 18.5222390625, ewa inertia: 18.393068168423145\n",
      "Minibatch step 255/1407: mean batch inertia: 18.4223734375, ewa inertia: 18.397231440063557\n",
      "Minibatch step 256/1407: mean batch inertia: 18.40870625, ewa inertia: 18.398861616215914\n",
      "Minibatch step 257/1407: mean batch inertia: 18.35095, ewa inertia: 18.392055022066756\n",
      "Minibatch step 258/1407: mean batch inertia: 18.28710625, ewa inertia: 18.377145408191666\n",
      "Minibatch step 259/1407: mean batch inertia: 18.50611875, ewa inertia: 18.395468087806428\n",
      "Minibatch step 260/1407: mean batch inertia: 18.150059375, ewa inertia: 18.360603943353176\n",
      "Minibatch step 261/1407: mean batch inertia: 18.3959546875, ewa inertia: 18.365626069244186\n",
      "Minibatch step 262/1407: mean batch inertia: 18.3406828125, ewa inertia: 18.36208248965274\n",
      "Minibatch step 263/1407: mean batch inertia: 18.3260609375, ewa inertia: 18.356965064996857\n",
      "Minibatch step 264/1407: mean batch inertia: 18.2697265625, ewa inertia: 18.344571471802247\n",
      "Minibatch step 265/1407: mean batch inertia: 18.3887515625, ewa inertia: 18.350847944411672\n",
      "Minibatch step 266/1407: mean batch inertia: 18.4695796875, ewa inertia: 18.367715644807795\n",
      "Minibatch step 267/1407: mean batch inertia: 18.328028125, ewa inertia: 18.36207741213159\n",
      "Minibatch step 268/1407: mean batch inertia: 18.5337359375, ewa inertia: 18.386464189425013\n",
      "Converged (lack of improvement in inertia) at step 268/1407\n",
      "2026-01-24 10:15:44 | INFO | learn_kmeans | total intertia: 18.37105\n",
      "2026-01-24 10:15:44 | INFO | learn_kmeans | finished successfully\n"
     ]
    }
   ],
   "source": [
    "!python examples/hubert/simple_kmeans/learn_kmeans.py \\\n",
    "    /home/khanh/Projects/KhoaLuan/hubert_feats \\\n",
    "    train \\\n",
    "    1 \\\n",
    "    /home/khanh/Projects/KhoaLuan/kmeans_model.joblib \\\n",
    "    1000 \\\n",
    "    --percent 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d929dad",
   "metadata": {},
   "source": [
    "Trích xuất đặc trưng (Feature Extraction): Bạn cần dump các vector đặc trưng từ mHuBERT ra để train K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu đoạn này vào file make_dict.py rồi chạy: python make_dict.py\n",
    "# Hoặc chạy trực tiếp trong terminal python\n",
    "\n",
    "path = \"/home/khanh/Projects/KhoaLuan/checkpoints/dict.txt\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    # Tạo 1003 dòng giả\n",
    "    for i in range(1000):\n",
    "        f.write(f\"token_{i} 1\\n\")\n",
    "\n",
    "print(f\"Đã tạo xong dict.txt với 1003 dòng tại {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f22a55",
   "metadata": {},
   "source": [
    "trainign k_means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3e0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31ea12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 10:17:23 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, kmeans_model_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', features_path=None, manifest_path='/home/khanh/Projects/KhoaLuan/manifest_temp/target/train.tsv', out_quantized_file_path='/home/khanh/Projects/KhoaLuan/train.unit', extension='.wav', channel_id=None, hide_fname=False)\n",
      "2026-01-24 10:17:23 | INFO | __main__ | Extracting hubert acoustic features...\n",
      "2026-01-24 10:17:24 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 10:17:24 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 10:17:25 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "100%|███████████████████████████████████████| 1541/1541 [01:57<00:00, 13.07it/s]\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Features extracted for 1541 utterances.\n",
      "\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Dimensionality of representation = 768\n",
      "2026-01-24 10:19:24 | INFO | __main__ | Loading K-means model from /home/khanh/Projects/KhoaLuan/kmeans_model.joblib ...\n",
      "Writing quantized predictions to /home/khanh/Projects/KhoaLuan/train.unit\n"
     ]
    }
   ],
   "source": [
    "#  cho tâp train \n",
    "!PYTHONPATH=. python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /home/khanh/Projects/KhoaLuan/kmeans_model.joblib --acoustic_model_path /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt --layer 11 --manifest_path /home/khanh/Projects/KhoaLuan/manifest_temp/target/train.tsv --out_quantized_file_path /home/khanh/Projects/KhoaLuan/train.unit --extension .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a86868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 10:19:41 | INFO | __main__ | Namespace(feature_type='hubert', acoustic_model_path='/home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt', layer=11, kmeans_model_path='/home/khanh/Projects/KhoaLuan/kmeans_model.joblib', features_path=None, manifest_path='/home/khanh/Projects/KhoaLuan/manifest_temp/target/valid.tsv', out_quantized_file_path='/home/khanh/Projects/KhoaLuan/valid.unit', extension='.wav', channel_id=None, hide_fname=False)\n",
      "2026-01-24 10:19:41 | INFO | __main__ | Extracting hubert acoustic features...\n",
      "2026-01-24 10:19:44 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/khanh/Projects/KhoaLuan/fairseq\n",
      "2026-01-24 10:19:44 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2026-01-24 10:19:44 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "100%|█████████████████████████████████████████| 192/192 [00:15<00:00, 12.39it/s]\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Features extracted for 192 utterances.\n",
      "\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Dimensionality of representation = 768\n",
      "2026-01-24 10:20:01 | INFO | __main__ | Loading K-means model from /home/khanh/Projects/KhoaLuan/kmeans_model.joblib ...\n",
      "Writing quantized predictions to /home/khanh/Projects/KhoaLuan/valid.unit\n"
     ]
    }
   ],
   "source": [
    "#  cho tập valid \n",
    "!PYTHONPATH=. python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /home/khanh/Projects/KhoaLuan/kmeans_model.joblib --acoustic_model_path /home/khanh/Projects/KhoaLuan/checkpoints/mhubert_base_vp_en_es_fr_it3.pt --layer 11 --manifest_path /home/khanh/Projects/KhoaLuan/manifest_temp/target/valid.tsv --out_quantized_file_path /home/khanh/Projects/KhoaLuan/valid.unit --extension .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9b3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file .unit (file này có chứa ID|...) sang thành .txt\n",
    "!mkdir -p ../manifest_temp/train\n",
    "!mkdir -p ../manifest_temp/valid\n",
    "!cp /home/khanh/Projects/KhoaLuan/train.unit /home/khanh/Projects/KhoaLuan/manifest_temp/train/train.txt\n",
    "\n",
    "# Nếu bạn có cả valid thì copy luôn\n",
    "!cp /home/khanh/Projects/KhoaLuan/valid.unit /home/khanh/Projects/KhoaLuan/manifest_temp/valid/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ca9213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khanh/Projects/KhoaLuan/fairseq'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19b4e7",
   "metadata": {},
   "source": [
    "các điều kiện bắt buộc để chạy chương trình này đó là \n",
    "1 : file train và vali phỉa là dạng .txt (chuyển từ unit -> txt)\n",
    "2: chuyển về đúng đinh dạng .wav nữa (chạy lệnh sed -i 's/|/.wav|/' /home/khanh/Projects/KhoaLuan/manifest_temp/train_vn/train.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252dc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  chuyển train.tsv vào trong train \n",
    "!cp ../manifest_temp/source/train.tsv ../manifest_temp/train/train.tsv\n",
    "!cp ../manifest_temp/source/valid.tsv ../manifest_temp/valid/valid.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72134286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sắp xếp lại "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fcd529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n",
      "Processing train\n",
      "100%|█████████████████████████████████████| 1541/1541 [00:00<00:00, 5596.54it/s]\n",
      "Processed 1541 samples\n",
      "Writing manifest to /home/khanh/Projects/KhoaLuan/data_bin/train.tsv...\n"
     ]
    }
   ],
   "source": [
    "# Set đường dẫn gốc cho gọn\n",
    "\n",
    "!PYTHONPATH=. python examples/speech_to_speech/preprocessing/prep_s2ut_data.py --source-dir /home/khanh/Projects/KhoaLuan/data/source --target-dir /home/khanh/Projects/KhoaLuan/manifest_temp/train --data-split train  --output-root /home/khanh/Projects/KhoaLuan/data_bin/ --reduce-unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f34674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manifest...\n",
      "Processing valid\n",
      "100%|███████████████████████████████████████| 192/192 [00:00<00:00, 5358.39it/s]\n",
      "Processed 192 samples\n",
      "Writing manifest to /home/khanh/Projects/KhoaLuan/data_bin/valid.tsv...\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=. python examples/speech_to_speech/preprocessing/prep_s2ut_data.py --source-dir /home/khanh/Projects/KhoaLuan/data/source --target-dir /home/khanh/Projects/KhoaLuan/manifest_temp/valid --data-split valid  --output-root /home/khanh/Projects/KhoaLuan/data_bin/ --reduce-unit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f0bc837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tạo /home/khanh/Projects/KhoaLuan/data_bin/dict.txt...\n",
      "✅ Đã tạo xong! File chuẩn phải có 1000 dòng.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn file dict (sửa lại nếu cần)\n",
    "output_file = '/home/khanh/Projects/KhoaLuan/data_bin/dict.txt'\n",
    "\n",
    "print(f\"Đang tạo {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # mHuBERT dùng 1000 clusters -> chạy từ 0 đến 999\n",
    "    for i in range(1000):\n",
    "        # Cấu trúc chuẩn: <Tên Unit> <Khoảng trắng> <Tần suất giả>\n",
    "        f.write(f\"{i} 1\\n\")\n",
    "\n",
    "print(\"✅ Đã tạo xong! File chuẩn phải có 1000 dòng.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a17eec",
   "metadata": {},
   "source": [
    "lệnh train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c5aa080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': 5, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 20, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_text', num_workers=4, skip_invalid_size_inputs_valid_test=False, max_tokens=2000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=2000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2t_transformer_s', max_epoch=0, max_update=100000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[8], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=20, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, decoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_normalize_before=True, decoder_learned_pos=False, attention_dropout=0.3, activation_dropout=0.3, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, quant_noise_pq=0, _name='s2t_transformer_s'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='speech_to_text', num_workers=4, skip_invalid_size_inputs_valid_test=False, max_tokens=2000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=2000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2t_transformer_s', max_epoch=0, max_update=100000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[8], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, keep_best_checkpoints=5, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=20, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, decoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_normalize_before=True, decoder_learned_pos=False, attention_dropout=0.3, activation_dropout=0.3, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, quant_noise_pq=0, _name='speech_to_text'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2026-01-24 15:03:17 | INFO | fairseq.tasks.speech_to_text | dictionary size (dict.txt): 1,004\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | S2TTransformerModel(\n",
      "  (encoder): S2TTransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (subsample): Conv1dSubsampler(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoderScriptable(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(1004, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=1004, bias=False)\n",
      "  )\n",
      ")\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | task: SpeechToTextTask\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | model: S2TTransformerModel\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | num. shared model params: 27,233,280 (num. trained: 27,233,280)\n",
      "2026-01-24 15:03:17 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2026-01-24 15:03:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 15:03:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 15:03:17 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 15:03:17 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
      "2026-01-24 15:03:17 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"valid\", n_samples=192, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 15:03:18 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2026-01-24 15:03:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2026-01-24 15:03:18 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 3.631 GB ; name = NVIDIA GeForce GTX 1650 with Max-Q Design\n",
      "2026-01-24 15:03:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2026-01-24 15:03:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2026-01-24 15:03:18 | INFO | fairseq_cli.train | max tokens per device = 2000 and max sentences per device = None\n",
      "2026-01-24 15:03:18 | INFO | fairseq.trainer | Preparing to load checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint_last.pt\n",
      "2026-01-24 15:03:19 | INFO | fairseq.trainer | Loaded checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint_last.pt (epoch 47 @ 3000 updates)\n",
      "2026-01-24 15:03:19 | INFO | fairseq.trainer | loading train data for epoch 47\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 15:03:19 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 15:03:19 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
      "2026-01-24 15:03:19 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"train\", n_samples=1_541, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 47\n",
      "2026-01-24 15:03:19 | WARNING | fairseq.tasks.fairseq_task | 11 samples have invalid sizes and will be skipped, max_positions=(2000, 1024), first few sample ids=[714, 1387, 757, 822, 571, 892, 147, 526, 889, 904]\n",
      "2026-01-24 15:03:19 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 15:03:19 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2026-01-24 15:03:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:03:19 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2026-01-24 15:03:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2026-01-24 15:05:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:05:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:05:56 | INFO | valid | {\"epoch\": 47, \"valid_loss\": \"3.765\", \"valid_nll_loss\": \"2.482\", \"valid_ppl\": \"5.59\", \"valid_wps\": \"5625.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3054\", \"valid_best_loss\": \"3.765\"}\n",
      "2026-01-24 15:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3054 updates\n",
      "2026-01-24 15:05:56 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint47.pt\n",
      "2026-01-24 15:05:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint47.pt\n",
      "2026-01-24 15:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint47.pt (epoch 47 @ 3054 updates, score 3.765) (writing took 2.1725491260003764 seconds)\n",
      "2026-01-24 15:05:58 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2026-01-24 15:05:58 | INFO | train | {\"epoch\": 47, \"train_loss\": \"3.424\", \"train_nll_loss\": \"2.272\", \"train_ppl\": \"4.83\", \"train_wps\": \"1757.5\", \"train_ups\": \"0.33\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3054\", \"train_lr\": \"0.0003054\", \"train_gnorm\": \"0.299\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"179\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:05:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:05:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:05:58 | INFO | fairseq.trainer | begin training epoch 48\n",
      "2026-01-24 15:05:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:08:04 | INFO | train_inner | {\"epoch\": 48, \"update\": 47.708, \"loss\": \"3.408\", \"nll_loss\": \"2.254\", \"ppl\": \"4.77\", \"wps\": \"1822.8\", \"ups\": \"0.34\", \"wpb\": \"5365.2\", \"bsz\": \"23.4\", \"num_updates\": \"3100\", \"lr\": \"0.00031\", \"gnorm\": \"0.298\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:08:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:08:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:09:05 | INFO | valid | {\"epoch\": 48, \"valid_loss\": \"3.761\", \"valid_nll_loss\": \"2.478\", \"valid_ppl\": \"5.57\", \"valid_wps\": \"5673.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3119\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3119 updates\n",
      "2026-01-24 15:09:05 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint48.pt\n",
      "2026-01-24 15:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint48.pt\n",
      "2026-01-24 15:09:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint48.pt (epoch 48 @ 3119 updates, score 3.761) (writing took 2.2352496040002734 seconds)\n",
      "2026-01-24 15:09:07 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2026-01-24 15:09:07 | INFO | train | {\"epoch\": 48, \"train_loss\": \"3.397\", \"train_nll_loss\": \"2.242\", \"train_ppl\": \"4.73\", \"train_wps\": \"1848.7\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3119\", \"train_lr\": \"0.0003119\", \"train_gnorm\": \"0.298\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:09:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:09:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:09:07 | INFO | fairseq.trainer | begin training epoch 49\n",
      "2026-01-24 15:09:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:12:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:12:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:12:14 | INFO | valid | {\"epoch\": 49, \"valid_loss\": \"3.763\", \"valid_nll_loss\": \"2.476\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5664.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3184\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3184 updates\n",
      "2026-01-24 15:12:14 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint49.pt\n",
      "2026-01-24 15:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint49.pt\n",
      "2026-01-24 15:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint49.pt (epoch 49 @ 3184 updates, score 3.763) (writing took 1.512544033001177 seconds)\n",
      "2026-01-24 15:12:16 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2026-01-24 15:12:16 | INFO | train | {\"epoch\": 49, \"train_loss\": \"3.388\", \"train_nll_loss\": \"2.23\", \"train_ppl\": \"4.69\", \"train_wps\": \"1855.2\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3184\", \"train_lr\": \"0.0003184\", \"train_gnorm\": \"0.307\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:12:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:12:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:12:16 | INFO | fairseq.trainer | begin training epoch 50\n",
      "2026-01-24 15:12:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:13:00 | INFO | train_inner | {\"epoch\": 50, \"update\": 49.246, \"loss\": \"3.389\", \"nll_loss\": \"2.232\", \"ppl\": \"4.7\", \"wps\": \"1822\", \"ups\": \"0.34\", \"wpb\": \"5376.4\", \"bsz\": \"23.4\", \"num_updates\": \"3200\", \"lr\": \"0.00032\", \"gnorm\": \"0.305\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:15:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:15:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:15:22 | INFO | valid | {\"epoch\": 50, \"valid_loss\": \"3.765\", \"valid_nll_loss\": \"2.48\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5672.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3249\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:15:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3249 updates\n",
      "2026-01-24 15:15:22 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint50.pt\n",
      "2026-01-24 15:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint50.pt\n",
      "2026-01-24 15:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint50.pt (epoch 50 @ 3249 updates, score 3.765) (writing took 1.9585816449980484 seconds)\n",
      "2026-01-24 15:15:24 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2026-01-24 15:15:24 | INFO | train | {\"epoch\": 50, \"train_loss\": \"3.38\", \"train_nll_loss\": \"2.221\", \"train_ppl\": \"4.66\", \"train_wps\": \"1852.4\", \"train_ups\": \"0.34\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3249\", \"train_lr\": \"0.0003249\", \"train_gnorm\": \"0.302\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:15:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:15:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:15:24 | INFO | fairseq.trainer | begin training epoch 51\n",
      "2026-01-24 15:15:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:17:43 | INFO | train_inner | {\"epoch\": 51, \"update\": 50.785, \"loss\": \"3.378\", \"nll_loss\": \"2.219\", \"ppl\": \"4.66\", \"wps\": \"1889\", \"ups\": \"0.35\", \"wpb\": \"5362.1\", \"bsz\": \"23.3\", \"num_updates\": \"3300\", \"lr\": \"0.00033\", \"gnorm\": \"0.311\", \"clip\": \"1\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:18:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:18:31 | INFO | valid | {\"epoch\": 51, \"valid_loss\": \"3.764\", \"valid_nll_loss\": \"2.476\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5678.2\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3314\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3314 updates\n",
      "2026-01-24 15:18:31 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint51.pt\n",
      "2026-01-24 15:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint51.pt\n",
      "2026-01-24 15:18:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint51.pt (epoch 51 @ 3314 updates, score 3.764) (writing took 1.5449976269992476 seconds)\n",
      "2026-01-24 15:18:33 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
      "2026-01-24 15:18:33 | INFO | train | {\"epoch\": 51, \"train_loss\": \"3.373\", \"train_nll_loss\": \"2.213\", \"train_ppl\": \"4.64\", \"train_wps\": \"1855.7\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3314\", \"train_lr\": \"0.0003314\", \"train_gnorm\": \"0.315\", \"train_clip\": \"1.5\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:18:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:18:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:18:33 | INFO | fairseq.trainer | begin training epoch 52\n",
      "2026-01-24 15:18:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:21:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:21:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:21:39 | INFO | valid | {\"epoch\": 52, \"valid_loss\": \"3.763\", \"valid_nll_loss\": \"2.476\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5672.9\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3379\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3379 updates\n",
      "2026-01-24 15:21:39 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint52.pt\n",
      "2026-01-24 15:21:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint52.pt\n",
      "2026-01-24 15:21:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint52.pt (epoch 52 @ 3379 updates, score 3.763) (writing took 1.5253277609990619 seconds)\n",
      "2026-01-24 15:21:41 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
      "2026-01-24 15:21:41 | INFO | train | {\"epoch\": 52, \"train_loss\": \"3.363\", \"train_nll_loss\": \"2.201\", \"train_ppl\": \"4.6\", \"train_wps\": \"1856.6\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3379\", \"train_lr\": \"0.0003379\", \"train_gnorm\": \"0.305\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:21:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:21:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:21:41 | INFO | fairseq.trainer | begin training epoch 53\n",
      "2026-01-24 15:21:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:22:40 | INFO | train_inner | {\"epoch\": 53, \"update\": 52.323, \"loss\": \"3.359\", \"nll_loss\": \"2.197\", \"ppl\": \"4.58\", \"wps\": \"1826.9\", \"ups\": \"0.34\", \"wpb\": \"5413.5\", \"bsz\": \"23.9\", \"num_updates\": \"3400\", \"lr\": \"0.00034\", \"gnorm\": \"0.305\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"276\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:24:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:24:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:24:48 | INFO | valid | {\"epoch\": 53, \"valid_loss\": \"3.77\", \"valid_nll_loss\": \"2.479\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5663.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3444\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 3444 updates\n",
      "2026-01-24 15:24:48 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint53.pt\n",
      "2026-01-24 15:24:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint53.pt\n",
      "2026-01-24 15:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint53.pt (epoch 53 @ 3444 updates, score 3.77) (writing took 1.170234328998049 seconds)\n",
      "2026-01-24 15:24:49 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
      "2026-01-24 15:24:49 | INFO | train | {\"epoch\": 53, \"train_loss\": \"3.358\", \"train_nll_loss\": \"2.195\", \"train_ppl\": \"4.58\", \"train_wps\": \"1860.5\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3444\", \"train_lr\": \"0.0003444\", \"train_gnorm\": \"0.308\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:24:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:24:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:24:49 | INFO | fairseq.trainer | begin training epoch 54\n",
      "2026-01-24 15:24:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:27:23 | INFO | train_inner | {\"epoch\": 54, \"update\": 53.862, \"loss\": \"3.361\", \"nll_loss\": \"2.198\", \"ppl\": \"4.59\", \"wps\": \"1898.7\", \"ups\": \"0.35\", \"wpb\": \"5371.9\", \"bsz\": \"22.9\", \"num_updates\": \"3500\", \"lr\": \"0.00035\", \"gnorm\": \"0.31\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"0\"}\n",
      "2026-01-24 15:27:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:27:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:27:56 | INFO | valid | {\"epoch\": 54, \"valid_loss\": \"3.771\", \"valid_nll_loss\": \"2.478\", \"valid_ppl\": \"5.57\", \"valid_wps\": \"5672.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3509\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 3509 updates\n",
      "2026-01-24 15:27:56 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint54.pt\n",
      "2026-01-24 15:27:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint54.pt\n",
      "2026-01-24 15:27:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint54.pt (epoch 54 @ 3509 updates, score 3.771) (writing took 1.1592814969990286 seconds)\n",
      "2026-01-24 15:27:57 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
      "2026-01-24 15:27:57 | INFO | train | {\"epoch\": 54, \"train_loss\": \"3.35\", \"train_nll_loss\": \"2.186\", \"train_ppl\": \"4.55\", \"train_wps\": \"1861\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3509\", \"train_lr\": \"0.0003509\", \"train_gnorm\": \"0.31\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:27:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:27:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:27:57 | INFO | fairseq.trainer | begin training epoch 55\n",
      "2026-01-24 15:27:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:30:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:30:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:31:03 | INFO | valid | {\"epoch\": 55, \"valid_loss\": \"3.768\", \"valid_nll_loss\": \"2.473\", \"valid_ppl\": \"5.55\", \"valid_wps\": \"5668.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3574\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:31:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 3574 updates\n",
      "2026-01-24 15:31:03 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint55.pt\n",
      "2026-01-24 15:31:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint55.pt\n",
      "2026-01-24 15:31:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint55.pt (epoch 55 @ 3574 updates, score 3.768) (writing took 1.1861449360003462 seconds)\n",
      "2026-01-24 15:31:05 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
      "2026-01-24 15:31:05 | INFO | train | {\"epoch\": 55, \"train_loss\": \"3.341\", \"train_nll_loss\": \"2.174\", \"train_ppl\": \"4.51\", \"train_wps\": \"1860.4\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3574\", \"train_lr\": \"0.0003574\", \"train_gnorm\": \"0.312\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:31:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:31:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:31:05 | INFO | fairseq.trainer | begin training epoch 56\n",
      "2026-01-24 15:31:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:32:16 | INFO | train_inner | {\"epoch\": 56, \"update\": 55.4, \"loss\": \"3.333\", \"nll_loss\": \"2.166\", \"ppl\": \"4.49\", \"wps\": \"1829.8\", \"ups\": \"0.34\", \"wpb\": \"5367.6\", \"bsz\": \"23.9\", \"num_updates\": \"3600\", \"lr\": \"0.00036\", \"gnorm\": \"0.312\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"0\"}\n",
      "2026-01-24 15:34:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:34:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:34:11 | INFO | valid | {\"epoch\": 56, \"valid_loss\": \"3.767\", \"valid_nll_loss\": \"2.474\", \"valid_ppl\": \"5.55\", \"valid_wps\": \"5661.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3639\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:34:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 3639 updates\n",
      "2026-01-24 15:34:11 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint56.pt\n",
      "2026-01-24 15:34:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint56.pt\n",
      "2026-01-24 15:34:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint56.pt (epoch 56 @ 3639 updates, score 3.767) (writing took 1.185776515998441 seconds)\n",
      "2026-01-24 15:34:13 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
      "2026-01-24 15:34:13 | INFO | train | {\"epoch\": 56, \"train_loss\": \"3.335\", \"train_nll_loss\": \"2.167\", \"train_ppl\": \"4.49\", \"train_wps\": \"1859.9\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3639\", \"train_lr\": \"0.0003639\", \"train_gnorm\": \"0.319\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:34:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:34:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:34:13 | INFO | fairseq.trainer | begin training epoch 57\n",
      "2026-01-24 15:34:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:37:00 | INFO | train_inner | {\"epoch\": 57, \"update\": 56.938, \"loss\": \"3.335\", \"nll_loss\": \"2.167\", \"ppl\": \"4.49\", \"wps\": \"1892.8\", \"ups\": \"0.35\", \"wpb\": \"5378.3\", \"bsz\": \"23.4\", \"num_updates\": \"3700\", \"lr\": \"0.00037\", \"gnorm\": \"0.319\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:37:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:37:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:37:19 | INFO | valid | {\"epoch\": 57, \"valid_loss\": \"3.771\", \"valid_nll_loss\": \"2.473\", \"valid_ppl\": \"5.55\", \"valid_wps\": \"5668.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3704\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 3704 updates\n",
      "2026-01-24 15:37:19 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint57.pt\n",
      "2026-01-24 15:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint57.pt\n",
      "2026-01-24 15:37:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint57.pt (epoch 57 @ 3704 updates, score 3.771) (writing took 1.1651443660011864 seconds)\n",
      "2026-01-24 15:37:20 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
      "2026-01-24 15:37:20 | INFO | train | {\"epoch\": 57, \"train_loss\": \"3.33\", \"train_nll_loss\": \"2.161\", \"train_ppl\": \"4.47\", \"train_wps\": \"1860.3\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3704\", \"train_lr\": \"0.0003704\", \"train_gnorm\": \"0.315\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:37:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:37:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:37:21 | INFO | fairseq.trainer | begin training epoch 58\n",
      "2026-01-24 15:37:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:40:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:40:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:40:27 | INFO | valid | {\"epoch\": 58, \"valid_loss\": \"3.768\", \"valid_nll_loss\": \"2.475\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5661.5\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3769\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 3769 updates\n",
      "2026-01-24 15:40:27 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint58.pt\n",
      "2026-01-24 15:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint58.pt\n",
      "2026-01-24 15:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint58.pt (epoch 58 @ 3769 updates, score 3.768) (writing took 1.1934260050002194 seconds)\n",
      "2026-01-24 15:40:28 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
      "2026-01-24 15:40:28 | INFO | train | {\"epoch\": 58, \"train_loss\": \"3.324\", \"train_nll_loss\": \"2.154\", \"train_ppl\": \"4.45\", \"train_wps\": \"1859.9\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3769\", \"train_lr\": \"0.0003769\", \"train_gnorm\": \"0.317\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:40:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:40:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:40:28 | INFO | fairseq.trainer | begin training epoch 59\n",
      "2026-01-24 15:40:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:41:54 | INFO | train_inner | {\"epoch\": 59, \"update\": 58.477, \"loss\": \"3.32\", \"nll_loss\": \"2.149\", \"ppl\": \"4.44\", \"wps\": \"1832.1\", \"ups\": \"0.34\", \"wpb\": \"5387.1\", \"bsz\": \"23.6\", \"num_updates\": \"3800\", \"lr\": \"0.00038\", \"gnorm\": \"0.316\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:43:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:43:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:43:35 | INFO | valid | {\"epoch\": 59, \"valid_loss\": \"3.779\", \"valid_nll_loss\": \"2.479\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5664.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3834\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 3834 updates\n",
      "2026-01-24 15:43:35 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint59.pt\n",
      "2026-01-24 15:43:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint59.pt\n",
      "2026-01-24 15:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint59.pt (epoch 59 @ 3834 updates, score 3.779) (writing took 1.1735902930013253 seconds)\n",
      "2026-01-24 15:43:36 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
      "2026-01-24 15:43:36 | INFO | train | {\"epoch\": 59, \"train_loss\": \"3.317\", \"train_nll_loss\": \"2.145\", \"train_ppl\": \"4.42\", \"train_wps\": \"1859.9\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3834\", \"train_lr\": \"0.0003834\", \"train_gnorm\": \"0.318\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:43:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:43:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:43:36 | INFO | fairseq.trainer | begin training epoch 60\n",
      "2026-01-24 15:43:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:46:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:46:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:46:43 | INFO | valid | {\"epoch\": 60, \"valid_loss\": \"3.777\", \"valid_nll_loss\": \"2.476\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5663.6\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3899\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:46:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 3899 updates\n",
      "2026-01-24 15:46:43 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint60.pt\n",
      "2026-01-24 15:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint60.pt\n",
      "2026-01-24 15:46:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint60.pt (epoch 60 @ 3899 updates, score 3.777) (writing took 1.1798132469994016 seconds)\n",
      "2026-01-24 15:46:44 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
      "2026-01-24 15:46:44 | INFO | train | {\"epoch\": 60, \"train_loss\": \"3.312\", \"train_nll_loss\": \"2.138\", \"train_ppl\": \"4.4\", \"train_wps\": \"1859.7\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3899\", \"train_lr\": \"0.0003899\", \"train_gnorm\": \"0.32\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:46:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:46:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:46:44 | INFO | fairseq.trainer | begin training epoch 61\n",
      "2026-01-24 15:46:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:46:47 | INFO | train_inner | {\"epoch\": 61, \"update\": 60.015, \"loss\": \"3.317\", \"nll_loss\": \"2.144\", \"ppl\": \"4.42\", \"wps\": \"1831.9\", \"ups\": \"0.34\", \"wpb\": \"5367.6\", \"bsz\": \"23.2\", \"num_updates\": \"3900\", \"lr\": \"0.00039\", \"gnorm\": \"0.32\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 15:49:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:49:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:49:51 | INFO | valid | {\"epoch\": 61, \"valid_loss\": \"3.776\", \"valid_nll_loss\": \"2.475\", \"valid_ppl\": \"5.56\", \"valid_wps\": \"5664.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"3964\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:49:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3964 updates\n",
      "2026-01-24 15:49:51 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint61.pt\n",
      "2026-01-24 15:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint61.pt\n",
      "2026-01-24 15:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint61.pt (epoch 61 @ 3964 updates, score 3.776) (writing took 1.1641061959999206 seconds)\n",
      "2026-01-24 15:49:52 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
      "2026-01-24 15:49:52 | INFO | train | {\"epoch\": 61, \"train_loss\": \"3.305\", \"train_nll_loss\": \"2.129\", \"train_ppl\": \"4.38\", \"train_wps\": \"1860.4\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"3964\", \"train_lr\": \"0.0003964\", \"train_gnorm\": \"0.324\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:49:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:49:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:49:52 | INFO | fairseq.trainer | begin training epoch 62\n",
      "2026-01-24 15:49:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:51:31 | INFO | train_inner | {\"epoch\": 62, \"update\": 61.554, \"loss\": \"3.296\", \"nll_loss\": \"2.119\", \"ppl\": \"4.34\", \"wps\": \"1893.8\", \"ups\": \"0.35\", \"wpb\": \"5370.2\", \"bsz\": \"23.7\", \"num_updates\": \"4000\", \"lr\": \"0.0004\", \"gnorm\": \"0.323\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"0\"}\n",
      "2026-01-24 15:52:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:52:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:52:59 | INFO | valid | {\"epoch\": 62, \"valid_loss\": \"3.774\", \"valid_nll_loss\": \"2.479\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5665.7\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4029\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4029 updates\n",
      "2026-01-24 15:52:59 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint62.pt\n",
      "2026-01-24 15:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint62.pt\n",
      "2026-01-24 15:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint62.pt (epoch 62 @ 4029 updates, score 3.774) (writing took 1.1708806850001565 seconds)\n",
      "2026-01-24 15:53:00 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
      "2026-01-24 15:53:00 | INFO | train | {\"epoch\": 62, \"train_loss\": \"3.3\", \"train_nll_loss\": \"2.123\", \"train_ppl\": \"4.36\", \"train_wps\": \"1860\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4029\", \"train_lr\": \"0.0004029\", \"train_gnorm\": \"0.323\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:53:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:53:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:53:00 | INFO | fairseq.trainer | begin training epoch 63\n",
      "2026-01-24 15:53:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:55:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:55:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:56:07 | INFO | valid | {\"epoch\": 63, \"valid_loss\": \"3.783\", \"valid_nll_loss\": \"2.477\", \"valid_ppl\": \"5.57\", \"valid_wps\": \"5658.2\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4094\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 4094 updates\n",
      "2026-01-24 15:56:07 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint63.pt\n",
      "2026-01-24 15:56:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint63.pt\n",
      "2026-01-24 15:56:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint63.pt (epoch 63 @ 4094 updates, score 3.783) (writing took 1.1980316869994567 seconds)\n",
      "2026-01-24 15:56:08 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
      "2026-01-24 15:56:08 | INFO | train | {\"epoch\": 63, \"train_loss\": \"3.295\", \"train_nll_loss\": \"2.117\", \"train_ppl\": \"4.34\", \"train_wps\": \"1860.2\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4094\", \"train_lr\": \"0.0004094\", \"train_gnorm\": \"0.324\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:56:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:56:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:56:08 | INFO | fairseq.trainer | begin training epoch 64\n",
      "2026-01-24 15:56:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 15:56:25 | INFO | train_inner | {\"epoch\": 64, \"update\": 63.092, \"loss\": \"3.301\", \"nll_loss\": \"2.125\", \"ppl\": \"4.36\", \"wps\": \"1831.9\", \"ups\": \"0.34\", \"wpb\": \"5383.4\", \"bsz\": \"23.3\", \"num_updates\": \"4100\", \"lr\": \"0.00041\", \"gnorm\": \"0.325\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"274\", \"gb_free\": \"2.8\", \"wall\": \"0\"}\n",
      "2026-01-24 15:59:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 15:59:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:59:15 | INFO | valid | {\"epoch\": 64, \"valid_loss\": \"3.777\", \"valid_nll_loss\": \"2.48\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5666\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4159\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 15:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 4159 updates\n",
      "2026-01-24 15:59:15 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint64.pt\n",
      "2026-01-24 15:59:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint64.pt\n",
      "2026-01-24 15:59:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint64.pt (epoch 64 @ 4159 updates, score 3.777) (writing took 1.2099444190025679 seconds)\n",
      "2026-01-24 15:59:16 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
      "2026-01-24 15:59:16 | INFO | train | {\"epoch\": 64, \"train_loss\": \"3.288\", \"train_nll_loss\": \"2.109\", \"train_ppl\": \"4.32\", \"train_wps\": \"1859.9\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4159\", \"train_lr\": \"0.0004159\", \"train_gnorm\": \"0.325\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 15:59:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 15:59:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 15:59:16 | INFO | fairseq.trainer | begin training epoch 65\n",
      "2026-01-24 15:59:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 16:01:08 | INFO | train_inner | {\"epoch\": 65, \"update\": 64.631, \"loss\": \"3.283\", \"nll_loss\": \"2.103\", \"ppl\": \"4.3\", \"wps\": \"1894.9\", \"ups\": \"0.35\", \"wpb\": \"5372.1\", \"bsz\": \"23.4\", \"num_updates\": \"4200\", \"lr\": \"0.00042\", \"gnorm\": \"0.326\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.8\", \"wall\": \"0\"}\n",
      "2026-01-24 16:02:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 16:02:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:02:23 | INFO | valid | {\"epoch\": 65, \"valid_loss\": \"3.786\", \"valid_nll_loss\": \"2.481\", \"valid_ppl\": \"5.58\", \"valid_wps\": \"5659.8\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4224\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 16:02:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 4224 updates\n",
      "2026-01-24 16:02:23 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint65.pt\n",
      "2026-01-24 16:02:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint65.pt\n",
      "2026-01-24 16:02:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint65.pt (epoch 65 @ 4224 updates, score 3.786) (writing took 1.2220129289999022 seconds)\n",
      "2026-01-24 16:02:24 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
      "2026-01-24 16:02:24 | INFO | train | {\"epoch\": 65, \"train_loss\": \"3.281\", \"train_nll_loss\": \"2.101\", \"train_ppl\": \"4.29\", \"train_wps\": \"1859.4\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4224\", \"train_lr\": \"0.0004224\", \"train_gnorm\": \"0.328\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 16:02:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:02:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 16:02:24 | INFO | fairseq.trainer | begin training epoch 66\n",
      "2026-01-24 16:02:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 16:05:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 16:05:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:05:31 | INFO | valid | {\"epoch\": 66, \"valid_loss\": \"3.785\", \"valid_nll_loss\": \"2.478\", \"valid_ppl\": \"5.57\", \"valid_wps\": \"5667.4\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4289\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 16:05:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 4289 updates\n",
      "2026-01-24 16:05:31 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint66.pt\n",
      "2026-01-24 16:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint66.pt\n",
      "2026-01-24 16:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint66.pt (epoch 66 @ 4289 updates, score 3.785) (writing took 1.1972783450000861 seconds)\n",
      "2026-01-24 16:05:32 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
      "2026-01-24 16:05:32 | INFO | train | {\"epoch\": 66, \"train_loss\": \"3.274\", \"train_nll_loss\": \"2.092\", \"train_ppl\": \"4.26\", \"train_wps\": \"1860.2\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4289\", \"train_lr\": \"0.0004289\", \"train_gnorm\": \"0.33\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.8\", \"train_wall\": \"0\"}\n",
      "2026-01-24 16:05:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:05:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 16:05:32 | INFO | fairseq.trainer | begin training epoch 67\n",
      "2026-01-24 16:05:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 16:06:03 | INFO | train_inner | {\"epoch\": 67, \"update\": 66.169, \"loss\": \"3.275\", \"nll_loss\": \"2.094\", \"ppl\": \"4.27\", \"wps\": \"1832.7\", \"ups\": \"0.34\", \"wpb\": \"5406.4\", \"bsz\": \"23.7\", \"num_updates\": \"4300\", \"lr\": \"0.00043\", \"gnorm\": \"0.329\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"275\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 16:08:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 16:08:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:08:39 | INFO | valid | {\"epoch\": 67, \"valid_loss\": \"3.792\", \"valid_nll_loss\": \"2.488\", \"valid_ppl\": \"5.61\", \"valid_wps\": \"5671\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4354\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 16:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 4354 updates\n",
      "2026-01-24 16:08:39 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint67.pt\n",
      "2026-01-24 16:08:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint67.pt\n",
      "2026-01-24 16:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint67.pt (epoch 67 @ 4354 updates, score 3.792) (writing took 1.1970694580013514 seconds)\n",
      "2026-01-24 16:08:40 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
      "2026-01-24 16:08:40 | INFO | train | {\"epoch\": 67, \"train_loss\": \"3.268\", \"train_nll_loss\": \"2.084\", \"train_ppl\": \"4.24\", \"train_wps\": \"1859.5\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4354\", \"train_lr\": \"0.0004354\", \"train_gnorm\": \"0.331\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 16:08:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:08:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65\n",
      "2026-01-24 16:08:40 | INFO | fairseq.trainer | begin training epoch 68\n",
      "2026-01-24 16:08:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2026-01-24 16:10:47 | INFO | train_inner | {\"epoch\": 68, \"update\": 67.708, \"loss\": \"3.269\", \"nll_loss\": \"2.086\", \"ppl\": \"4.24\", \"wps\": \"1889.9\", \"ups\": \"0.35\", \"wpb\": \"5358.3\", \"bsz\": \"23.1\", \"num_updates\": \"4400\", \"lr\": \"0.00044\", \"gnorm\": \"0.333\", \"clip\": \"0\", \"loss_scale\": \"128\", \"train_wall\": \"273\", \"gb_free\": \"2.9\", \"wall\": \"0\"}\n",
      "2026-01-24 16:11:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2026-01-24 16:11:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 16:11:47 | INFO | valid | {\"epoch\": 68, \"valid_loss\": \"3.795\", \"valid_nll_loss\": \"2.486\", \"valid_ppl\": \"5.6\", \"valid_wps\": \"5673.3\", \"valid_wpb\": \"658.7\", \"valid_bsz\": \"2.6\", \"valid_num_updates\": \"4419\", \"valid_best_loss\": \"3.761\"}\n",
      "2026-01-24 16:11:47 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 20 runs\n",
      "2026-01-24 16:11:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 4419 updates\n",
      "2026-01-24 16:11:47 | INFO | fairseq.trainer | Saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint68.pt\n",
      "2026-01-24 16:11:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint68.pt\n",
      "2026-01-24 16:11:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official/checkpoint68.pt (epoch 68 @ 4419 updates, score 3.795) (writing took 1.1831062140008726 seconds)\n",
      "2026-01-24 16:11:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
      "2026-01-24 16:11:48 | INFO | train | {\"epoch\": 68, \"train_loss\": \"3.264\", \"train_nll_loss\": \"2.079\", \"train_ppl\": \"4.23\", \"train_wps\": \"1860\", \"train_ups\": \"0.35\", \"train_wpb\": \"5378.2\", \"train_bsz\": \"23.5\", \"train_num_updates\": \"4419\", \"train_lr\": \"0.0004419\", \"train_gnorm\": \"0.333\", \"train_clip\": \"0\", \"train_loss_scale\": \"128\", \"train_train_wall\": \"178\", \"train_gb_free\": \"2.9\", \"train_wall\": \"0\"}\n",
      "2026-01-24 16:11:48 | INFO | fairseq_cli.train | done training in 4108.8 seconds\n"
     ]
    }
   ],
   "source": [
    "!fairseq-train /home/khanh/Projects/KhoaLuan/data_bin \\\n",
    "    --config-yaml config.yaml \\\n",
    "    --task speech_to_text \\\n",
    "    --arch s2t_transformer_s \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-8 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt \\\n",
    "    --warmup-updates 10000 \\\n",
    "    --clip-norm 1.0 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --max-tokens 2000 \\\n",
    "    --update-freq 8 \\\n",
    "    --max-update 100000 \\\n",
    "    --log-format json --log-interval 100 \\\n",
    "    --save-dir /home/khanh/Projects/KhoaLuan/checkpoints/s2ut_official \\\n",
    "    --keep-last-epochs 10 \\\n",
    "    --keep-best-checkpoints 5 \\\n",
    "    --patience 20 \\\n",
    "    --num-workers 4 \\\n",
    "    --fp16 \\\n",
    "    --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f92aab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:01:58 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../checkpoints/s2ut_official/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': '/home/khanh/Projects/KhoaLuan/results/test_run'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='speech_to_text', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=1, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=1, max_valid_steps=None, curriculum=0, gen_subset='valid', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='../checkpoints/s2ut_official/checkpoint_best.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/home/khanh/Projects/KhoaLuan/results/test_run', beam=5, beam_mt=0, nbest=1, max_len_a=0, max_len_b=200, max_len_a_mt=0, max_len_b_mt=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, lenpen_mt=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, arch='wav2vec2', data='/home/khanh/Projects/KhoaLuan/data_bin', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='speech_to_text'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2026-01-24 19:01:58 | INFO | fairseq.tasks.speech_to_text | dictionary size (dict.txt): 1,004\n",
      "2026-01-24 19:01:58 | INFO | fairseq_cli.generate | loading model(s) from ../checkpoints/s2ut_official/checkpoint_best.pt\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 19:01:59 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
      "2026-01-24 19:01:59 | INFO | fairseq.data.audio.speech_to_text_dataset | 'valid' has 0.00% OOV\n",
      "2026-01-24 19:01:59 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"valid\", n_samples=192, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
      "))\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "  0%|                                                   | 0/191 [00:00<?, ?it/s]2026-01-24 19:01:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2026-01-24 19:01:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': None}\n",
      "2026-01-24 19:04:03 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2026-01-24 19:04:03 | INFO | fairseq_cli.generate | Translated 191 sentences (30,076 tokens) in 123.1s (1.55 sentences/s, 244.36 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-generate /home/khanh/Projects/KhoaLuan/data_bin \\\n",
    "    --config-yaml config.yaml \\\n",
    "    --path ../checkpoints/s2ut_official/checkpoint_best.pt \\\n",
    "    --task speech_to_text \\\n",
    "    --gen-subset valid \\\n",
    "    --beam 5 \\\n",
    "    --batch-size 1 \\\n",
    "    --results-path /home/khanh/Projects/KhoaLuan/results/test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be78fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24 14:54:47 | INFO | __main__ | Namespace(in_code_file='../test_unit.txt', vocoder='../vocoder/g_00500000', vocoder_cfg='../vocoder/config.json', results_path='../results_audio', dur_prediction=False, speaker_id=-1, cpu=False)\n",
      "/home/khanh/miniconda3/envs/hubert/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Removing weight norm...\n",
      "2026-01-24 14:54:48 | INFO | fairseq.models.text_to_speech.vocoder | loaded CodeHiFiGAN checkpoint from ../vocoder/g_00500000\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#  tesst vocoder \n",
    "# Tạo thư mục kết quả\n",
    "!mkdir -p results_audio\n",
    "\n",
    "\n",
    "# Chạy lệnh convert\n",
    "!python examples/speech_to_speech/generate_waveform_from_code.py \\\n",
    "    --in-code-file ../test_unit.txt \\\n",
    "    --vocoder ../vocoder/g_00500000 \\\n",
    "    --vocoder-cfg ../vocoder/config.json \\\n",
    "    --results-path ../results_audio "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
